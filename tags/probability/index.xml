<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Probability on Jose Storopoli, PhD</title>
  <link rel="alternate" href="https://storopoli.io/tags/probability/" />
  <link rel="self" href="https://storopoli.io/tags/probability/index.xml" />
  <subtitle>Recent content in Probability on Jose Storopoli, PhD</subtitle>
  <id>https://storopoli.io/tags/probability/</id>
  <generator uri="http://gohugo.io" version="0.133.0">Hugo</generator>
  <language>en-us</language>
  <updated>2024-02-11T15:59:02Z</updated>
  <author>
    <name>Jose Storopoli</name>
    
  </author>
  <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)</rights>
      <entry>
        <title>Seed Phrases and Entropy</title>
        <link rel="alternate" href="https://storopoli.io/2024-02-11-mnemonic/" />
        <id>https://storopoli.io/2024-02-11-mnemonic/</id>
        <published>2024-02-11T15:59:02Z</published>
        <updated>2024-04-10T08:57:32-03:00</updated>
        <summary type="html">Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&amp;rsquo;ll have to unfortunately enable JavaScript.
In this post, let&amp;rsquo;s dive into a topic that is very important for anyone who uses the internet: passwords. We&amp;rsquo;ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin &amp;ldquo;seed phrases&amp;rdquo;1.
Entropy Before we go into passwords, I&amp;rsquo;ll introduce the concept of Entropy.</summary>
          <content type="html"><![CDATA[<p><img loading="lazy" src="password_strength.png#center" alt="Password Meme"  />
</p>
<blockquote>
<p>Warning: This post has <a href="https://katex.org/">KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p>
</blockquote>
<p>In this post, let&rsquo;s dive into a topic that is very important for anyone who uses the internet: <strong>passwords</strong>.
We&rsquo;ll cover what the hell is <strong>Entropy</strong>,
good <strong>password practices</strong>,
and how it relates to <strong>Bitcoin &ldquo;seed phrases&rdquo;</strong><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h2 id="entropy">Entropy</h2>
<p>Before we go into passwords,
I&rsquo;ll introduce the concept of <strong><em>Entropy</em></strong>.</p>
<p><a href="https://en.wikipedia.org/wiki/Entropy">Entropy</a>
is a measure of the <strong>amount of disorder in a system</strong>.
It has its origins in <strong>Thermodynamics</strong>,
where it&rsquo;s used to measure the amount of energy in a system that is not available to do work.</p>
<p>The etymology of the word &ldquo;Entropy&rdquo; is after the Greek word for &ldquo;transformation&rdquo;.</p>
<p>It was given a proper statistical definition by <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">Ludwig Boltzmann</a> in 1870s.
while establishing the field of <a href="https://en.wikipedia.org/wiki/Statistical_dynamics">Statistical Dynamics</a>,
a field of physics that studies the behavior of large collections of particles.</p>
<figure>
    <img loading="lazy" src="boltzmann.jpg#center"
         alt="Ludwig Boltzmann" width="300"/> <figcaption>
            Ludwig Boltzmann
        </figcaption>
</figure>

<p>In the context of Statistical Dynamics,
<strong>Entropy is a measure of the number of ways a system can be arranged</strong>.
The more ways a system can be arranged,
the higher its Entropy.
Specifically, <strong>Entropy is a logarithmic measure of the number of system states with significant probability of being occupied</strong>:</p>
<p>$$S = -k \cdot \sum_i p_i \ln p_i$$</p>
<p>Where:</p>
<ul>
<li>$S$: Entropy.</li>
<li>$k$: Boltzmann&rsquo;s constant, a physical constant that relates temperature to energy.</li>
<li>$p_i$: probability of the system being in state $i$.</li>
</ul>
<p>In this formula, if all states are equally likely,
i.e $p_i = \frac{1}{N}$,
where $N$ is the number of states,
then the entropy is maximized.
You can see this since a probability $p$ is a real number between 0 and 1,
and as $N$ approaches infinity,
the sum of the logarithms approaches negative infinity.
Then, multiplying by $-k$ yields positive infinity.</p>
<h3 id="how-the-hell-physics-came-to-passwords">How the hell Physics came to Passwords?</h3>
<p>There&rsquo;s once a great men called <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>,
who single-handedly founded the field of <a href="https://en.wikipedia.org/wiki/Information_theory"><strong>Information Theory</strong></a>,
invented the concept of a <a href="https://en.wikipedia.org/wiki/Bit"><strong>Bit</strong></a>,
and was the first to think about Boolean algebra in the context of electrical circuits.
He laid the foundation for the <a href="https://en.wikipedia.org/wiki/Digital_Revolution"><strong>Digital Revolution</strong></a>.</p>
<p>If you are happy using your smartphone, laptop, or any other digital device,
in you high speed fiber internet connection,
through a wireless router to send cats pictures to your friends,
then you should thank Claude Shannon.</p>
<figure>
    <img loading="lazy" src="shannon.jpg#center"
         alt="Claude Shannon" width="300"/> <figcaption>
            Claude Shannon
        </figcaption>
</figure>

<p>He was trying to find a formula to quantify the amount of information in a message.
He wanted three things:</p>
<ol>
<li>The measure should be a <strong>function of the probability of the message</strong>.
Messages that are more likely should have less information.</li>
<li>The measure should be <strong>additive</strong>.
The information in a message should be the sum of the information in its parts.</li>
<li>The measure should be <strong>continuous</strong>.
Small changes in the message should result in small changes in the measure.</li>
</ol>
<p>He pretty much found that the formula for Entropy in statistical mechanics
was a good measure of information.
He called it <em>Entropy</em> to honor Boltzmann&rsquo;s work.
To differentiate it from the Statistical Dynamics&rsquo; Entropy,
he changed the letter to $H$,
in honor of <a href="https://en.wikipedia.org/wiki/H-theorem">Boltzmann&rsquo;s $H$-theorem</a>.
So the formula for the Entropy of a message is:</p>
<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​)$$</p>
<p>Where:</p>
<ul>
<li>$X$: random discrete variable.</li>
<li>$H(X)$: Entropy of $X$</li>
<li>$P(x_i)$: probability of the random variable $X$ taking the value $x_i$.
Also known as the probability mass function (PMF) of the discrete random variable $X$.</li>
<li>$\log$: base 2 logarithm, to measure the Entropy in bits.</li>
</ul>
<p>In information theory,
the <strong>Entropy of a random variable is the average level of &ldquo;information&rdquo;, &ldquo;surprise&rdquo;,
or &ldquo;uncertainty&rdquo; inherent to the variable&rsquo;s possible outcomes</strong><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Let&rsquo;s take the simple example of a fair coin.
The Entropy of the random variable $X$ that represents the outcome of a fair coin flip is:</p>
<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = -\left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{2}\right) = 1 \text{ bit}$$</p>
<p>So the outcome of a fair coin flip has 1 bit of Entropy.
This means that the outcome of a fair coin flip has 1 bit of information,
or 1 bit of uncertainty.
Once the message is received,
that the coin flip was heads or tails,
the receiver has 1 bit of information about the outcome.</p>
<p>Alternatively, we only need 1 bit to encode the outcome of a fair coin flip.
Hence, there&rsquo;s a connection between Entropy, search space, and information.</p>
<p>Another good example is the outcome of a fair 6-sided die.
The Entropy of the random variable $X$ that represents the outcome of a fair 6-sided die is:</p>
<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = - \sum_{i=1}^6\left(\frac{1}{6} * \log \frac{1}{6} \right) \approx 2.58 \text{ bits}$$</p>
<p>This means that the outcome of a fair 6-sided die has 2.58 bits of Entropy.
we need $\operatorname{ceil}(2.58) = 3$ bits to encode the outcome of a fair 6-sided die.</p>
<h3 id="entropy-and-passwords">Entropy and Passwords</h3>
<p>Ok now we come full circle.
Let&rsquo;s talk, finally, about passwords.</p>
<p>In the context of passwords, <strong>Entropy</strong> is a measure of how unpredictable a password is.
The higher the Entropy, the harder it is to guess the password.
The Entropy of a password is measured in bits,
and it&rsquo;s calculated using the formula:</p>
<p>$$H = L \cdot \log_2(N)$$</p>
<p>Where:</p>
<ul>
<li>$H$: Entropy in bits</li>
<li>$N$: number of possible characters in the password</li>
<li>$L$: length of the password</li>
<li>$\log_2$:​ (N) calculates how many bits are needed to represent each character from the set.</li>
</ul>
<p>For example,
if we have a password with 8 characters and each character can be any of the 26 lowercase letters,
the standard english alphabet,
the Entropy would be:</p>
<p>$$H = 8 \cdot \log_2(26) \approx 37.6 \text{ bits}$$</p>
<p>This means that an attacker would need to try $2^{37.6} \approx 2.01 \cdot 10^{11}$ combinations<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> to guess the password.</p>
<p>If the password were to include uppercase letters, numbers, and symbols
(let&rsquo;s assume 95 possible characters in total),
the Entropy for an 8-character password would be:</p>
<p>$$H = 8 \cdot \log_2(95) \approx 52.6 \text{ bits}$$</p>
<p>This means that an attacker would need to try $2^{52.6} \approx 6.8 \cdot 10^{15}$ combinations to guess the password.</p>
<p>This sounds a lot but it&rsquo;s not that much.</p>
<p>For the calculations below, we&rsquo;ll assume that the attacker now your dictionary set,
i.e. the set of characters you use to create your password,
and the password length.</p>
<p>If an attacker get a hold of an NVIDIA RTX 4090,
MSRP USD 1,599, which can do
<a href="https://www.tomshardware.com/news/rtx-4090-password-cracking-comparison">300 GH/s (300,000,000,000 hashes/second)</a>,
i.e. $3 \cdot 10^{11}$ hashes/second,
it would take:</p>
<ol>
<li>8-length lowercase-only password:</li>
</ol>
<p>$$\frac{2.01 \cdot 10^{11}}{3 \cdot 10^{11}} \approx 0.67 \text{ seconds}$$</p>
<ol>
<li>8-length password with uppercase letters, numbers, and symbols:</li>
</ol>
<p>$$\frac{6.8 \cdot 10^{15}}{3 \cdot 10^{11}} \approx 22114 \text{ seconds} \approx 6.14 \text{ hours}$$</p>
<p>So, the first password would be cracked in less than a second,
while the second would take a few hours.
This with just one 1.5k USD GPU.</p>
<h2 id="bitcoin-seed-phrases">Bitcoin Seed Phrases</h2>
<p>Now that we understand Entropy and how it relates to passwords,
let&rsquo;s talk about bitcoin seed phrases<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Remember that our private key is a big-fucking number?
If not, check my <a href="../2024-02-05-crypto-basics/">post on cryptographics basics</a>.</p>
<p><a href="https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki">BIP-39</a>
specifies how to use easy-to-remember seed phrases to store and recover
private keys.
The <a href="https://github.com/bitcoin/bips/blob/master/bip-0039/english.txt">wordlist</a>
adheres to the following principles:</p>
<ol>
<li><strong>smart selection of words</strong>:
the wordlist is created in such a way that it&rsquo;s enough to type the first four
letters to unambiguously identify the word.</li>
<li><strong>similar words avoided</strong>:
word pairs like &ldquo;build&rdquo; and &ldquo;built&rdquo;, &ldquo;woman&rdquo; and &ldquo;women&rdquo;, or &ldquo;quick&rdquo; and &ldquo;quickly&rdquo;
not only make remembering the sentence difficult but are also more error
prone and more difficult to guess.</li>
</ol>
<p>Here is a simple 7-word seed phrase: <code>brave sadness grocery churn wet mammal tube</code>.
Surprisingly enough, this badboy here gives you $77$ bits of Entropy,
while also being easy to remember.
This is due to the fact that the wordlist has 2048 words,
so each word gives you $\log_2(2048) = 11$ bits of Entropy<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>There&rsquo;s a minor caveat to cover here.
The last word in the seed phrase is a checksum,
which is used to verify that the phrase is valid.</p>
<p>So, if you have a 12-word seed phrase,
you have $11 \cdot 11 = 121$ bits of Entropy.
And for a 24-word seed phrase,
you have $23 \cdot 11 = 253$ bits of Entropy.</p>
<p>The National Institute of Standards and Technology (NIST) recommends a
<a href="https://crypto.stackexchange.com/a/87059">minimum of 112 bits of Entropy for all things cryptographic</a>.
And Bitcoin has a <a href="https://bitcoin.stackexchange.com/a/118929">minimum of 128 bits of Entropy</a>.</p>
<p>Depending on your threat model,
<a href="https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html">&ldquo;Assume that your adversary is capable of a trillion guesses per second&rdquo;</a>,
it can take a few years to crack a 121-bit Entropy seed phrase:</p>
<p>$$\frac{2^{121}}{10^{12}} \approx 2.66 \cdot 10^{24} \text{ seconds} \approx 3.08 \cdot 10^{19} \text{ days} \approx 8.43 \cdot 10^{16} \text{ years}$$</p>
<p>That&rsquo;s a lot of years.
Now for a 253-bit Entropy seed phrase:</p>
<p>$$\frac{2^{253}}{10^{12}} \approx 1.45 \cdot 10^{64} \text{ seconds} \approx 1.68 \cdot 10^{59} \text{ days} \approx 4.59 \cdot 10^{56} \text{ years}$$</p>
<p>That&rsquo;s another huge number of years.</p>
<h2 id="seed-phrases-and-passwords">Seed Phrases and Passwords</h2>
<p>You can also use a seed phrase as a password.
The bonus point is that you don&rsquo;t need to use the last word as a checksum,
so you get 11 bits of Entropy free, compared to a Bitcoin seed phrase.</p>
<p>Remember the 7-words badboy seed phrase we generated earlier?
<code>brave sadness grocery churn wet mammal tube</code>.</p>
<p>It has $66$ bits of Entropy.
This would take, assuming
<a href="https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html">&ldquo;that your adversary is capable of a trillion guesses per second&rdquo;</a>:</p>
<p>$$\frac{2^{77}}{10^{12}} \approx 1.51 \cdot 10^{11} \text{ seconds} \approx 1.75 \cdot 10^{6} \text{ days} \approx 4.79 \cdot 10^{3} \text{ years}$$</p>
<p>That&rsquo;s why tons of people use seed phrases as passwords.
Even if you know the dictionary set and the length of the password,
i.e. the number of words in the seed phrase,
it would take a lot of years to crack it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Entropy is a measure of the amount of disorder in a system.
In the context of passwords, it&rsquo;s a measure of how unpredictable a password is.
The higher the Entropy, the harder it is to guess the password.</p>
<p>Bitcoin seed phrases are a great way to store and recover private keys.
They are easy to remember and have a high amount of Entropy.
You can even use a seed phrase as a password.</p>
<p>Even it your attacker is capable of a trillion guesses per second,
like the <a href="https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html">NSA</a>,
it would take them a lot of years to crack even a 7-word seed phrase.</p>
<p>If you want to generate a seed phrase,
you can use <a href="https://keepassxc.org/">KeePassXC</a>,
which is a great open-source <strong><em>offline</em></strong> password manager that supports seed phrases<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>seed phrases are technically called &ldquo;mnemonic phrases&rdquo;,
but I&rsquo;ll use the term &ldquo;seed phrases&rdquo; for the rest of the post.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>there is a Bayesian argument about
the use of priors that should adhere to the
<a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Principle of Maximal Entropy</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>technically, we need to divide the number of combinations by 2,
since we are assuming that the attacker is using a brute-force attack,
which means that the attacker is trying all possible combinations,
and the password could be at the beginning or at the end of the search space.
This is called the <a href="https://en.wikipedia.org/wiki/Birthday_problem">birthday paradox</a>,
and it assumes that the password is uniformly distributed in the search space.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>remember that $2^{11} = 2048$.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>technically, KeePassXC uses the <a href="https://www.eff.org/files/2016/07/18/eff_large_wordlist.txt">EFF wordlist</a>,
which has 7,776 words, so each word gives you $\log_2(7776) \approx 12.9$ bits of Entropy.
They were created to be easy to use with 6-sided dice.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content>
      </entry>
      <entry>
        <title>Lindley&#39;s Paradox, or The consistency of Bayesian Thinking</title>
        <link rel="alternate" href="https://storopoli.io/2023-11-23-lindley_paradox/" />
        <id>https://storopoli.io/2023-11-23-lindley_paradox/</id>
        <published>2023-11-22T07:06:59-03:00</published>
        <updated>2024-04-10T08:57:32-03:00</updated>
        <summary type="html">Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&amp;rsquo;ll have to unfortunately enable JavaScript.
Dennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: &amp;ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&amp;rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell&amp;rsquo;s Rule2.</summary>
          <content type="html"><![CDATA[<p><img loading="lazy" src="lindley.jpg#center" alt="Dennis Lindley"  />
</p>
<blockquote>
<p>Warning: This post has <a href="https://katex.org/">KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Dennis_Lindley">Dennis Lindley</a>,
one of my many heroes,
was an English statistician,
decision theorist and leading advocate of Bayesian statistics.
He published a pivotal book,
<a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118650158">Understanding Uncertainty</a>,
that changed my view on what is and how to handle uncertainty in a
coherent<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> way.
He is responsible for one of my favorites quotes:
&ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&rdquo;;
and one of my favorite heuristics around prior probabilities:
<a href="https://en.wikipedia.org/wiki/Cromwell%27s_rule">Cromwell&rsquo;s Rule</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.
Lindley predicted in 1975 that &ldquo;Bayesian methods will indeed become pervasive,
enabled by the development of powerful computing facilities&rdquo; (Lindley, 1975).
You can find more about all of Lindley&rsquo;s achievements in his <a href="https://www.theguardian.com/science/2014/mar/16/dennis-lindley">obituary</a>.</p>
<h2 id="lindleys-paradox">Lindley&rsquo;s Paradox</h2>
<p>Lindley&rsquo;s paradox<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> is a counterintuitive situation in statistics
in which the Bayesian and frequentist approaches to a hypothesis testing problem
give different results for certain choices of the prior distribution.</p>
<p>More formally, the paradox is as follows.
We have some parameter $\theta$ that we are interested in.
Then, we proceed with an experiment to test two competing hypotheses:</p>
<ol>
<li>$H_0$ (also known as <em>null hypothesis</em>):
there is no &ldquo;effect&rdquo;, or, more specifically,
$\theta = 0$.</li>
<li>$H_a$ (also known as <em>alternative hypothesis</em>):
there is an &ldquo;effect&rdquo;, or, more specifically,
$\theta \ne 0$.</li>
</ol>
<p>The paradox occurs when two conditions are met:</p>
<ol>
<li>The result of the experiment is <em>significant</em> by a frequentist test of $H_0$,
which indicates sufficient evidence to reject $H_0$, at a certain threshold of
probability<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</li>
<li>The posterior probability (Bayesian approach) of $H_0 \mid \theta$
(null hypothesis given $\theta$) is high,
which indicates strong evidence that $H_0$ should be favored over $H_a$,
that is, to <em>not</em> reject $H_0$.</li>
</ol>
<p>These results can occur at the same time when $H_0$ is very specific,
$H_a$ more diffuse,
and the prior distribution does not strongly favor one or the other.
These conditions are pervasive across science
and common in traditional null-hypothesis significance testing approaches.</p>
<p>This is a duel of frequentist versus Bayesian approaches,
and one of the many in which Bayesian emerges as the most coherent.
Let&rsquo;s give a example and go over the analytical result with a ton of math,
but also a computational result with <a href="https://julialang.org">Julia</a>.</p>
<h2 id="example">Example</h2>
<p>Here&rsquo;s the setup for the example.
In a certain city 49,581 boys and 48,870 girls have been
born over a certain time period.
The observed proportion of male births is thus
$\frac{49,581}{98,451} \approx 0.5036$.</p>
<p>We assume that the birth of a child is independent with a certain probability
$\theta$.
Since our data is a sequence of $n$ independent <a href="https://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli trials</a>,
i.e., $n$ independent random experiments with exactly two possible outcomes:
&ldquo;success&rdquo; and &ldquo;failure&rdquo;,
in which the probability of success is the same every time the
experiment is conducted.
We can safely assume that it follows a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>
with parameters:</p>
<ul>
<li>$n$: the number of &ldquo;trials&rdquo; (or the total number of births).</li>
<li>$\theta$: the probability of male births.</li>
</ul>
<p>We then set up our two competing hypotheses:</p>
<ol>
<li>$H_0$: $\theta = 0.5$.</li>
<li>$H_a$: $\theta \ne 0.5$.</li>
</ol>
<h3 id="analytical-solution">Analytical Solution</h3>
<p>This is a toy-problem and, like most toy problems,
we can solve it analytically<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> for both the frequentist and the Bayesian approaches.</p>
<h4 id="analytical-solutions----frequentist-approach">Analytical Solutions &ndash; Frequentist Approach</h4>
<p>The frequentist approach to testing $H_0$ is to compute a $p$-value<sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>,
the probability of observing births of boys at least as large as 49,581
assuming $H_0$ is true.
Because the number of births is very large,
we can use a normal approximation<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> for the
binomial-distributed number of male births.
Let&rsquo;s define $X$ as the total number of male births,
then $X$ follows a normal distribution:</p>
<p>$$X \sim \text{Normal}(\mu, \sigma)$$</p>
<p>where $\mu$ is the mean parameter,
$n \theta$ in our case,
and $\sigma$ is the standard deviation parameter,
$\sqrt{n \theta (1 - \theta)}$.
We need to calculate the conditional probability of
$X \geq \frac{49,581}{98,451} \approx 0.5036$
given $\mu = n \theta = 98,451 \cdot \frac{1}{2} = 49,225.5$
and
$\sigma = \sqrt{n \theta (1 - \theta)} = \sqrt{98,451 \cdot \frac{1}{2} \cdot (1 - \frac{1}{2})}$:</p>
<p>$$P(X \ge 0.5036 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75})$$</p>
<p>This is basically a
<a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function (CDF)</a>
of $X$ on the interval $[49,225.5, 98,451]$:</p>
<p>$$\int_{49,225.5}^{98,451} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{\left( \frac{x - \mu}{\sigma} \right)^2}{2}} dx$$</p>
<p>After inserting the values and doing some arithmetic,
our answer is approximately $0.0117$.
Note that this is a one-sided test,
since it is symmetrical,
the two-sided test would be
$0.0117 \cdot 2 = 0.0235$.
Since we don&rsquo;t deviate from the Fisher&rsquo;s canon,
this is well below the 5% threshold.
Hooray! We rejected the null hypothesis!
Quick! Grab a frequentist celebratory cigar!
But, wait. Let&rsquo;s check the Bayesian approach.</p>
<h4 id="analytical-solutions----bayesian-approach">Analytical Solutions &ndash; Bayesian Approach</h4>
<p>For the Bayesian approach, we need to set prior probabilities on both hypotheses.
Since we do not favor one from another, let&rsquo;s set equal prior probabilities:</p>
<p>$$P(H_0) = P(H_a) = \frac{1}{2}$$</p>
<p>Additionally, all parameters of interest need a prior distribution.
So, let&rsquo;s put a prior distribution on $\theta$.
We could be fancy here, but let&rsquo;s not.
We&rsquo;ll use a uniform distribution on $[0, 1]$.</p>
<p>We have everything we need to compute the posterior probability of $H_0$ given
$\theta$.
For this, we&rsquo;ll use <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes theorem</a><sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>:</p>
<p>$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$$</p>
<p>Now again let&rsquo;s plug in all the values:</p>
<p>$$P(H_0 \mid \theta) = \frac{P(\theta \mid H_0) P(H_0)}{P(\theta)}$$</p>
<p>Note that by the <a href="https://en.wikipedia.org/wiki/Probability_axioms">axioms of probability</a>
and by the <a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">product rule of probability</a>
we can decompose $P(\theta)$ into:</p>
<p>$$P(\theta) = P(\theta \mid H_0) P(H_0) + P(\theta \mid H_a) P(H_a)$$</p>
<p>Again, we&rsquo;ll use the normal approximation:</p>
<p>$$
\begin{aligned}
&amp;P \left( \theta = 0.5 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75} \right) \\
&amp;= \frac{
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \left( \frac{(\mu - \mu \cdot 0.5)}{2 \sigma} \right)^2} \cdot 0.5
}
{
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{ \left( -\frac{(\mu - \mu \cdot 0.5)}{2 \sigma} \right)^2} \cdot 0.5 +
\int_0^1 \frac {1}{\sqrt{2 \pi \sigma^2} } e^{- \left( \frac{\mu - \mu \cdot \theta)}{2 \sigma} \right)^2}d \theta \cdot 0.5
} \\
&amp;= 0.9505
\end{aligned}
$$</p>
<p>The likelihood of the alternative hypothesis,
$P(\theta \mid H_a)$,
is just the CDF of all possible values of $\theta \ne 0.5$.</p>
<p>$$P(H_0 \mid \text{data}) = P \left( \theta = 0.5 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75} \right) &gt; 0.95$$</p>
<p>And we fail to reject the null hypothesis, in frequentist terms.
However, we can also say in Bayesian terms, that we strongly favor $H_0$
over $H_a$.</p>
<p>Quick! Grab the Bayesian celebratory cigar!
The null is back on the game!</p>
<h3 id="computational-solutional">Computational Solutional</h3>
<p>For the computational solution, we&rsquo;ll use <a href="https://julialang.org">Julia</a>
and the following packages:</p>
<ul>
<li><a href="https://github.com/JuliaStats/HypothesisTests.jl"><code>HypothesisTest.jl</code></a></li>
<li><a href="https://turinglang.org/"><code>Turing.jl</code></a></li>
</ul>
<h4 id="computational-solutions----frequentist-approach">Computational Solutions &ndash; Frequentist Approach</h4>
<p>We can perform a <a href="https://juliastats.org/HypothesisTests.jl/stable/nonparametric/#Binomial-test"><code>BinomialTest</code></a>
with <code>HypothesisTest.jl</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">using</span> <span class="n">HypothesisTests</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">BinomialTest</span><span class="p">(</span><span class="mi">49_225</span><span class="p">,</span> <span class="mi">98_451</span><span class="p">,</span> <span class="mf">0.5036</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Binomial</span> <span class="n">test</span>
</span></span><span class="line"><span class="cl"><span class="o">-------------</span>
</span></span><span class="line"><span class="cl"><span class="n">Population</span> <span class="n">details</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameter</span> <span class="n">of</span> <span class="n">interest</span><span class="o">:</span>   <span class="n">Probability</span> <span class="n">of</span> <span class="n">success</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="n">under</span> <span class="n">h_0</span><span class="o">:</span>         <span class="mf">0.5036</span>
</span></span><span class="line"><span class="cl">    <span class="n">point</span> <span class="n">estimate</span><span class="o">:</span>          <span class="mf">0.499995</span>
</span></span><span class="line"><span class="cl">    <span class="mi">95</span><span class="o">%</span> <span class="n">confidence</span> <span class="n">interval</span><span class="o">:</span> <span class="p">(</span><span class="mf">0.4969</span><span class="p">,</span> <span class="mf">0.5031</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Test</span> <span class="n">summary</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">outcome</span> <span class="n">with</span> <span class="mi">95</span><span class="o">%</span> <span class="n">confidence</span><span class="o">:</span> <span class="n">reject</span> <span class="n">h_0</span>
</span></span><span class="line"><span class="cl">    <span class="n">two</span><span class="o">-</span><span class="n">sided</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="o">:</span>           <span class="mf">0.0239</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Details</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">number</span> <span class="n">of</span> <span class="n">observations</span><span class="o">:</span> <span class="mi">98451</span>
</span></span><span class="line"><span class="cl">    <span class="n">number</span> <span class="n">of</span> <span class="n">successes</span><span class="o">:</span>    <span class="mi">49225</span>
</span></span></code></pre></div><p>This is the two-sided test,
and I had to round $49,225.5$ to $49,225$
since <code>BinomialTest</code> do not support real numbers.
But the results match with the analytical solution,
we still reject the null.</p>
<h4 id="computational-solutions----bayesian-approach">Computational Solutions &ndash; Bayesian Approach</h4>
<p>Now, for the Bayesian computational approach,
I&rsquo;m going to use a generative modeling approach,
and one of my favorites probabilistic programming languages,
<code>Turing.jl</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">using</span> <span class="n">Turing</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@model</span> <span class="k">function</span> <span class="n">birth_rate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">           <span class="n">θ</span> <span class="o">~</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">           <span class="n">total_births</span> <span class="o">=</span> <span class="mi">98_451</span>
</span></span><span class="line"><span class="cl">           <span class="n">male_births</span> <span class="o">~</span> <span class="n">Binomial</span><span class="p">(</span><span class="n">total_births</span><span class="p">,</span> <span class="n">θ</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="k">end</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">birth_rate</span><span class="p">()</span> <span class="o">|</span> <span class="p">(;</span> <span class="n">male_births</span> <span class="o">=</span> <span class="mi">49_225</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(</span><span class="mi">1_000</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="mi">1_000</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Chains</span> <span class="n">MCMC</span> <span class="n">chain</span> <span class="p">(</span><span class="mi">1000</span><span class="o">×</span><span class="mi">13</span><span class="o">×</span><span class="mi">4</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">3</span><span class="p">})</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Iterations</span>        <span class="o">=</span> <span class="mi">1001</span><span class="o">:</span><span class="mi">1</span><span class="o">:</span><span class="mi">2000</span>
</span></span><span class="line"><span class="cl"><span class="kt">Number</span> <span class="n">of</span> <span class="n">chains</span>  <span class="o">=</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl"><span class="n">Samples</span> <span class="n">per</span> <span class="n">chain</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="n">Wall</span> <span class="n">duration</span>     <span class="o">=</span> <span class="mf">0.2</span> <span class="n">seconds</span>
</span></span><span class="line"><span class="cl"><span class="n">Compute</span> <span class="n">duration</span>  <span class="o">=</span> <span class="mf">0.19</span> <span class="n">seconds</span>
</span></span><span class="line"><span class="cl"><span class="n">parameters</span>        <span class="o">=</span> <span class="n">θ</span>
</span></span><span class="line"><span class="cl"><span class="n">internals</span>         <span class="o">=</span> <span class="n">lp</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">is_accept</span><span class="p">,</span> <span class="n">acceptance_rate</span><span class="p">,</span> <span class="n">log_density</span><span class="p">,</span> <span class="n">hamiltonian_energy</span><span class="p">,</span> <span class="n">hamiltonian_energy_error</span><span class="p">,</span> <span class="n">max_hamiltonian_energy_error</span><span class="p">,</span> <span class="n">tree_depth</span><span class="p">,</span> <span class="n">numerical_error</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">nom_step_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Summary</span> <span class="n">Statistics</span>
</span></span><span class="line"><span class="cl">  <span class="n">parameters</span>      <span class="n">mean</span>       <span class="n">std</span>      <span class="n">mcse</span>    <span class="n">ess_bulk</span>    <span class="n">ess_tail</span>      <span class="n">rhat</span>   <span class="n">ess_per_sec</span>
</span></span><span class="line"><span class="cl">      <span class="kt">Symbol</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>     <span class="kt">Float64</span>     <span class="kt">Float64</span>   <span class="kt">Float64</span>       <span class="kt">Float64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">θ</span>    <span class="mf">0.4999</span>    <span class="mf">0.0016</span>    <span class="mf">0.0000</span>   <span class="mf">1422.2028</span>   <span class="mf">2198.1987</span>    <span class="mf">1.0057</span>     <span class="mf">7368.9267</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Quantiles</span>
</span></span><span class="line"><span class="cl">  <span class="n">parameters</span>      <span class="mf">2.5</span><span class="o">%</span>     <span class="mf">25.0</span><span class="o">%</span>     <span class="mf">50.0</span><span class="o">%</span>     <span class="mf">75.0</span><span class="o">%</span>     <span class="mf">97.5</span><span class="o">%</span>
</span></span><span class="line"><span class="cl">      <span class="kt">Symbol</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">θ</span>    <span class="mf">0.4969</span>    <span class="mf">0.4988</span>    <span class="mf">0.4999</span>    <span class="mf">0.5011</span>    <span class="mf">0.5031</span>
</span></span></code></pre></div><p>We can see from the output of the quantiles that the 95% quantile for $\theta$ is
the interval $(0.4969, 0.5031)$.
Although it overlaps zero, that is not the equivalent of a hypothesis test.
For that, we&rsquo;ll use the
<a href="https://en.wikipedia.org/wiki/highest_posterior_density_interval">highest posterior density interval (HPDI)</a>,
which is defined as &ldquo;choosing the narrowest interval&rdquo; that
captures a certain posterior density threshold value.
In this case, we&rsquo;ll use a threshold interval of 95%,
i.e. an $\alpha = 0.05$:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">hpd</span><span class="p">(</span><span class="n">chain</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">HPD</span>
</span></span><span class="line"><span class="cl">  <span class="n">parameters</span>     <span class="n">lower</span>     <span class="n">upper</span>
</span></span><span class="line"><span class="cl">      <span class="kt">Symbol</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">θ</span>    <span class="mf">0.4970</span>    <span class="mf">0.5031</span>
</span></span></code></pre></div><p>We see that we fail to reject the null,
$\theta = 0.5$ at $\alpha = 0.05$ which is in accordance with the analytical
solution.</p>
<h2 id="why-the-frequentist-and-bayesian-approaches-disagree">Why the Frequentist and Bayesian Approaches Disagree</h2>
<p>Why do the approaches disagree?
What is going on under the hood?</p>
<p>The answer is disappointing<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.
The main problem is that the frequentist approach only allows fixed significance
levels with respect to sample size.
Whereas the Bayesian approach is consistent and robust to sample size variations.</p>
<p>Taken to extreme, in some cases, due to huge sample sizes,
the $p$-value is pretty much a <em>proxy</em> for sample size
and have little to no utility on hypothesis testing.
This is known as $p$-hacking<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
<h2 id="references">References</h2>
<p>Lindley, Dennis V. &ldquo;The future of statistics: A Bayesian 21st century&rdquo;.
<em>Advances in Applied Probability</em> 7 (1975): 106-115.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>as far as I know there&rsquo;s only one coherent approach to uncertainty,
and it is the Bayesian approach.
Otherwise, as de Finetti and Ramsey proposed,
you are susceptible to a <a href="https://en.wikipedia.org/wiki/Dutch_book">Dutch book</a>.
This is a topic for another blog post&hellip;&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Cromwell&rsquo;s rule states that the use of prior probabilities of 1
(&ldquo;the event will definitely occur&rdquo;) or 0 (&ldquo;the event will definitely not occur&rdquo;)
should be avoided, except when applied to statements that are logically true or false.
Hence, anything that is not a math theorem should have priors in $(0,1)$.
The reference comes from <a href="https://en.wikipedia.org/wiki/Oliver_Cromwell">Oliver Cromwell</a>,
asking, very politely, for the Church of Scotland to consider that their prior probability
might be wrong.
This footnote also deserves a whole blog post&hellip;&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy">Stigler&rsquo;s law of eponymy</a>
states that no scientific discovery is named after its original discoverer.
The paradox was already was discussed in <a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Harold Jeffreys</a>'
1939 textbook.
Also, fun fact, Stigler&rsquo;s is not the original creator of such law&hellip;
Now that&rsquo;s a self-referential paradox, and a broad version of the <a href="https://en.wikipedia.org/wiki/Halting_problem">Halting problem</a>,
which should earn its own footnote.
Nevertheless, we are getting into self-referential danger zone here with
footnotes&rsquo; of footnotes&rsquo; of footnotes&rsquo;&hellip;&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>this is called $p$-value and can be easily defined as
&ldquo;the probability of sampling data from a target population given that $H_0$
is true as the number of sampling procedures $\to \infty$&rdquo;.
Yes, it is not that intuitive, and it deserves not a blog post,
but a full curriculum to hammer it home.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>that is not true for most of the real-world problems.
For Bayesian approaches,
we need to run computational asymptotic exact approximations using a class
of methods called <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo (MCMC)</a>.
Furthermore, for some nasty problems we need to use different set of methods
called <a href="https://en.wikipedia.org/wiki/Variational_Inference">variational inference (VI)</a>
or <a href="https://en.wikipedia.org/wiki/Approximate_Bayesian_computation">approximate Bayesian computation (ABC)</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>if you are curious about how this approximation works,
check the backup slides of my
<a href="https://github.com/storopoli/Bayesian-Statistics">open access and open source graduate course on Bayesian statistics</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Bayes&rsquo; theorem is officially called Bayes-Price-Laplace theorem.
Bayes was trying to disprove David Hume&rsquo;s argument that miracles did not exist
(How dare he?).
He used the probabilistic approach of trying to quantify the probability of a parameter
(god exists) given data (miracles happened).
He died without publishing any of his ideas.
His wife probably freaked out when she saw the huge pile of notes that he had
and called his buddy Richard Price to figure out what to do with it.
Price struck gold and immediately noticed the relevance of Bayes&rsquo; findings.
He read it aloud at the Royal Society.
Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes,
used the same probabilistic approach to perform statistical inference using France&rsquo;s
first census data in the early-Napoleonic era.
Somehow we had the answer to statistical inference back then,
and we had to rediscover everything again in the late-20th century&hellip;&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>disappointing because most of
published scientific studies suffer from this flaw.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>and, like all footnotes here, it deserves its own blog post&hellip;&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content>
      </entry>

</feed>


