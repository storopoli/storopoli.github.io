<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cryptography on Jose Storopoli, PhD</title>
  <link rel="alternate" href="https://storopoli.io/tags/cryptography/" />
  <link rel="self" href="https://storopoli.io/tags/cryptography/index.xml" />
  <subtitle>Recent content in Cryptography on Jose Storopoli, PhD</subtitle>
  <id>https://storopoli.io/tags/cryptography/</id>
  <generator uri="http://gohugo.io" version="0.123.6">Hugo</generator>
  <language>en-us</language>
  <updated>2024-02-11T15:59:02Z</updated>
  <author>
    <name>Jose Storopoli</name>
    
  </author>
  <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)</rights>
      <entry>
        <title>Seed Phrases and Entropy</title>
        <link rel="alternate" href="https://storopoli.io/2024-02-11-mnemonic/" />
        <id>https://storopoli.io/2024-02-11-mnemonic/</id>
        <published>2024-02-11T15:59:02Z</published>
        <updated>2024-02-11T17:11:57Z</updated>
        <summary type="html">Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&amp;rsquo;ll have to unfortunately enable JavaScript.
In this post, let&amp;rsquo;s dive into a topic that is very important for anyone who uses the internet: passwords. We&amp;rsquo;ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin &amp;ldquo;seed phrases&amp;rdquo;1.
Entropy Before we go into passwords, I&amp;rsquo;ll introduce the concept of Entropy.</summary>
          <content type="html"><![CDATA[<p><img loading="lazy" src="password_strength.png#center" alt="Password Meme"  />
</p>
<blockquote>
<p>Warning: This post has <a href="https://katex.org/">KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p>
</blockquote>
<p>In this post, let&rsquo;s dive into a topic that is very important for anyone who uses the internet: <strong>passwords</strong>.
We&rsquo;ll cover what the hell is <strong>Entropy</strong>,
good <strong>password practices</strong>,
and how it relates to <strong>Bitcoin &ldquo;seed phrases&rdquo;</strong><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h2 id="entropy">Entropy</h2>
<p>Before we go into passwords,
I&rsquo;ll introduce the concept of <strong><em>Entropy</em></strong>.</p>
<p><a href="https://en.wikipedia.org/wiki/Entropy">Entropy</a>
is a measure of the <strong>amount of disorder in a system</strong>.
It has its origins in <strong>Thermodynamics</strong>,
where it&rsquo;s used to measure the amount of energy in a system that is not available to do work.</p>
<p>The etymology of the word &ldquo;Entropy&rdquo; is after the Greek word for &ldquo;transformation&rdquo;.</p>
<p>It was given a proper statistical definition by <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">Ludwig Boltzmann</a> in 1870s.
while establishing the field of <a href="https://en.wikipedia.org/wiki/Statistical_dynamics">Statistical Dynamics</a>,
a field of physics that studies the behavior of large collections of particles.</p>
<figure>
    <img loading="lazy" src="boltzmann.jpg#center"
         alt="Ludwig Boltzmann" width="300"/> <figcaption>
            Ludwig Boltzmann
        </figcaption>
</figure>

<p>In the context of Statistical Dynamics,
<strong>Entropy is a measure of the number of ways a system can be arranged</strong>.
The more ways a system can be arranged,
the higher its Entropy.
Specifically, <strong>Entropy is a logarithmic measure of the number of system states with significant probability of being occupied</strong>:</p>
<p>$$S = -k \cdot \sum_i p_i \ln p_i$$</p>
<p>Where:</p>
<ul>
<li>$S$: Entropy.</li>
<li>$k$: Boltzmann&rsquo;s constant, a physical constant that relates temperature to energy.</li>
<li>$p_i$: probability of the system being in state $i$.</li>
</ul>
<p>In this formula, if all states are equally likely,
i.e $p_i = \frac{1}{N}$,
where $N$ is the number of states,
then the entropy is maximized.
You can see this since a probability $p$ is a real number between 0 and 1,
and as $N$ approaches infinity,
the sum of the logarithms approaches negative infinity.
Then, multiplying by $-k$ yields positive infinity.</p>
<h3 id="how-the-hell-physics-came-to-passwords">How the hell Physics came to Passwords?</h3>
<p>There&rsquo;s once a great men called <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>,
who single-handedly founded the field of <a href="https://en.wikipedia.org/wiki/Information_theory"><strong>Information Theory</strong></a>,
invented the concept of a <a href="https://en.wikipedia.org/wiki/Bit"><strong>Bit</strong></a>,
and was the first to think about Boolean algebra in the context of electrical circuits.
He laid the foundation for the <a href="https://en.wikipedia.org/wiki/Digital_Revolution"><strong>Digital Revolution</strong></a>.</p>
<p>If you are happy using your smartphone, laptop, or any other digital device,
in you high speed fiber internet connection,
through a wireless router to send cats pictures to your friends,
then you should thank Claude Shannon.</p>
<figure>
    <img loading="lazy" src="shannon.jpg#center"
         alt="Claude Shannon" width="300"/> <figcaption>
            Claude Shannon
        </figcaption>
</figure>

<p>He was trying to find a formula to quantify the amount of information in a message.
He wanted three things:</p>
<ol>
<li>The measure should be a <strong>function of the probability of the message</strong>.
Messages that are more likely should have less information.</li>
<li>The measure should be <strong>additive</strong>.
The information in a message should be the sum of the information in its parts.</li>
<li>The measure should be <strong>continuous</strong>.
Small changes in the message should result in small changes in the measure.</li>
</ol>
<p>He pretty much found that the formula for Entropy in statistical mechanics
was a good measure of information.
He called it <em>Entropy</em> to honor Boltzmann&rsquo;s work.
To differentiate it from the Statistical Dynamics&rsquo; Entropy,
he changed the letter to $H$,
in honor of <a href="https://en.wikipedia.org/wiki/H-theorem">Boltzmann&rsquo;s $H$-theorem</a>.
So the formula for the Entropy of a message is:</p>
<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​)$$</p>
<p>Where:</p>
<ul>
<li>$X$: random discrete variable.</li>
<li>$H(X)$: Entropy of $X$</li>
<li>$P(x_i)$: probability of the random variable $X$ taking the value $x_i$.
Also known as the probability mass function (PMF) of the discrete random variable $X$.</li>
<li>$\log$: base 2 logarithm, to measure the Entropy in bits.</li>
</ul>
<p>In information theory,
the <strong>Entropy of a random variable is the average level of &ldquo;information&rdquo;, &ldquo;surprise&rdquo;,
or &ldquo;uncertainty&rdquo; inherent to the variable&rsquo;s possible outcomes</strong><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Let&rsquo;s take the simple example of a fair coin.
The Entropy of the random variable $X$ that represents the outcome of a fair coin flip is:</p>
<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = -\left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{2}\right) = 1 \text{ bit}$$</p>
<p>So the outcome of a fair coin flip has 1 bit of Entropy.
This means that the outcome of a fair coin flip has 1 bit of information,
or 1 bit of uncertainty.
Once the message is received,
that the coin flip was heads or tails,
the receiver has 1 bit of information about the outcome.</p>
<p>Alternatively, we only need 1 bit to encode the outcome of a fair coin flip.
Hence, there&rsquo;s a connection between Entropy, search space, and information.</p>
<p>Another good example is the outcome of a fair 6-sided die.
The Entropy of the random variable $X$ that represents the outcome of a fair 6-sided die is:</p>
<p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = - \sum_{i=1}^6\left(\frac{1}{6} * \log \frac{1}{6} \right) \approx 2.58 \text{ bits}$$</p>
<p>This means that the outcome of a fair 6-sided die has 2.58 bits of Entropy.
we need $\operatorname{ceil}(2.58) = 3$ bits to encode the outcome of a fair 6-sided die.</p>
<h3 id="entropy-and-passwords">Entropy and Passwords</h3>
<p>Ok now we come full circle.
Let&rsquo;s talk, finally, about passwords.</p>
<p>In the context of passwords, <strong>Entropy</strong> is a measure of how unpredictable a password is.
The higher the Entropy, the harder it is to guess the password.
The Entropy of a password is measured in bits,
and it&rsquo;s calculated using the formula:</p>
<p>$$H = L \cdot \log_2(N)$$</p>
<p>Where:</p>
<ul>
<li>$H$: Entropy in bits</li>
<li>$N$: number of possible characters in the password</li>
<li>$L$: length of the password</li>
<li>$\log_2$:​ (N) calculates how many bits are needed to represent each character from the set.</li>
</ul>
<p>For example,
if we have a password with 8 characters and each character can be any of the 26 lowercase letters,
the standard english alphabet,
the Entropy would be:</p>
<p>$$H = 8 \cdot \log_2(26) \approx 37.6 \text{ bits}$$</p>
<p>This means that an attacker would need to try $2^{37.6} \approx 2.01 \cdot 10^{11}$ combinations<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> to guess the password.</p>
<p>If the password were to include uppercase letters, numbers, and symbols
(let&rsquo;s assume 95 possible characters in total),
the Entropy for an 8-character password would be:</p>
<p>$$H = 8 \cdot \log_2(95) \approx 52.6 \text{ bits}$$</p>
<p>This means that an attacker would need to try $2^{52.6} \approx 6.8 \cdot 10^{15}$ combinations to guess the password.</p>
<p>This sounds a lot but it&rsquo;s not that much.</p>
<p>For the calculations below, we&rsquo;ll assume that the attacker now your dictionary set,
i.e. the set of characters you use to create your password,
and the password length.</p>
<p>If an attacker get a hold of an NVIDIA RTX 4090,
MSRP USD 1,599, which can do
<a href="https://www.tomshardware.com/news/rtx-4090-password-cracking-comparison">300 GH/s (300,000,000,000 hashes/second)</a>,
i.e. $3 \cdot 10^{11}$ hashes/second,
it would take:</p>
<ol>
<li>8-length lowercase-only password:</li>
</ol>
<p>$$\frac{2.01 \cdot 10^{11}}{3 \cdot 10^{11}} \approx 0.67 \text{ seconds}$$</p>
<ol>
<li>8-length password with uppercase letters, numbers, and symbols:</li>
</ol>
<p>$$\frac{6.8 \cdot 10^{15}}{3 \cdot 10^{11}} \approx 22114 \text{ seconds} \approx 6.14 \text{ hours}$$</p>
<p>So, the first password would be cracked in less than a second,
while the second would take a few hours.
This with just one 1.5k USD GPU.</p>
<h2 id="bitcoin-seed-phrases">Bitcoin Seed Phrases</h2>
<p>Now that we understand Entropy and how it relates to passwords,
let&rsquo;s talk about bitcoin seed phrases<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Remember that our private key is a big-fucking number?
If not, check my <a href="../2024-02-05-crypto-basics/">post on cryptographics basics</a>.</p>
<p><a href="https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki">BIP-39</a>
specifies how to use easy-to-remember seed phrases to store and recover
private keys.
The <a href="https://github.com/bitcoin/bips/blob/master/bip-0039/english.txt">wordlist</a>
adheres to the following principles:</p>
<ol>
<li><strong>smart selection of words</strong>:
the wordlist is created in such a way that it&rsquo;s enough to type the first four
letters to unambiguously identify the word.</li>
<li><strong>similar words avoided</strong>:
word pairs like &ldquo;build&rdquo; and &ldquo;built&rdquo;, &ldquo;woman&rdquo; and &ldquo;women&rdquo;, or &ldquo;quick&rdquo; and &ldquo;quickly&rdquo;
not only make remembering the sentence difficult but are also more error
prone and more difficult to guess.</li>
</ol>
<p>Here is a simple 7-word seed phrase: <code>brave sadness grocery churn wet mammal tube</code>.
Surprisingly enough, this badboy here gives you $77$ bits of Entropy,
while also being easy to remember.
This is due to the fact that the wordlist has 2048 words,
so each word gives you $\log_2(2048) = 11$ bits of Entropy<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>There&rsquo;s a minor caveat to cover here.
The last word in the seed phrase is a checksum,
which is used to verify that the phrase is valid.</p>
<p>So, if you have a 12-word seed phrase,
you have $11 \cdot 11 = 121$ bits of Entropy.
And for a 24-word seed phrase,
you have $23 \cdot 11 = 253$ bits of Entropy.</p>
<p>The National Institute of Standards and Technology (NIST) recommends a
<a href="https://crypto.stackexchange.com/a/87059">minimum of 112 bits of Entropy for all things cryptographic</a>.
And Bitcoin has a <a href="https://bitcoin.stackexchange.com/a/118929">minimum of 128 bits of Entropy</a>.</p>
<p>Depending on your threat model,
<a href="https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html">&ldquo;Assume that your adversary is capable of a trillion guesses per second&rdquo;</a>,
it can take a few years to crack a 121-bit Entropy seed phrase:</p>
<p>$$\frac{2^{121}}{10^{12}} \approx 2.66 \cdot 10^{24} \text{ seconds} \approx 3.08 \cdot 10^{19} \text{ days} \approx 8.43 \cdot 10^{16} \text{ years}$$</p>
<p>That&rsquo;s a lot of years.
Now for a 253-bit Entropy seed phrase:</p>
<p>$$\frac{2^{253}}{10^{12}} \approx 1.45 \cdot 10^{64} \text{ seconds} \approx 1.68 \cdot 10^{59} \text{ days} \approx 4.59 \cdot 10^{56} \text{ years}$$</p>
<p>That&rsquo;s another huge number of years.</p>
<h2 id="seed-phrases-and-passwords">Seed Phrases and Passwords</h2>
<p>You can also use a seed phrase as a password.
The bonus point is that you don&rsquo;t need to use the last word as a checksum,
so you get 11 bits of Entropy free, compared to a Bitcoin seed phrase.</p>
<p>Remember the 7-words badboy seed phrase we generated earlier?
<code>brave sadness grocery churn wet mammal tube</code>.</p>
<p>It has $66$ bits of Entropy.
This would take, assuming
<a href="https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html">&ldquo;that your adversary is capable of a trillion guesses per second&rdquo;</a>:</p>
<p>$$\frac{2^{77}}{10^{12}} \approx 1.51 \cdot 10^{11} \text{ seconds} \approx 1.75 \cdot 10^{6} \text{ days} \approx 4.79 \cdot 10^{3} \text{ years}$$</p>
<p>That&rsquo;s why tons of people use seed phrases as passwords.
Even if you know the dictionary set and the length of the password,
i.e. the number of words in the seed phrase,
it would take a lot of years to crack it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Entropy is a measure of the amount of disorder in a system.
In the context of passwords, it&rsquo;s a measure of how unpredictable a password is.
The higher the Entropy, the harder it is to guess the password.</p>
<p>Bitcoin seed phrases are a great way to store and recover private keys.
They are easy to remember and have a high amount of Entropy.
You can even use a seed phrase as a password.</p>
<p>Even it your attacker is capable of a trillion guesses per second,
like the <a href="https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html">NSA</a>,
it would take them a lot of years to crack even a 7-word seed phrase.</p>
<p>If you want to generate a seed phrase,
you can use <a href="https://keepassxc.org/">KeePassXC</a>,
which is a great open-source <strong><em>offline</em></strong> password manager that supports seed phrases<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>seed phrases are technically called &ldquo;mnemonic phrases&rdquo;,
but I&rsquo;ll use the term &ldquo;seed phrases&rdquo; for the rest of the post.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>there is a Bayesian argument about
the use of priors that should adhere to the
<a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Principle of Maximal Entropy</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>technically, we need to divide the number of combinations by 2,
since we are assuming that the attacker is using a brute-force attack,
which means that the attacker is trying all possible combinations,
and the password could be at the beginning or at the end of the search space.
This is called the <a href="https://en.wikipedia.org/wiki/Birthday_problem">birthday paradox</a>,
and it assumes that the password is uniformly distributed in the search space.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>remember that $2^{11} = 2048$.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>technically, KeePassXC uses the <a href="https://www.eff.org/files/2016/07/18/eff_large_wordlist.txt">EFF wordlist</a>,
which has 7,776 words, so each word gives you $\log_2(7776) \approx 12.9$ bits of Entropy.
They were created to be easy to use with 6-sided dice.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content>
      </entry>
      <entry>
        <title>Cryptography Basics</title>
        <link rel="alternate" href="https://storopoli.io/2024-02-05-crypto-basics/" />
        <id>https://storopoli.io/2024-02-05-crypto-basics/</id>
        <published>2024-02-05T18:53:28-03:00</published>
        <updated>2024-02-11T15:55:02-03:00</updated>
        <summary type="html">Euclid&amp;rsquo;s one-way function
Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&amp;rsquo;ll have to unfortunately enable JavaScript.
This is the companion post to the cryptography workshop that I gave at a local BitDevs. Let&amp;rsquo;s explore the basics of cryptography. We&amp;rsquo;ll go through the following topics:
One-way functions Hash functions Public-key cryptography DSA Schnorr Why we don&amp;rsquo;t reuse nonces? Why we can combine Schnorr Signatures and not DSA?</summary>
          <content type="html"><![CDATA[<figure>
    <img loading="lazy" src="euclid.webp"
         alt="Euclid&#39;s one-way function"/> <figcaption>
            <p>Euclid&rsquo;s one-way function</p>
        </figcaption>
</figure>

<blockquote>
<p>Warning: This post has <a href="https://katex.org/">KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p>
</blockquote>
<p>This is the companion post to the <a href="https://github.com/storopoli/cryptography-workshop">cryptography workshop</a>
that I gave at a local BitDevs.
Let&rsquo;s explore the basics of cryptography.
We&rsquo;ll go through the following topics:</p>
<ul>
<li>One-way functions</li>
<li>Hash functions</li>
<li>Public-key cryptography</li>
<li>DSA</li>
<li>Schnorr</li>
<li>Why we don&rsquo;t reuse nonces?</li>
<li>Why we can combine Schnorr Signatures and not DSA?</li>
</ul>
<h2 id="one-way-functions">One-way functions</h2>
<p>A one-way function is a <strong>function that is easy to compute on every input,
but hard to invert given the image<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> of a random input</strong>.
For example, imagine an omelet.
It&rsquo;s easy to make an omelet from eggs,
but it&rsquo;s hard to make eggs from an omelet.
In a sense we can say that the function $\text{omelet}$ is a one-way function</p>
<p>$$\text{omelet}^{-1}(x) = \ldots$$</p>
<p>That is, we don&rsquo;t know how to invert the function $\text{omelet}$ to get the original eggs back.
Or, even better, <strong>the benefit we get from reverting the omelet to eggs is not worth the effort,
either in time or money</strong>.</p>
<p>Not all functions are one-way functions.
The exponential function, $f(x) = e^x$, is not a one-way function.
It is easy to undo the exponential function by taking the natural logarithm,</p>
<p>$$f^{-1}(x) = \ln(x)$$</p>
<p>To showcase one-way functions, let&rsquo;s take a look at the following example.
Let&rsquo;s play around with some numbers.
Not any kind of numbers, but very special numbers called <strong>primes</strong>.
A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.</p>
<p>If I give you a big number $n$ and ask you to find its prime factors,
and point a gun at your head,
you&rsquo;ll pretty much screwed.
There&rsquo;s no known efficient algorithm<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> to factorize a big number into its prime factors.
You&rsquo;ll be forced to test all numbers from 2 to $\sqrt{n}$ to see if they divide $n$.</p>
<p>Here&rsquo;s a number:</p>
<p>$$90809$$</p>
<p>What are its prime factors?
It&rsquo;s $1279 \cdot 71$.
Easy to check, right?
Hard to find.
That&rsquo;s because prime factorization, if you choose a fucking big number, is a one-way function.</p>
<h2 id="hash-functions">Hash Functions</h2>
<p>Let&rsquo;s spice things up.
There is a special class of one-way functions called <strong>hash functions</strong>.</p>
<p><strong>A hash function is any function that can be used to map data of arbitrary size to fixed-size values</strong>.</p>
<p>But we are most interested in <strong><em>cryptographic</em> hash functions</strong>,
which are hash functions that have statistical properties desirable for cryptographic application:</p>
<ul>
<li><strong>One-way function</strong>: easy to compute $y = f(x)$, hard as fuck to do the opposite, $x = f^{-1}(y)$.</li>
<li><strong>Deterministic</strong>: given a function that maps elements from set $X$ to set $Y$, $f: X \to Y$,
for every $x \in X$ there&rsquo;s <em>at least one</em> $y \in Y$<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.
This means that if I give you a certain input, it will always map to the same output.
It is deterministic.</li>
<li><strong>Collision resistance</strong>: the possible values of $f: X \to Y$ follows a uniform distribution,
that is, given the size of the set $Y$,
it is hard to find two $x_1, x_2 \in X$ that have the same $y \in Y$ value<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.
This property is really important because if an attacker wants to brute-force the
hash function, there&rsquo;s no option than searching uniformly across the whole possible
space of possible values that the hash function outputs.</li>
</ul>
<p>These properties make enable cryptographic hash functions to be used in a wide range of applications,
including but not limited to:</p>
<ul>
<li>
<p><strong>Digital signatures</strong>: Hash functions are used to create a digest of the message to be signed.
The digital signature is then generated using the hash, rather than the message itself,
to ensure integrity and non-repudiation.</p>
</li>
<li>
<p><strong>Password hashing</strong>: Storing passwords as hash values instead of plain text.
Even if the hash values are exposed,
the original passwords remain secure due to the pre-image resistance property.</p>
</li>
<li>
<p><strong>Blockchain and cryptocurrency</strong>: Hash functions are used to maintain the integrity of the blockchain.
Each block contains the hash of the previous block, creating a secure link.
Cryptographic hashes also underpin various aspects of cryptocurrency transactions.</p>
</li>
<li>
<p><strong>Data integrity verification</strong>: Hash functions are used to ensure that files, messages,
or data blocks have not been altered.
By comparing hash values computed before and after transmission or storage,
any changes in the data can be detected.</p>
</li>
</ul>
<p>We&rsquo;ll cover just the <strong>digital signatures</strong> part in this post.</p>
<h3 id="sha-2-and-its-variants">SHA-2 and its variants</h3>
<p>The Secure Hash Algorithm 2 (SHA-2) is a set of cryptographic hash functions designed by the National Security Agency (NSA).
It was first published in 2001.</p>
<p>It is composed of six hash functions with digests that are 224, 256, 384, 512, 512/224, and 512/256 bits long:</p>
<ul>
<li><code>SHA-224</code></li>
<li><code>SHA-256</code></li>
<li><code>SHA-384</code></li>
<li><code>SHA-512</code></li>
<li><code>SHA-512/224</code></li>
<li><code>SHA-512/256</code></li>
</ul>
<p>Amongst these, let&rsquo;s focus on SHA-256, which is the most widely used while also being notoriously adopted by bitcoin.</p>
<p>SHA-256 does not have any known vulnerabilities and is considered secure.
It comprises of 32-bit words and operates on 64-byte blocks.
The algorithm does 64 rounds of the following operations:</p>
<ul>
<li><code>AND</code>: bitwise boolean AND</li>
<li><code>XOR</code>: bitwise boolean XOR</li>
<li><code>OR</code>: bitwise boolean OR</li>
<li><code>ROT</code>: right rotation bit shift</li>
<li><code>ADD</code>: addition modulo $2^{32}$</li>
</ul>
<p>You can check <a href="https://en.wikipedia.org/wiki/SHA-2#Pseudocode">SHA-256 Pseudocode on Wikipedia</a>.
It really scrambles the input message in a way that is very hard to reverse.</p>
<p>These operations are non-linear and very difficult to keep track of.
In other words, you can&rsquo;t reverse-engineer the hash to find the original message.
There&rsquo;s no <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">&ldquo;autodiff&rdquo;</a> for hash functions.</p>
<p>Since it is a cryptographic hash function,
if we change just one bit of the input,
the output will be completely different.
Check this example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="s2">&#34;The quick brown fox jumps over the lazy dog&#34;</span> <span class="p">|</span> shasum -a <span class="m">256</span>   
</span></span><span class="line"><span class="cl">c03905fcdab297513a620ec81ed46ca44ddb62d41cbbd83eb4a5a3592be26a69  -
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="s2">&#34;The quick brown fox jumps over the lazy dog.&#34;</span> <span class="p">|</span> shasum -a <span class="m">256</span>
</span></span><span class="line"><span class="cl">b47cc0f104b62d4c7c30bcd68fd8e67613e287dc4ad8c310ef10cbadea9c4380  -
</span></span></code></pre></div><p>Here we are only adding a period at the end of the sentence,
and the hash is completely different.
This is due to the property of collision resistance that we mentioned earlier.</p>
<h2 id="fields">Fields</h2>
<p>Before we dive into public-key cryptography,
we need a brief interlude on fields.</p>
<p><strong><a href="https://en.wikipedia.org/wiki/Field_(mathematics)">Fields</a> are sets with two binary operations,
called addition $+$ and multiplication $\times$</strong>.
We write</p>
<p>$$F = (F, +, \times)$$</p>
<p>to denote a field,
where $F$ is the set, $+$ is the addition operation,
and $\times$ is the multiplication operation.</p>
<p>Addition and multiplication behave similar to the addition and multiplication of real numbers.
For example, addition is <strong>commutative</strong> and <strong>associative</strong></p>
<p>$$a + b = b + a,$$</p>
<p>and multiplication is <strong>distributive</strong></p>
<p>$$a \times (b + c) = a \times b + a \times c.$$</p>
<p>Also, there are two special elements in the field,
called the <strong>additive identity</strong> $-a$ and the <strong>multiplicative identity</strong> $a^{-1}$,
such that</p>
<p>$$a + (-a) = I,$$</p>
<p>and</p>
<p>$$a \times a^{-1} = I,$$</p>
<p>where $I$ is the identity element.</p>
<p>Note that this allows us to define <strong>subtraction</strong></p>
<p>$$a - b = a + (-b),$$</p>
<p>and <strong>division</strong></p>
<p>$$a \div b = a \times b^{-1}.$$</p>
<h3 id="finite-fields">Finite Fields</h3>
<p>Now we are ready for finite fields.
A <a href="https://en.wikipedia.org/wiki/Finite_field"><em>finite field</em></a>, also called a Galois field (in honor of Évariste Galois),
is a <strong>field with a finite number of elements.
As with any field, a finite field is a set on which the operations of multiplication,
addition, subtraction and division are defined and satisfy the rules above for fields</strong>.</p>
<p>Finite fields is a very rich topic in mathematics,
and there are many ways to construct them.
The easiest way to construct a finite field is to take the <strong>integers modulo a prime number $p$</strong>.
For example $\mathbb{Z}_5$ is a finite field with 5 elements:</p>
<p>$$\mathbb{Z}_5 = \lbrace 0, 1, 2, 3, 4 \rbrace.$$</p>
<p>In general, $\mathbb{Z}_n$ is a finite field with $n$ elements:</p>
<p>$$\mathbb{Z}_n = \lbrace 0, 1, 2, \ldots, n - 1 \rbrace.$$</p>
<p><strong>The number of elements in a finite field is called the <em>order</em> of the field</strong>.
The order of a finite field is <strong>always a prime number $p$</strong>.
The $\mathbb{Z}_5$ example above is a finite field of order 5.
However, $\mathbb{Z}_4$ is not a finite field,
because 4 is not a prime number, but rather a composite number.</p>
<p>$$4 = 2 \times 2.$$</p>
<p>And we can write $\mathbb{Z}_4$ as</p>
<p>$$\mathbb{Z}_4 = 2 \times \mathbb{Z}_2.$$</p>
<p>This means that every element in $a \in \mathbb{Z}_4$ can be written as</p>
<p>$$a = 2 \times b,$$</p>
<p>where $b$ is an element in $\mathbb{Z}_2$.</p>
<p>Hence, not every element of $\mathbb{Z}_4$ is unique, and they are equivalent to the elements in $\mathbb{Z}_2$.</p>
<p>In general if $n$ is a composite number,
then $\mathbb{Z}_n$ is not a finite field.
However, if $n = r \times s$ where $r$ and $s$ are prime numbers,
and $r &lt; s$,
then $\mathbb{Z}_n$ is a finite field of order $r$.</p>
<h4 id="operations-in-finite-fields">Operations in Finite Fields</h4>
<p><strong>Addition</strong> in finite fields is defined as the remainder of the sum of two elements modulo the order of the
field.</p>
<p>For example, in $\mathbb{Z}_3$,</p>
<p>$$1 + 2 = 3 \mod 3 = 0.$$</p>
<p>We can also define subtraction in finite fields as the remainder of the difference of two elements modulo the order of the field.</p>
<p>For example, in $\mathbb{Z}_3$,</p>
<p>$$1 - 2 = -1 \mod 3 = 2.$$</p>
<p>Multiplication in finite fields can be written as multiple additions.
For example, in $\mathbb{Z}_3$,</p>
<p>$$2 \times 2 = 2 + 2 = 4 \mod 3 = 1.$$</p>
<p>Exponentiation in finite fields can be written as multiple multiplications.
For example, in $\mathbb{Z}_3$,</p>
<p>$$2^2 = 2 \times 2 = 4 \mod 3 = 1.$$</p>
<p>As you can see addition, subtraction, and multiplication becomes linear operations.
This is very trivial for any finite field.</p>
<p>However, for division we are pretty much screwed.
It is really hard to find the multiplicative inverse of an element in a finite field.
For example, suppose that we have numbers $a,b$ in a very large finite field $\mathbb{Z}_p$,
such that</p>
<p>$$c = a \times b \mod p.$$</p>
<p>Then we can write division as</p>
<p>$$a = c \div b = c \times b^{-1} \mod p.$$</p>
<p>Now we need to find $b^{-1}$, which is the multiplicative inverse of $b$.
This is called the <a href="https://en.wikipedia.org/wiki/Discrete_logarithm"><strong><em>discrete logarithm problem</em></strong></a>.
Because we need to find $b^{-1}$ such that</p>
<p>$$b^{-1} = \log_b c \mod p.$$</p>
<p>Since this number is a discrete number and not a real number,
that&rsquo;s why it&rsquo;s called the discrete logarithm problem.</p>
<p>Good luck my friend, no efficient method is known for computing them in general.
You can try brute force, but that&rsquo;s not efficient.</p>
<h4 id="why-the-discrete-logarithm-problem-is-hard-as-fuck">Why the Discrete Logarithm Problem is Hard as Fuck</h4>
<p>To get a feeling why the discrete logarithm problem is difficult,
let&rsquo;s add one more concept to our bag of knowledge.
Every finite field has <em><strong>generators</strong></em>,
also known as <em><strong>primitive roots</strong></em>,
which is also a member of the group,
such that applying multiplication to this one single element
makes possible to generate the whole finite field.</p>
<p>Let&rsquo;s illustrate this with an example.
Below we have a table of all the results of the following operation</p>
<p>$$b^x \mod 7$$</p>
<p>for every possible value of $x$.
As you&rsquo;ve guessed right this is the $\mathbb{Z}_7$ finite field.</p>
<table>
<thead>
<tr>
<th style="text-align:center">$b$</th>
<th style="text-align:center">$b^1 \mod 7$</th>
<th style="text-align:center">$b^2 \mod 7$</th>
<th style="text-align:center">$b^3 \mod 7$</th>
<th style="text-align:center">$b^4 \mod 7$</th>
<th style="text-align:center">$b^5 \mod 7$</th>
<th style="text-align:center">$b^6 \mod 7$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
</tr>
<tr>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$1$</td>
</tr>
<tr>
<td style="text-align:center">$3$</td>
<td style="text-align:center">$3$</td>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$6$</td>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$5$</td>
<td style="text-align:center">$1$</td>
</tr>
<tr>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$1$</td>
</tr>
<tr>
<td style="text-align:center">$5$</td>
<td style="text-align:center">$5$</td>
<td style="text-align:center">$4$</td>
<td style="text-align:center">$6$</td>
<td style="text-align:center">$2$</td>
<td style="text-align:center">$3$</td>
<td style="text-align:center">$1$</td>
</tr>
<tr>
<td style="text-align:center">$6$</td>
<td style="text-align:center">$6$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$6$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
<td style="text-align:center">$1$</td>
</tr>
</tbody>
</table>
<p>You see that something interesting is happening here.
For specific values of $b$, such as $b = 3$, and $b = 5$, we are able to <strong>generate the whole finite field</strong>.
Hence, say that $3$ and $5$ are <em><strong>generators</strong></em> or <em><strong>primitive roots</strong></em> of $\mathbb{Z}_7$.</p>
<p>Now suppose I ask you to find $x$ in the following equation</p>
<p>$$3^x \mod p = 11$$</p>
<p>where $p$ is a very large prime number.
Then you don&rsquo;t have any other option than brute forcing it.
<strong>You&rsquo;ll need to try each exponent $x \in \mathbb{Z}_p$ until you find the one that satisfies the equation</strong>.</p>
<p>Notice that this operation is very asymmetric.
It is very easy to compute $3^x \mod p$ for any $x$,
but it is very hard to find $x$ given $3^x \mod p$.</p>
<p>Now we are ready to dive into public-key cryptography.</p>
<h4 id="numerical-example-of-the-discrete-logarithm-problem">Numerical Example of the Discrete Logarithm Problem</h4>
<p>Let&rsquo;s illustrate the discrete logarithm problem with a numerical example.</p>
<ol>
<li><strong>Choose a prime number $p$</strong>. Let&rsquo;s pick $p = 17$.</li>
<li><strong>Choose a generator $g$ of the group</strong>.
For $p = 17$, we can choose $g = 3$ because $3$ is a primitive root of $\mathbb{Z}_{17}$.</li>
<li><strong>Choose an element $x$</strong>.
Let&rsquo;s pick $x = 15$.</li>
</ol>
<p>The discrete logarithm problem is to find $x$ given $g^x \mod p$.
So let&rsquo;s plug in the numbers; find $x$ in</p>
<p>$$3^x = 15 \mod 17 $$</p>
<p>Try to find it.
Good luck<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<h2 id="public-key-cryptography">Public-key cryptography</h2>
<p>Public-key cryptography, or asymmetric cryptography, is a cryptographic system that uses pairs of keys:
private and public.
The public key you can share with anyone,
but the private key you must keep secret.
The keys are related mathematically,
but it is computationally infeasible to derive the private key from the public key.
In other words, the public key is a one-way function of the private key.</p>
<p>Before we dive into the details of the public-key cryptography, and signing and verifying messages,
let me introduce some notation:</p>
<ul>
<li>$p$: big fucking huge prime number (4096 bits or more)</li>
<li>$\mathbb{Z}_p$: the finite field of order $p$</li>
<li>$g$: a generator of $\mathbb{Z}_p$</li>
<li>$S_k$: secret key, a random integer in the finite field $\mathbb{Z}_p$</li>
<li>$P_k$: public key derived by $P_k = g^{S_k}$</li>
</ul>
<p>If you know $S_k$ and $g$ (which is almost always part of the spec),
then it&rsquo;s easy to derive the $P_k$.
However, if you only know $g$ and $P_k$, good luck finding $S_k$.
It&rsquo;s the discrete log problem again.
And as long $p$ is HUGE you are pretty confident that no one will find your secret key
from your public key.</p>
<p>Now what we can do with these keys and big prime numbers?
We&rsquo;ll we can sign a message with our secret key and everyone can verify the authenticity of
the message using our public key.
The message in our case it is commonly a hash function of the &ldquo;original message&rdquo;.
Due to the collision resistance property, we can definitely assert that:</p>
<ol>
<li>the message has not been altered</li>
<li>the message was signed by the owner of the private key</li>
</ol>
<p>Fun fact, I once gave a recommendation letter to a very bright student,
that was only a plain text file signed with my private key.
I could rest assured that the letter was not altered,
and the student and other people could verify that I was the author of the letter.</p>
<p>Next, we&rsquo;ll dive into the details of the Digital Signature Algorithm (DSA)
and the Schnorr signature algorithm.</p>
<h2 id="dsa">DSA</h2>
<p>DSA stands for <a href="https://en.wikipedia.org/wiki/Digital_Signature_Algorithm">Digital Signature Algorithm</a>.
It was first proposed by the National Institute of Standards and Technology (NIST) in 1991.
Note that <a href="https://lwn.net/Articles/958048/">OpenSSH announced that DSA is scheduled for removal in 2025</a>.</p>
<p>Here&rsquo;s how you can sign a message using DSA:</p>
<ol>
<li>Choose two prime numbers $p, q$ such that $p - 1 \mod q = 0$ (e.g., 1279 and 71).</li>
<li>Choose your private key $S_k$ as a random integer $\in [1, q-1]$.</li>
<li>Choose a generator $g$.</li>
<li>Compute your public key $P_k$: $g^{S_k} \mod p$.</li>
<li>Choose your nonce $k$: as a random integer $\in [1, q-1]$.</li>
<li>Compute your &ldquo;public nonce&rdquo; $K$: $(g^k \mod p) \mod q$ (also known as $r$).</li>
<li>Get your message ($m$) through a cryptographic hash function $H$: $H(m)$.</li>
<li>Compute your signature $s$: $(k^{-1} (H(m) + S_k K)) \mod q$.</li>
<li>Send to your buddy $(p, q, g)$, $P_k$, and $(K, s)$.</li>
</ol>
<p>And here&rsquo;s how you can verify the signature:</p>
<ol>
<li>Compute $w = s^{-1} \mod q$.</li>
<li>Compute $u_1 = H{m} \cdot w \mod q$.</li>
<li>Compute $u_2 = K \cdot w \mod q$.</li>
<li>Compute $K^* = {g^{u_1} P^{u_2}_k \mod p} \mod q$.</li>
<li>Assert $K = K^*$.</li>
</ol>
<p>How this works?
Let&rsquo;s go through a proof of correctness.
I added some comments to every operation in parentheses to make it easier to follow.</p>
<ol>
<li>$s = k^{-1} \cdot {H + S_k K} \mod q$ ($\mod p$ and $H(m)$ implicit).</li>
<li>$k = s^{-1} \cdot {H + S_k K} \mod q$ (move $s$ to $k$).</li>
<li>$k = H \cdot s^{-1} + S_k K \cdot s^{-1} \mod q$ (distribute $s^{-1}$).</li>
<li>$k = H \cdot w + S_k K \cdot w \mod q$ ($w = s^{-1}$).</li>
<li>$g^k = g^{H \cdot w + S_k K \cdot w \mod q}$ (put $g$ in both sides).</li>
<li>$g^k = g^{H \cdot w \mod q} \cdot g^{S_k K \cdot w \mod q}$ (product of the exponents).</li>
<li>$g^k = g^{H \cdot w \mod q} \cdot P^{K \cdot w \mod q}_k$ ($P_k = g^{S_k}$).</li>
<li>$g^k = g^{u_1} \cdot P^{u_2}_k$ (replace $u_1$ and $u_2$).</li>
<li>$K = K^*$ (replace $K$ and $K^*$).</li>
</ol>
<p>There you go.
This attest that the signature is correct and the message was signed by the owner of the private key.</p>
<h2 id="schnorr">Schnorr</h2>
<p><a href="https://en.wikipedia.org/wiki/Schnorr_signature">Schnorr signature algorithm</a>
is a very similar algorithm to DSA.
It was proposed by Claus-Peter Schnorr in 1989.
It is considered to be more secure than DSA and is also more efficient.
The patent for Schnorr signatures expired in 2008,
just in time for Satoshi to include it in Bitcoin.
However, it was probably not included due to the fact that there wasn&rsquo;t
good battle-tested software implementations of it at the time.
However, it was added to Bitcoin in the Taproot upgrade<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<p>Schnorr is a marvelous algorithm.
It is so much simpler than DSA.
Here&rsquo;s how you sign a message using Schnorr:</p>
<ol>
<li>Choose a prime number $p$.</li>
<li>Choose your private key $S_k$ as a random integer $\in [1, p-1]$.</li>
<li>Choose a generator $g$.</li>
<li>Compute your public key $P_k$: $g^{S_k}$.</li>
<li>Choose your nonce $k$: as a random integer $\in [1, p-1]$.</li>
<li>Compute your &ldquo;public nonce&rdquo; $K$: $g^k \mod p$ (also known as $r$).</li>
<li>Get your message ($m$) through a cryptographic hash function $H$ concatenating with $K$: $e = H(K || m)$.</li>
<li>Compute your signature $s$: $k - S_k e$.</li>
<li>Send to your buddy $(p, g)$, $P_k$, and $(K, s)$.</li>
</ol>
<p>And here&rsquo;s how you can verify the signature:</p>
<ol>
<li>Compute $e = H(K || m)$.</li>
<li>Compute $K^* = g^s P_k^e$.</li>
<li>Compute $e^* = H(K^* || m)$.</li>
<li>Assert $e = e^*$.</li>
</ol>
<p>How this works?
Let&rsquo;s go through a proof of correctness.
As before, I added some comments to every operation in parentheses to make it easier to follow.</p>
<ol>
<li>$K^* = g^s P_k^e$ ($\mod p$ implicit).</li>
<li>$K^* = g^{k - S_k e} g^{S_k e}$ ($s = k - S_k e$ and $P_k = g^{S_k}$).</li>
<li>$K^* = g^k$ (cancel $S_k e$ in the exponent of $g$).</li>
<li>$K^* = K$ ($K = g^k$).</li>
<li>Hence $H(K^* || m) = H(K || m)$.</li>
</ol>
<p>There you go.
This attest that the signature is correct and the message was signed by the owner of the private key.</p>
<h2 id="why-we-dont-reuse-nonces">Why we don&rsquo;t reuse nonces?</h2>
<p>Never, ever, reuse a nonce.
Why?
First, because nonce is short for &ldquo;number used once&rdquo;.
It is supposed to be used only once.
Because if you reuse a nonce, then you are pretty much screwed.
An attacker can derive your private key from two signatures with the same nonce.
This is called the &ldquo;nonce reuse attack&rdquo;.</p>
<p>Fun fact: this is what happened to the
<a href="https://en.wikipedia.org/wiki/PlayStation_3_homebrew#Private_key_compromised">PlayStation 3</a>.</p>
<p>Let&rsquo;s see how we can derive the private key from two signatures with the same nonce.
Here we are in a context that we have two signatures $s$ and $s^\prime$,
both using the same nonce $k = k^\prime$.</p>
<p>First, let&rsquo;s do the <del>ugly</del> DSA math:</p>
<p>$$\begin{aligned}
s^\prime - s &amp;= (k^{\prime {-1}} (H(m_1) + S_k K&rsquo;)) - (k^{-1} (H(m_2) + S_k K)) \\
s^\prime - s &amp;= k^{-1} (H(m_1) - H(m_2)) \\
k &amp;= (H(m_1) - H(m_2)) (s^\prime - s)^{-1}
\end{aligned}$$</p>
<p>Now remember you know $s$, $s^\prime$, $H(m_1)$, $H(m_2)$ $K$, and $K^\prime$.
Let&rsquo;s do the final step and solve for $S_k$:</p>
<p>$$S_k = K^{-1}  (k s - H(m_1))$$</p>
<p>Now let&rsquo;s do the Schnorr math.
But in Schnorr, everything is simpler.
Even nonce reuse attacks.</p>
<p>$$s^\prime - s = (k^\prime - k) - S_k (e^\prime - e)$$</p>
<p>If $k^\prime = k$ (nonce reuse) then you can easily isolate $S_k$ with simple algebra.</p>
<p>Remember: you know $s^\prime, s, e, e^\prime$ and $k^\prime - k = 0$.</p>
<h2 id="why-we-can-combine-schnorr-signatures-and-not-dsa">Why we can combine Schnorr Signatures and not DSA?</h2>
<p>In Bitcoin, we can combine Schnorr signatures and not DSA.
Why?
Because Schnorr signatures are linear.
This means that you can add two Schnorr signatures and get a valid signature for the sum of the messages.
This is not possible with DSA.
This is called the &ldquo;linearity property&rdquo; of Schnorr signatures.</p>
<p>Remember that in $Z_p$ addition, multiplication, and exponentiation,
i.e anything with $+, \cdot, -$, are linear operations
However, division (modular inverse),
.i.e anything that is $^{-1}$, is not linear.
That is:</p>
<p>$$x^{-1} + y^{-1} != (x + y)^{-1}.$$</p>
<p>Here&rsquo;s a trivial python code that shows that modular inverse is not linear:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">71</span><span class="p">;</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">13</span><span class="p">;</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">17</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="nb">pow</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">==</span> <span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="kc">False</span>
</span></span></code></pre></div><p>Let&rsquo;s revisit the signature step of DSA and Schnorr:</p>
<ul>
<li>DSA: $s = k^{-1} (H(m) + S_k K)$</li>
<li>Schnorr: $s = k - S_k H(K || m)$</li>
</ul>
<p>So if you have two Schnorr signatures $s_1$ and $s_2$ for two messages $m_1$ and $m_2$,
then you can easily compute a valid signature for the sum of the messages $m_1 + m_2$:</p>
<p>$$s = s_1 + s_2$$</p>
<p>Also note that we can combine Schnorr public keys:</p>
<p>$$P^\prime_k + P_k = g^{S^\prime_k} + g^{S_k} = g^{S_k^\prime + S_k}$$</p>
<p>And the signature $s$ for the sum of the messages $m_1 + m_2$
can be verified with the public key $P^\prime_k + P_k$.</p>
<p>This is not possible with DSA.</p>
<p>Because the signature step in DSA is not linear,
it has a $k^{-1}$ in it.</p>
<h2 id="technical-interlude-elliptic-curves">Technical Interlude: Elliptic Curves</h2>
<p>Technically speaking, Bitcoin uses the Elliptic Curve Digital Signature Algorithm (ECDSA),
and the Schnorr signature algorithm is based on the same elliptic curve (EC) as ECDSA.</p>
<p>And trivially speaking EC public-key cryptography in the end is just a finite field
on $\mathbb{Z}_p$.
It has everything that we&rsquo;ve seen so far:</p>
<ul>
<li>Addition</li>
<li>Subtraction</li>
<li>Multiplication</li>
<li>Division</li>
<li>Exponentiation</li>
<li>Generators</li>
<li>Discrete Logarithm Problem</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>I hope you enjoyed this companion post to the
<a href="https://github.com/storopoli/cryptography-workshop">cryptography workshop</a>.
Remember don&rsquo;t reuse nonces.</p>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>the image of a function $f$ is the set of all values that $f$ may produce.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>the problem of factoring a number into its prime factors is not known to be in
the class of problems that can be solved in polynomial time, P.
It is not known to be NP-complete, NP, either.
Actually to find it P is NP or not is the hardest way to earn a million dollars,
<a href="https://en.m.wikipedia.org/wiki/Millennium_Prize_Problems#P_versus_NP">the P vs NP problem</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>this is called <a href="https://en.wikipedia.org/wiki/Bijection%2C_injection_and_surjection">surjection</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>at least $\frac{1}{N}$ where $N$ is the size of $Y$.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>The answer is $x = 6$. This means that $3^6 = 15 \mod 17$.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Taproot is a proposed Bitcoin protocol upgrade that was deployed
as a forward-compatible soft fork.
The validation of Taproot is based on Schnorr signatures.
You can find more in BIPS
<a href="https://github.com/bitcoin/bips/blob/master/bip-0340.mediawiki">340</a>,
<a href="https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki">341</a>, and
<a href="https://github.com/bitcoin/bips/blob/master/bip-0342.mediawiki">342</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content>
      </entry>

</feed>


