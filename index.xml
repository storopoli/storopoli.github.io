<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jose Storopoli, PhD</title>
    <link>https://storopoli.io/</link>
    <description>Recent content on Jose Storopoli, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)</copyright>
    <lastBuildDate>Wed, 22 Nov 2023 07:06:59 -0300</lastBuildDate><atom:link href="https://storopoli.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lindley&#39;s Paradox, or The consistency of Bayesian Thinking</title>
      <link>https://storopoli.io/2023-11-23-lindley_paradox/</link>
      <pubDate>Wed, 22 Nov 2023 07:06:59 -0300</pubDate>
      
      <guid>https://storopoli.io/2023-11-23-lindley_paradox/</guid>
      <description>Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&amp;rsquo;ll have to unfortunately enable JavaScript.
Dennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: &amp;ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&amp;rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell&amp;rsquo;s Rule2.</description>
      <content:encoded><![CDATA[<blockquote>
<p>Warning: This post has <a href="https://katex.org/">KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p>
</blockquote>
<p><img loading="lazy" src="lindley.jpg#center" alt="Dennis Lindley"  />
</p>
<p><a href="https://en.wikipedia.org/wiki/Dennis_Lindley">Dennis Lindley</a>,
one of my many heroes,
was an English statistician,
decision theorist and leading advocate of Bayesian statistics.
He published a pivotal book,
<a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118650158">Understanding Uncertainty</a>,
that changed my view on what is and how to handle uncertainty in a
coherent<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> way.
He is responsible for one of my favorites quotes:
&ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&rdquo;;
and one of my favorite heuristics around prior probabilities:
<a href="https://en.wikipedia.org/wiki/Cromwell%27s_rule">Cromwell&rsquo;s Rule</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.
Lindley predicted in 1975 that &ldquo;Bayesian methods will indeed become pervasive,
enabled by the development of powerful computing facilities&rdquo; (Lindley, 1975).
You can find more about all of Lindley&rsquo;s achievements in his <a href="https://www.theguardian.com/science/2014/mar/16/dennis-lindley">obituary</a>.</p>
<h2 id="lindleys-paradox">Lindley&rsquo;s Paradox</h2>
<p>Lindley&rsquo;s paradox<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> is a counterintuitive situation in statistics
in which the Bayesian and frequentist approaches to a hypothesis testing problem
give different results for certain choices of the prior distribution.</p>
<p>More formally, the paradox is as follows.
We have some parameter $\theta$ that we are interested in.
Then, we proceed with an experiment to test two competing hypotheses:</p>
<ol>
<li>$H_0$ (also known as <em>null hypothesis</em>):
there is no &ldquo;effect&rdquo;, or, more specifically,
$\theta = 0$.</li>
<li>$H_a$ (also known as <em>alternative hypothesis</em>):
there is an &ldquo;effect&rdquo;, or, more specifically,
$\theta \ne 0$.</li>
</ol>
<p>The paradox occurs when two conditions are met:</p>
<ol>
<li>The result of the experiment is <em>significant</em> by a frequentist test of $H_0$,
which indicates sufficient evidence to reject $H_0$, at a certain threshold of
probability<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</li>
<li>The posterior probability (Bayesian approach) of $H_0 \mid \theta$
(null hypothesis given $\theta$) is high,
which indicates strong evidence that $H_0$ should be favored over $H_a$,
that is, to <em>not</em> reject $H_0$.</li>
</ol>
<p>These results can occur at the same time when $H_0$ is very specific,
$H_a$ more diffuse,
and the prior distribution does not strongly favor one or the other.
These conditions are pervasive across science
and common in traditional null-hypothesis significance testing approaches.</p>
<p>This is a duel of frequentist versus Bayesian approaches,
and one of the many in which Bayesian emerges as the most coherent.
Let&rsquo;s give a example and go over the analytical result with a ton of math,
but also a computational result with <a href="https://julia-lang.org">Julia</a>.</p>
<h2 id="example">Example</h2>
<p>Here&rsquo;s the setup for the example.
In a certain city 49,581 boys and 48,870 girls have been
born over a certain time period.
The observed proportion of male births is thus
$\frac{49,581}{98,451} \approx 0.5036$.</p>
<p>We assume that the birth of a child is independent with a certain probability
$\theta$.
Since our data is a sequence of $n$ independent <a href="https://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli trials</a>,
i.e., $n$ independent random experiments with exactly two possible outcomes:
&ldquo;success&rdquo; and &ldquo;failure&rdquo;,
in which the probability of success is the same every time the
experiment is conducted.
We can safely assume that it follows a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>
with parameters:</p>
<ul>
<li>$n$: the number of &ldquo;trials&rdquo; (or the total number of births).</li>
<li>$\theta$: the probability of male births.</li>
</ul>
<p>We then set up our two competing hypotheses:</p>
<ol>
<li>$H_0$: $\theta = 0.5$.</li>
<li>$H_a$: $\theta \ne 0.5$.</li>
</ol>
<h3 id="analytical-solution">Analytical Solution</h3>
<p>This is a toy-problem and, like most toy problems,
we can solve it analytically<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> for both the frequentist and the Bayesian approaches.</p>
<h4 id="analytical-solutions----frequentist-approach">Analytical Solutions &ndash; Frequentist Approach</h4>
<p>The frequentist approach to testing $H_0$ is to compute a $p$-value<sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>,
the probability of observing births of boys at least as large as 49,581
assuming $H_0$ is true.
Because the number of births is very large,
we can use a normal approximation<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> for the
binomial-distributed number of male births.
Let&rsquo;s define $X$ as the total number of male births,
then $X$ follows a normal distribution:</p>
<p>$$X \sim \text{Normal}(\mu, \sigma)$$</p>
<p>where $\mu$ is the mean parameter,
$n \theta$ in our case,
and $\sigma$ is the standard deviation parameter,
$\sqrt{n \theta (1 - \theta)}$.
We need to calculate the conditional probability of
$X \geq \frac{49,581}{98,451} \approx 0.5036$
given $\mu = n \theta = 98,451 \cdot \frac{1}{2} = 49,225.5$
and
$\sigma = \sqrt{n \theta (1 - \theta)} = \sqrt{98,451 \cdot \frac{1}{2} \cdot (1 - \frac{1}{2})}$:</p>
<p>$$P(X \ge 0.5036 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75})$$</p>
<p>This is basically a
<a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function (CDF)</a>
of $X$ on the interval $[49,225.5, 98,451]$:</p>
<p>$$\int_{49,225.5}^{98,451} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{\left( \frac{x - \mu}{\sigma} \right)^2}{2}} dx$$</p>
<p>After inserting the values and doing some arithmetic,
our answer is approximately $0.0117$.
Note that this is a one-sided test,
since it is symmetrical,
the two-sided test would be
$0.0117 \cdot 2 = 0.0235$.
Since we don&rsquo;t deviate from the Fisher&rsquo;s canon,
this is well below the 5% threshold.
Hooray! We rejected the null hypothesis!
Quick! Grab a frequentist celebratory cigar!
But, wait. Let&rsquo;s check the Bayesian approach.</p>
<h4 id="analytical-solutions----bayesian-approach">Analytical Solutions &ndash; Bayesian Approach</h4>
<p>For the Bayesian approach, we need to set prior probabilities on both hypotheses.
Since we do not favor one from another, let&rsquo;s set equal prior probabilities:</p>
<p>$$P(H_0) = P(H_a) = \frac{1}{2}$$</p>
<p>Additionally, all parameters of interest need a prior distribution.
So, let&rsquo;s put a prior distribution on $\theta$.
We could be fancy here, but let&rsquo;s not.
We&rsquo;ll use a uniform distribution on $[0, 1]$.</p>
<p>We have everything we need to compute the posterior probability of $H_0$ given
$\theta$.
For this, we&rsquo;ll use <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes theorem</a><sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>:</p>
<p>$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$$</p>
<p>Now again let&rsquo;s plug in all the values:</p>
<p>$$P(H_0 \mid \theta) = \frac{P(\theta \mid H_0) P(H_0)}{P(\theta)}$$</p>
<p>Note that by the <a href="https://en.wikipedia.org/wiki/Probability_axioms">axioms of probability</a>
and by the <a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">product rule of probability</a>
we can decompose $P(\theta)$ into:</p>
<p>$$P(\theta) = P(\theta \mid H_0) P(H_0) + P(\theta \mid H_a) P(H_a)$$</p>
<p>Again, we&rsquo;ll use the normal approximation:</p>
<p>$$\begin{aligned}
&amp;P \left( \theta = 0.5 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75} \right) \\
&amp;= \frac{
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \left( \frac{(\mu - \mu \cdot 0.5)}{2 \sigma} \right)^2} \cdot 0.5
}
{
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{ \left( -\frac{(\mu - \mu \cdot 0.5)}{2 \sigma} \right)^2} \cdot 0.5 +
\int_0^1 \frac {1}{\sqrt{2 \pi \sigma^2} } e^{- \left( \frac{\mu - \mu \cdot \theta)}{2 \sigma} \right)^2}d \theta \cdot 0.5
} \\
&amp;= 0.9505
\end{aligned}$$</p>
<p>The likelihood of the alternative hypothesis,
$P(\theta \mid H_a)$,
is just the CDF of all possible values of $\theta \ne 0.5$.</p>
<p>$$P(H_0 \mid \text{data}) = P \left( \theta = 0.5 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75} \right) &gt; 0.95$$</p>
<p>And we fail to reject the null hypothesis, in frequentist terms.
However, we can also say in Bayesian terms, that we strongly favor $H_0$
over $H_a$.</p>
<p>Quick! Grab the Bayesian celebratory cigar!
The null is back on the game!</p>
<h3 id="computational-solutional">Computational Solutional</h3>
<p>For the computational solution, we&rsquo;ll use <a href="https://julialang.org">Julia</a>
and the following packages:</p>
<ul>
<li><a href="https://github.com/JuliaStats/HypothesisTests.jl"><code>HypothesisTest.jl</code></a></li>
<li><a href="https://turinglang.org/"><code>Turing.jl</code></a></li>
</ul>
<h4 id="computational-solutions----frequentist-approach">Computational Solutions &ndash; Frequentist Approach</h4>
<p>We can perform a <a href="https://juliastats.org/HypothesisTests.jl/stable/nonparametric/#Binomial-test"><code>BinomialTest</code></a>
with <code>HypothesisTest.jl</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">using</span> <span class="n">HypothesisTests</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">BinomialTest</span><span class="p">(</span><span class="mi">49_225</span><span class="p">,</span> <span class="mi">98_451</span><span class="p">,</span> <span class="mf">0.5036</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Binomial</span> <span class="n">test</span>
</span></span><span class="line"><span class="cl"><span class="o">-------------</span>
</span></span><span class="line"><span class="cl"><span class="n">Population</span> <span class="n">details</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameter</span> <span class="n">of</span> <span class="n">interest</span><span class="o">:</span>   <span class="n">Probability</span> <span class="n">of</span> <span class="n">success</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="n">under</span> <span class="n">h_0</span><span class="o">:</span>         <span class="mf">0.5036</span>
</span></span><span class="line"><span class="cl">    <span class="n">point</span> <span class="n">estimate</span><span class="o">:</span>          <span class="mf">0.499995</span>
</span></span><span class="line"><span class="cl">    <span class="mi">95</span><span class="o">%</span> <span class="n">confidence</span> <span class="n">interval</span><span class="o">:</span> <span class="p">(</span><span class="mf">0.4969</span><span class="p">,</span> <span class="mf">0.5031</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Test</span> <span class="n">summary</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">outcome</span> <span class="n">with</span> <span class="mi">95</span><span class="o">%</span> <span class="n">confidence</span><span class="o">:</span> <span class="n">reject</span> <span class="n">h_0</span>
</span></span><span class="line"><span class="cl">    <span class="n">two</span><span class="o">-</span><span class="n">sided</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="o">:</span>           <span class="mf">0.0239</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Details</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">number</span> <span class="n">of</span> <span class="n">observations</span><span class="o">:</span> <span class="mi">98451</span>
</span></span><span class="line"><span class="cl">    <span class="n">number</span> <span class="n">of</span> <span class="n">successes</span><span class="o">:</span>    <span class="mi">49225</span>
</span></span></code></pre></div><p>This is the two-sided test,
and I had to round $49,225.5$ to $49,225$
since <code>BinomialTest</code> do not support real numbers.
But the results match with the analytical solution,
we still reject the null.</p>
<h4 id="computational-solutions----bayesian-approach">Computational Solutions &ndash; Bayesian Approach</h4>
<p>Now, for the Bayesian computational approach,
I&rsquo;m going to use a generative modeling approach,
and one of my favorites probabilistic programming languages,
<code>Turing.jl</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">using</span> <span class="n">Turing</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@model</span> <span class="k">function</span> <span class="n">birth_rate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">           <span class="n">θ</span> <span class="o">~</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">           <span class="n">total_births</span> <span class="o">=</span> <span class="mi">98_451</span>
</span></span><span class="line"><span class="cl">           <span class="n">male_births</span> <span class="o">~</span> <span class="n">Binomial</span><span class="p">(</span><span class="n">total_births</span><span class="p">,</span> <span class="n">θ</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="k">end</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">birth_rate</span><span class="p">()</span> <span class="o">|</span> <span class="p">(;</span> <span class="n">male_births</span> <span class="o">=</span> <span class="mi">49_225</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(</span><span class="mi">1_000</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="mi">1_000</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Chains</span> <span class="n">MCMC</span> <span class="n">chain</span> <span class="p">(</span><span class="mi">1000</span><span class="o">×</span><span class="mi">13</span><span class="o">×</span><span class="mi">4</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">3</span><span class="p">})</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Iterations</span>        <span class="o">=</span> <span class="mi">1001</span><span class="o">:</span><span class="mi">1</span><span class="o">:</span><span class="mi">2000</span>
</span></span><span class="line"><span class="cl"><span class="kt">Number</span> <span class="n">of</span> <span class="n">chains</span>  <span class="o">=</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl"><span class="n">Samples</span> <span class="n">per</span> <span class="n">chain</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="n">Wall</span> <span class="n">duration</span>     <span class="o">=</span> <span class="mf">0.2</span> <span class="n">seconds</span>
</span></span><span class="line"><span class="cl"><span class="n">Compute</span> <span class="n">duration</span>  <span class="o">=</span> <span class="mf">0.19</span> <span class="n">seconds</span>
</span></span><span class="line"><span class="cl"><span class="n">parameters</span>        <span class="o">=</span> <span class="n">θ</span>
</span></span><span class="line"><span class="cl"><span class="n">internals</span>         <span class="o">=</span> <span class="n">lp</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">is_accept</span><span class="p">,</span> <span class="n">acceptance_rate</span><span class="p">,</span> <span class="n">log_density</span><span class="p">,</span> <span class="n">hamiltonian_energy</span><span class="p">,</span> <span class="n">hamiltonian_energy_error</span><span class="p">,</span> <span class="n">max_hamiltonian_energy_error</span><span class="p">,</span> <span class="n">tree_depth</span><span class="p">,</span> <span class="n">numerical_error</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">nom_step_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Summary</span> <span class="n">Statistics</span>
</span></span><span class="line"><span class="cl">  <span class="n">parameters</span>      <span class="n">mean</span>       <span class="n">std</span>      <span class="n">mcse</span>    <span class="n">ess_bulk</span>    <span class="n">ess_tail</span>      <span class="n">rhat</span>   <span class="n">ess_per_sec</span>
</span></span><span class="line"><span class="cl">      <span class="kt">Symbol</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>     <span class="kt">Float64</span>     <span class="kt">Float64</span>   <span class="kt">Float64</span>       <span class="kt">Float64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">θ</span>    <span class="mf">0.4999</span>    <span class="mf">0.0016</span>    <span class="mf">0.0000</span>   <span class="mf">1422.2028</span>   <span class="mf">2198.1987</span>    <span class="mf">1.0057</span>     <span class="mf">7368.9267</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Quantiles</span>
</span></span><span class="line"><span class="cl">  <span class="n">parameters</span>      <span class="mf">2.5</span><span class="o">%</span>     <span class="mf">25.0</span><span class="o">%</span>     <span class="mf">50.0</span><span class="o">%</span>     <span class="mf">75.0</span><span class="o">%</span>     <span class="mf">97.5</span><span class="o">%</span>
</span></span><span class="line"><span class="cl">      <span class="kt">Symbol</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">θ</span>    <span class="mf">0.4969</span>    <span class="mf">0.4988</span>    <span class="mf">0.4999</span>    <span class="mf">0.5011</span>    <span class="mf">0.5031</span>
</span></span></code></pre></div><p>We can see from the output of the quantiles that the 95% quantile for $\theta$ is
the interval $(0.4969, 0.5031)$.
Although it overlaps zero, that is not the equivalent of a hypothesis test.
For that, we&rsquo;ll use the
<a href="https://en.wikipedia.org/wiki/highest_posterior_density_interval">highest posterior density interval (HPDI)</a>,
which is defined as &ldquo;choosing the narrowest interval&rdquo; that
captures a certain posterior density threshold value.
In this case, we&rsquo;ll use a threshold interval of 95%,
i.e. an $\alpha = 0.05$:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">hpd</span><span class="p">(</span><span class="n">chain</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">HPD</span>
</span></span><span class="line"><span class="cl">  <span class="n">parameters</span>     <span class="n">lower</span>     <span class="n">upper</span>
</span></span><span class="line"><span class="cl">      <span class="kt">Symbol</span>   <span class="kt">Float64</span>   <span class="kt">Float64</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">θ</span>    <span class="mf">0.4970</span>    <span class="mf">0.5031</span>
</span></span></code></pre></div><p>We see that we fail to reject the null,
$\theta = 0.5$ at $\alpha = 0.05$ which is in accordance with the analytical
solution.</p>
<h2 id="why-the-frequentist-and-bayesian-approaches-disagree">Why the Frequentist and Bayesian Approaches Disagree</h2>
<p>Why do the approaches disagree?
What is going on under the hood?</p>
<p>The answer is disappointing<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.
The main problem is that the frequentist approach only allows fixed significance
levels with respect to sample size.
Whereas the Bayesian approach is consistent and robust to sample size variations.</p>
<p>Taken to extreme, in some cases, due to huge sample sizes,
the $p$-value is pretty much a <em>proxy</em> for sample size
and have little to no utility on hypothesis testing.
This is known as $p$-hacking<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
<h2 id="references">References</h2>
<p>Lindley, Dennis V. &ldquo;The future of statistics: A Bayesian 21st century&rdquo;.
<em>Advances in Applied Probability</em> 7 (1975): 106-115.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>as far as I know there&rsquo;s only one coherent approach to uncertainty,
and it is the Bayesian approach.
Otherwise, as de Finetti and Ramsey proposed,
you are susceptible to a <a href="https://en.wikipedia.org/wiki/Dutch_book">Dutch book</a>.
This is a topic for another blog post&hellip;&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Cromwell&rsquo;s rule states that the use of prior probabilities of 1
(&ldquo;the event will definitely occur&rdquo;) or 0 (&ldquo;the event will definitely not occur&rdquo;)
should be avoided, except when applied to statements that are logically true or false.
Hence, anything that is not a math theorem should have priors in $(0,1)$.
The reference comes from <a href="https://en.wikipedia.org/wiki/Oliver_Cromwell">Oliver Cromwell</a>,
asking, very politely, for the Church of Scotland to consider that their prior probability
might be wrong.
This footnote also deserves a whole blog post&hellip;&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy">Stigler&rsquo;s law of eponymy</a>
states that no scientific discovery is named after its original discoverer.
The paradox was already was discussed in <a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Harold Jeffreys</a>'
1939 textbook.
Also, fun fact, Stigler&rsquo;s is not the original creator of such law&hellip;
Now that&rsquo;s a self-referential paradox, and a broad version of the <a href="https://en.wikipedia.org/wiki/Halting_problem">Halting problem</a>,
which should earn its own footnote.
Nevertheless, we are getting into self-referential danger zone here with
footnotes&rsquo; of footnotes&rsquo; of footnotes&rsquo;&hellip;&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>this is called $p$-value and can be easily defined as
&ldquo;the probability of sampling data from a target population given that $H_0$
is true as the number of sampling procedures $\to \infty$&rdquo;.
Yes, it is not that intuitive, and it deserves not a blog post,
but a full curriculum to hammer it home.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>that is not true for most of the real-world problems.
For Bayesian approaches,
we need to run computational asymptotic exact approximations using a class
of methods called <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo (MCMC)</a>.
Furthermore, for some nasty problems we need to use different set of methods
called <a href="https://en.wikipedia.org/wiki/Variational_Inference">variational inference (VI)</a>
or <a href="https://en.wikipedia.org/wiki/Approximate_Bayesian_computation">approximate Bayesian computation (ABC)</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>if you are curious about how this approximation works,
check the backup slides of my
<a href="https://github.com/storopoli/Bayesian-Statistic">open access and open source graduate course on Bayesian statistics</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Bayes&rsquo; theorem is officially called Bayes-Price-Laplace theorem.
Bayes was trying to disprove David Hume&rsquo;s argument that miracles did not exist
(How dare he?).
He used the probabilistic approach of trying to quantify the probability of a parameter
(god exists) given data (miracles happened).
He died without publishing any of his ideas.
His wife probably freaked out when she saw the huge pile of notes that he had
and called his buddy Richard Price to figure out what to do with it.
Price struck gold and immediately noticed the relevance of Bayes&rsquo; findings.
He read it aloud at the Royal Society.
Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes,
used the same probabilistic approach to perform statistical inference using France&rsquo;s
first census data in the early-Napoleonic era.
Somehow we had the answer to statistical inference back then,
and we had to rediscover everything again in the late-20th century&hellip;&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>disappointing because most of
published scientific studies suffer from this flaw.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>and, like all footnotes here, it deserves its own blog post&hellip;&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Word Embeddings</title>
      <link>https://storopoli.io/2023-11-20-word_embeddings/</link>
      <pubDate>Sun, 19 Nov 2023 22:49:51 -0300</pubDate>
      
      <guid>https://storopoli.io/2023-11-20-word_embeddings/</guid>
      <description>Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&amp;rsquo;ll have to unfortunately enable JavaScript.
I wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way.</description>
      <content:encoded><![CDATA[<blockquote>
<p>Warning: This post has <a href="https://katex.org/">KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p>
</blockquote>
<p><img loading="lazy" src="euclid.jpg#center" alt="Euclid of Alexandria"  />
</p>
<p>I wish I could go back in time and tell my younger self
that you can make a machine understand human language with trigonometry.
That would definitely have made me more aware and interested in the
subject during my school years.
I would have looked at triangles, circles, sines, cosines, and tangents
in a whole different way.
Alas, better late than never.</p>
<p>In this post, we&rsquo;ll learn how to represent words using word embeddings,
and how to use basic trigonometry to play around with them.
Of course, we&rsquo;ll use <a href="https://julialang.org">Julia</a>.</p>
<h2 id="word-embeddings">Word Embeddings</h2>
<p><strong><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a> is a way to
represent words as a real-valued vector that encodes the meaning of the word
in such a way that words that are closer in the vector space are expected
to be similar in meaning</strong>.</p>
<p>Ok, let&rsquo;s unwrap the above definition.
First, a <strong>real-valued vector</strong> is any vector which its elements belong to the real
numbers.
Generally we denote vectors with a bold lower-case letter,
and we denote its elements (also called components) using square brackets.
Hence, a vector $\bold{v}$ that has 3 elements, $1$, $2$, and $3$,
can be written as</p>
<p>$$\bold{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$$</p>
<p>Next, what &ldquo;close&rdquo; means for vectors?
We can use distance functions to get a measurable value.
The most famous and commonly used distance function is the <strong>Euclidean distance</strong>,
in honor of <a href="https://en.wikipedia.org/wiki/Euclid">Euclid</a>, the &ldquo;father of geometry&rdquo;,
and the guy pictured in the image at the top of this post.
The Euclidean distance is defined in trigonometry for 2-D and 3-D spaces.
However, it can be generalized to any dimension $n &gt; 1$ by using vectors.</p>
<p>Since every word is represented by an $n$-dimensional vector,
we can use distances to compute a metric that represent similarity between vectors.
And, more interesting, we can add and subtract words
(or any other linear combination of one or more words) to generate new words.</p>
<p>Before we jump to code and examples, a quick note about how word embeddings
are constructed.
They are trained like a regular machine learning algorithm,
where the cost function measures the difference between
some vector distance between the vectors and a &ldquo;semantic distance&rdquo;.
The goal is to iteratively find good vector values that minimize the cost.
So, if a vector is close to another vector measured by a distance function,
but far apart measured by some semantic distance on the words that these
vectors represent, then the cost function will be higher.
The algorithm cannot change the semantic distance, it is treated as a fixed value.
However, it can change the vector elements&rsquo; values so that the vector distance function
closely resembles the semantic distance function.
Lastly, generally the dimensionality of the vectors used in word embeddings
are high, $n &gt; 50$, since it needs a proper amount of dimensions in order to
represent all the semantic information of words with vectors.</p>
<h2 id="pre-trained-word-embeddings">Pre-Trained Word Embeddings</h2>
<p>Generally we don&rsquo;t train our own word embeddings from scratch,
we use pre-trained ones.
Here is a list of some of the most popular ones:</p>
<ul>
<li><a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>:
One of the first public available word embeddings,
made by Google in 2013.
Only supports English.</li>
<li><a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>:
made by Stanford in 2014.
Only supports English.</li>
<li><a href="https://fasttext.cc/">FastText</a>:
From Facebook, released in 2016.
Supports hundreds of languages.</li>
</ul>
<h2 id="julia-code">Julia Code</h2>
<p>We will use the <a href="https://github.com/JuliaText/Embeddings.jl"><code>Embeddings.jl</code></a>
package to easily load word embeddings as vectors,
and the <a href="https://github.com/JuliaStats/Distances.jl"><code>Distances.jl</code></a>
package for the convenience of several distance functions.
This is a nice example of the Julia package ecosystem composability,
where one package can define types, another can define functions,
and another can define custom behavior of these functions on types that
are defined in other packages.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-jl" data-lang="jl"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">using</span> <span class="n">Embeddings</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">using</span> <span class="n">Distances</span>
</span></span></code></pre></div><p>Let&rsquo;s load the <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>
word embeddings.
First, let&rsquo;s check what we have in store to choose from
GloVe&rsquo;s English language embeddings:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-jl" data-lang="jl"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">language_files</span><span class="p">(</span><span class="kt">GloVe</span><span class="p">{</span><span class="ss">:en</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="mi">20</span><span class="o">-</span><span class="n">element</span> <span class="kt">Vector</span><span class="p">{</span><span class="kt">String</span><span class="p">}</span><span class="o">:</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.50d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.100d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.200d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.300d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.42B.300d/glove.42B.300d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.840B.300d/glove.840B.300d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.25d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.50d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.100d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.200d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.50d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.100d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.200d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.6B/glove.6B.300d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.42B.300d/glove.42B.300d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.840B.300d/glove.840B.300d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.25d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.50d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.100d.txt&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="s">&#34;glove.twitter.27B/glove.twitter.27B.200d.txt&#34;</span>
</span></span></code></pre></div><p>I&rsquo;ll use the <code>&quot;glove.6B/glove.6B.50d.txt&quot;</code>.
This means that it was trained with 6 billion tokens,
and it provides embeddings with 50-dimensional vectors.
The <code>load_embeddings</code> function takes an optional second positional
argument as an <code>Int</code> to choose from which index of the <code>language_files</code> to use.
Finally, I just want the words &ldquo;king&rdquo;, &ldquo;queen&rdquo;, &ldquo;man&rdquo;, &ldquo;woman&rdquo;;
so I am passing these words as a <code>Set</code> to the <code>keep_words</code> keyword argument:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-jl" data-lang="jl"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="k">const</span> <span class="n">glove</span> <span class="o">=</span> <span class="n">load_embeddings</span><span class="p">(</span><span class="kt">GloVe</span><span class="p">{</span><span class="ss">:en</span><span class="p">},</span> <span class="mi">1</span><span class="p">;</span> <span class="n">keep_words</span><span class="o">=</span><span class="kt">Set</span><span class="p">([</span><span class="s">&#34;king&#34;</span><span class="p">,</span> <span class="s">&#34;queen&#34;</span><span class="p">,</span> <span class="s">&#34;man&#34;</span><span class="p">,</span> <span class="s">&#34;woman&#34;</span><span class="p">]));</span>
</span></span><span class="line"><span class="cl"><span class="n">Embeddings</span><span class="o">.</span><span class="kt">EmbeddingTable</span><span class="p">{</span><span class="kt">Matrix</span><span class="p">{</span><span class="kt">Float32</span><span class="p">},</span> <span class="kt">Vector</span><span class="p">{</span><span class="kt">String</span><span class="p">}}(</span><span class="kt">Float32</span><span class="p">[</span><span class="o">-</span><span class="mf">0.094386</span> <span class="mf">0.50451</span> <span class="o">-</span><span class="mf">0.18153</span> <span class="mf">0.37854</span><span class="p">;</span> <span class="mf">0.43007</span> <span class="mf">0.68607</span> <span class="mf">0.64827</span> <span class="mf">1.8233</span><span class="p">;</span> <span class="o">…</span> <span class="p">;</span> <span class="mf">0.53135</span> <span class="o">-</span><span class="mf">0.64426</span> <span class="mf">0.48764</span> <span class="mf">0.0092753</span><span class="p">;</span> <span class="o">-</span><span class="mf">0.11725</span> <span class="o">-</span><span class="mf">0.51042</span> <span class="o">-</span><span class="mf">0.10467</span> <span class="o">-</span><span class="mf">0.60284</span><span class="p">],</span> <span class="p">[</span><span class="s">&#34;man&#34;</span><span class="p">,</span> <span class="s">&#34;king&#34;</span><span class="p">,</span> <span class="s">&#34;woman&#34;</span><span class="p">,</span> <span class="s">&#34;queen&#34;</span><span class="p">])</span>
</span></span></code></pre></div><p>Watch out with the order that we get back.
If you see the output of <code>load_embeddings</code>,
the order is <code>&quot;man&quot;, &quot;king&quot;, &quot;woman&quot;, &quot;queen&quot;]</code>
Let&rsquo;s see how a word is represented:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-jl" data-lang="jl"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">queen</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="mi">50</span><span class="o">-</span><span class="n">element</span> <span class="kt">Vector</span><span class="p">{</span><span class="kt">Float32</span><span class="p">}</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="mf">0.37854</span>
</span></span><span class="line"><span class="cl">  <span class="mf">1.8233</span>
</span></span><span class="line"><span class="cl"> <span class="o">-</span><span class="mf">1.2648</span>
</span></span><span class="line"><span class="cl">  <span class="o">⋮</span>
</span></span><span class="line"><span class="cl"> <span class="o">-</span><span class="mf">2.2839</span>
</span></span><span class="line"><span class="cl">  <span class="mf">0.0092753</span>
</span></span><span class="line"><span class="cl"> <span class="o">-</span><span class="mf">0.60284</span>
</span></span></code></pre></div><p>They are 50-dimensional vectors of <code>Float32</code>.</p>
<p>Now, here&rsquo;s the fun part:
let&rsquo;s add words and check the similarity between the
result and some other word.
A classical example is to start with the word &ldquo;king&rdquo;,
subtract the word &ldquo;men&rdquo;,
add the word &ldquo;woman&rdquo;,
and check the distance of the result to the word &ldquo;queen&rdquo;:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-jl" data-lang="jl"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">man</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">king</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="mi">2</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">woman</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="mi">3</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">cosine_dist</span><span class="p">(</span><span class="n">king</span> <span class="o">-</span> <span class="n">man</span> <span class="o">+</span> <span class="n">woman</span><span class="p">,</span> <span class="n">queen</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.13904202f0</span>
</span></span></code></pre></div><p>This is less than 1/4 of the distance of &ldquo;woman&rdquo; to &ldquo;king&rdquo;:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-jl" data-lang="jl"><span class="line"><span class="cl"><span class="n">julia</span><span class="o">&gt;</span> <span class="n">cosine_dist</span><span class="p">(</span><span class="n">woman</span><span class="p">,</span> <span class="n">king</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.58866215f0</span>
</span></span></code></pre></div><p>Feel free to play around with others words.
If you want suggestions, another classical example is:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">cosine_dist</span><span class="p">(</span><span class="n">Madrid</span> <span class="o">-</span> <span class="n">Spain</span> <span class="o">+</span> <span class="n">France</span><span class="p">,</span> <span class="n">Paris</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>I think that by allying interesting applications to abstract math topics
like trigonometry is the vital missing piece in STEM education.
I wish every new kid that is learning math could have the opportunity to contemplate
how new and exciting technologies have some amazing simple math under the hood.
If you liked this post, you would probably like <a href="https://en.wikipedia.org/wiki/Linear_algebra">linear algebra</a>.
I would highly recommend <a href="https://math.mit.edu/~gs/">Gilbert Strang&rsquo;s books</a>
and <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3blue1brown series on linear algebra</a>.</p>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What is soydev? And why do I hate it</title>
      <link>https://storopoli.io/2023-11-10-2023-11-13-soydev/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://storopoli.io/2023-11-10-2023-11-13-soydev/</guid>
      <description>Let&amp;rsquo;s dive into the concept of &amp;ldquo;soydev&amp;rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="soydev.jpg" alt="soydev meme"  />
</p>
<p>Let&rsquo;s dive into the <strong>concept of &ldquo;soydev&rdquo;</strong>,
a term often used pejoratively to describe developers with
a superficial understanding of technology.
I provide my definition of what soydev is,
why is bad, and how it came to be.
To counteract soydev inclinations,
I propose an abstract approach centered on timeless concepts, protocols,
and first principles, fostering a mindset of exploration,
resilience in the face of failure, and an insatiable hunger for knowledge.</p>
<p>While we&rsquo;ll start with a look at the soydev stereotype,
our journey will lead us to a wider reflection on the importance of depth in
technological understanding.</p>
<h2 id="definition">Definition</h2>
<p>First, let&rsquo;s tackle the definition of <em>soydev</em>.
<a href="https://www.urbandictionary.com/define.php?term=Soydev">Urban Dictionary</a>
provides two interesting definitions:</p>
<p>Urban Dictionary definition 1:</p>
<blockquote>
<p>Soydev is a &ldquo;programmer&rdquo; that works at a bigh tech company and
only knows JavaScript and HTML.
They love IDEs like Visual Studio Code and inefficient frameworks
that slow their code down.
They represent the majority of &ldquo;programmers&rdquo; today and
if their numbers continue growing,
not one person on earth will know how a computer works by the year 2050
when all the gigachad 1980s C and Unix programmers are gone.</p>
</blockquote>
<p>Urban Dictionary definition 2:</p>
<blockquote>
<p>Soydev is a type of most abundant Software Developer.
The Software he/she makes is always inefficient and
uses more CPU and RAM than it should.
This person always prefers hard work to smart work,
Has little or no knowledge of existing solutions of a problem,
Comes up with very complex solution for a simple problem and
has fear of native and fast programming languages like C, C++ and Rust</p>
</blockquote>
<p>These definitions give a glimpse of what a soydev is.
However, they are loaded with pejorative language,
and also are based on non-timeless technologies and tools.
I, much prefer to rely on concepts and principles that are timeless.
Hence, I will provide my own definition of soydev:</p>
<p><strong>Soydev is someone who only has a superficial conception of technology
and computers that is restricted to repeating patterns learned
from popular workflows on the internet;
but who doesn&rsquo;t dedicate time or effort to learning concepts in a deeper way.</strong></p>
<p>Although soydev is a term with specific connotations,
it opens the door to a larger conversation about the depth of our engagement
with technology.
This superficiality is not unique to soydevs
but is a symptom of a broader trend in our relationship with technology.</p>
<p>Most of us start our journey in a skill by having the superficial conception of it.
However, some are not satisfied with this superficial conception,
and strive to understand what lies beyond the surface.</p>
<p>Understanding concepts from first principles allows us to achieve a deep graceful
kind of mastery that when seems almost effortless to others.
Deep down lies a lot of effort and time spent in learning and practicing.
Innumerable hours of deep thinking and reflecting on
why things are the way they are, and how they could be different if you
tried to implement them from scratch yourself.</p>
<p>There is also an inherently rare mixture of curiosity and creativity in the
process of profoundly learning and understanding concepts in this way.
You start not only to ask the &ldquo;Why?&rdquo; questions but also the &ldquo;What if?&rdquo; questions.
I feel that this posture on understanding concepts paves the way for joyful mastery.</p>
<p><a href="https://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a>
once said &ldquo;What I cannot create, I do not understand&rdquo;.
You cannot create anything that you don&rsquo;t know the underlying concepts.
Therefore, by allying creativity and discovery with deep knowledge,
Feynman&rsquo;s argument was that in order for you truly master something,
you&rsquo;ll need to be able to recreate it from scratch.</p>
<p>If you are struggling with my abstractions, I can provide some concrete examples.
A soydev might be someone who:</p>
<ol>
<li>Instead of using a simple text editor like vim/emacs/nano,
prefers a totally bloated IDE like VSCode to write even basic things in plaintext.
Or cannot SSH into a remote server without a GUI.</li>
<li>Instead of choosing a native solution like HTML for a simple web page section,
prefers to overengineer with a JavaScript framework that has a lot of dependencies,
and takes 3 seconds to render in the browser.</li>
<li>Prefers to use black box abstractions instead of learning about basic tech
primitives and protocols that would get the job done.</li>
<li>Has no idea what about most concepts that are at the core of computing like
bit, byte, heap, stack, garbage collector, async, parallel, CPU cycles,
cache miss, L1/L2/L3 cache etc.</li>
<li>Don&rsquo;t understand the math, or what is going on under the hood when
using machine learning libraries.</li>
</ol>
<h2 id="why-soydev-is-bad">Why soydev is bad</h2>
<p>First, let&rsquo;s understand that being a soydev is not necessarily bad,
but is highly limited on his ability and curiosity.
A soydev will never be able to achieve the same level of mastery
as someone who is willing to go deep and learn concepts from first principles.</p>
<p>Now, on the other hand,
soydev is bad because it perpetuates a mindset of superficiality.
The path of technology innovation is guided by curiosity and creativity.
And paved with hard work and deep understanding.
Imagine if all the great minds in technology took the easy path of mindless
tooling and problem-solving?
We would be in a stagnant and infertile scenario,
where everyone would use the same technology and tools without
questioning or thinking about the problems that they are trying to solve.</p>
<p>Hence, the culture of soydev is bad for the future of technology,
where most new developers will be highly limited in their ability to innovate.</p>
<h2 id="why-soydev-is-on-the-rise">Why soydev is on the rise</h2>
<p>I think that soydev culture is highly correlated with
the increase of technology and
decrease of barriers to access such technology.
We live in an age that not only technology is everywhere,
but also to interact with it is quite effortless.</p>
<p>My computational statistician mind is always aware of cognitive
and statistical bias.
Whenever I see a correlation across time,
I always take a step back and try to think about the assumptions
and conceptual models behind it.</p>
<p>Does the increase in technology usage and importance in daily life
results in more people using technology from a professional point-of-view?
Yes.
Does the increase in people professionally using technology
results in an increase of tooling and conceptual abstractions
that allows superficial interactions without need to deeply understand
the concepts behind such technology?
I do think that this is true as well.</p>
<p>These assumptions cover the constituents of the rise of soydev
from a &ldquo;demand&rdquo; viewpoint.
Nevertheless, there is also the analogous &ldquo;supply&rdquo; viewpoint.
If these trends in demand are not met by trends in supply,
we would not see the establishment of the soydev phenomenon.
There is an emerging trend to standardize all the available tech
into commodities.</p>
<p>While commoditization of technological solutions has inherent advantages,
such as scalability and lower opportunity costs,
it has some disadvantages.
The main disadvantage is the abrupt decrease of technological innovations.
If we have strong standardization that are enforced by market and social forces,
then why care to innovate?
Why bring new solutions or new ways to solve problems if it will not be adopted
and are doomed to oblivion?
Why decide to try to do things different if there is such a high maintenance
cost, especially when training and expanding human resources capable of
dealing with such non-standard solutions?</p>
<p>In this context, technological innovation can only be undertaken
by big corporations that, not only have big budgets,
but also big influence to push its innovations as industry standards.</p>
<p>Don&rsquo;t get me wrong: I do think that industry standards are important.
However, I much prefer a protocol standard than product standards.
First, protocol standards are generally not tied to a single company or brand.
Second, protocol standards have a higher propensity to expose its underlying
concepts to developers.
Think about TCP/IP versus your favorite front-end framework:
Which one would result in deeper understanding of the underlying concepts?</p>
<p>The rise of soydevs mirrors a societal shift towards immediate gratification and
away from the pursuit of deep knowledge.</p>
<h2 id="how-to-stop-being-a-soydev">How to stop being a soydev</h2>
<p>Despite these unstoppable trends I do think that it is possible to use
tools and shallow abstractions without being a soydev.
Or, to stop being a soydev and advance towards deep understanding
of what constitutes your craft.
Moving beyond the &lsquo;soydev&rsquo; mindset is about embracing the richness that
comes from a deep understanding of technology.
Here is a short, not by any means exhaustive list of things that you can start doing:</p>
<ol>
<li><strong>Stop thinking about what is latest and greatest</strong>.
The tools are not important as the problems they are solving.
Understand what is the need that the tool tries to address,
and the major concepts on how it works.</li>
<li><strong>Focus on concepts, protocols and first principles</strong>.
Forget about frameworks, languages, editors, apps, etc.
Focus on what probably won&rsquo;t change in the next decade in technology.
It is easy (and less uncertain) to think concepts and problems,
than in tools and solutions.</li>
<li><strong>Set aside some weekly time to explore new ideas</strong>.
Time block it, make it a calendar event.
Try to find a time slot that you can be free of distraction and worries.
Free all of your working memory when you are starting a &ldquo;new idea&rdquo; session.
Think about what problem is this idea trying to solve.
Also don&rsquo;t be stuck to just using the &ldquo;problem-solving&rdquo; lens.
Most concepts that are worth knowing in computer science and programming
don&rsquo;t have a clear problem tied to them.</li>
<li><strong>Tinker. Play around</strong>. Break something down and see if you can build from scratch.
Remember Feynman&rsquo;s &ldquo;What I cannot create, I do not understand&rdquo;.</li>
<li><strong>Failure is almost certain</strong>. Yes, you will definitely fail.
I say to a lot of junior devs, interns, and students that the only advantage
that I have compared to them is that I&rsquo;ve failed more times that they have tried.</li>
<li><strong>Stay curious and hungry for knowledge</strong>.
I am always impressed on how children are so curious.
I feel sad that almost no one retains their childhood curiosity as an adult
(Maybe that&rsquo;s why I love to interact with children).
Also, I am amazed by how deprived of will to learn some are.
That is an idea that for me, it is hard to grasp,
since I feel the exactly opposite.
Often I have to hold myself not diving into certain areas, ideas or concepts
because I cannot afford the time to learn them.
However, I am always tempted by them.</li>
</ol>
<h2 id="license">License</h2>
<p>This post is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img loading="lazy" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt="CC BY-NC-SA 4.0"  />
</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>About</title>
      <link>https://storopoli.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://storopoli.io/about/</guid>
      <description>I am a computational statistician and an ardent Bayesian. I like Julia, Rust, Neovim, and Nix. I love math, computer science, coding, stats, and teaching. I hate bloatware and soydevs. My natural habitat is the terminal. Everything that I do is either open source or has a permissive Creative Commons license.
Here&amp;rsquo;s a list of some resources that I&amp;rsquo;ve made or contributed:
Julia Data Science book. Graduate course on Bayesian Statistics with Stan and Turing.</description>
      <content:encoded><![CDATA[<p>I am a computational statistician and an ardent <a href="https://youtu.be/RMNwsdb5VU4">Bayesian</a>.
I like <a href="https://julialang.org">Julia</a>,
<a href="https://rust-lang.org/">Rust</a>,
<a href="https://neovim.org/">Neovim</a>,
and <a href="https://nixos.org">Nix</a>.
I love math, computer science, coding, stats, and teaching.
I hate bloatware and <a href="../2023-11-10-2023-11-13-soydev/">soydevs</a>.
My natural habitat is the terminal.
Everything that I do is either open source or has a permissive Creative Commons license.</p>
<p>Here&rsquo;s a list of some resources that I&rsquo;ve made or contributed:</p>
<ul>
<li><a href="https://juliadatascience.io/">Julia Data Science book</a>.</li>
<li><a href="https://github.com/storopoli/Bayesian-Statistics">Graduate course on Bayesian Statistics</a>
with <a href="https://mc-stan.org"><code>Stan</code></a> and
<a href="https://turinglang.org"><code>Turing.jl</code></a> code examples.</li>
<li><a href="https://storopoli.io/Bayesian-Julia">Bayesian Statistics with Julia and <code>Turing.jl</code></a></li>
<li><a href="https://github.com/TuringLang/TuringGLM.jl"><code>TuringGLM.jl</code></a></li>
<li><a href="https://storopoli.io/Computacao-Cientifica/">Data Science and Scientific Computing with Julia (Portuguese)</a></li>
<li><a href="https://github.com/storopoli/ciencia-de-dados">Undergraduate course on Data Science, Machine Learning and Deep Learning (Portuguese)</a></li>
<li><a href="https://storopoli.io/Why-Julia">Why Julia? A gentle pitch</a></li>
<li><a href="https://storopoli.io/Estatistica/">Graduate course on Statistics (Portuguese)</a>
with <code>R</code></li>
<li><a href="https://storopoli.io/Rcpp/"><code>Rcpp</code> tutorials (Portuguese)</a></li>
<li><a href="https://storopoli.io/topic-modeling-workshop/">Topic Modeling workshop (Portuguese)</a></li>
</ul>
<p>I don&rsquo;t have social media, since I think they are overrated
and &ldquo;they sell your data&rdquo;.
If you want to contact me, please send an email.</p>
]]></content:encoded>
    </item>
    
    
  </channel>
</rss>
