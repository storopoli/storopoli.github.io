<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Word Embeddings | Jose Storopoli, PhD</title>
<meta name=keywords content="julia,machine learning"><meta name=description content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
I wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way."><meta name=author content="Jose Storopoli"><link rel=canonical href=https://storopoli.io/2023-11-20-word_embeddings/><link crossorigin=anonymous href=/assets/css/stylesheet.5d45b8bd1a3cf526e72959d51f1bdc688d8e97fa0df2a697a93df6bdc746feb4.css integrity="sha256-XUW4vRo89SbnKVnVHxvcaI2Ol/oN8qaXqT32vcdG/rQ=" rel="preload stylesheet" as=style><noscript><link crossorigin=anonymous href=/css/includes/noscript.30127fa68e36d08f5dd7f9d4e717dac42e729b844672afd0fbcacb0d9e508595.css integrity="sha256-MBJ/po420I9d1/nU5xfaxC5ym4RGcq/Q+8rLDZ5QhZU=" rel="preload stylesheet" as=style></noscript><link rel=icon href=https://storopoli.io/assets/favicon.svg><link rel=icon type=image/png sizes=16x16 href=https://storopoli.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://storopoli.io/favicon-32x32.png><link rel=apple-touch-icon href=https://storopoli.io/apple-touch-icon.png><link rel=mask-icon href=https://storopoli.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://storopoli.io/2023-11-20-word_embeddings/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="Word Embeddings"><meta property="og:description" content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
I wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way."><meta property="og:type" content="article"><meta property="og:url" content="https://storopoli.io/2023-11-20-word_embeddings/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-19T22:49:51-03:00"><meta property="article:modified_time" content="2024-02-11T15:55:02-03:00"><meta property="og:site_name" content="Jose Storopoli, PhD"><meta name=twitter:card content="summary"><meta name=twitter:title content="Word Embeddings"><meta name=twitter:description content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
I wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://storopoli.io/posts/"},{"@type":"ListItem","position":2,"name":"Word Embeddings","item":"https://storopoli.io/2023-11-20-word_embeddings/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Word Embeddings","name":"Word Embeddings","description":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way.","keywords":["julia","machine learning"],"articleBody":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you’ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way. Alas, better late than never.\nIn this post, we’ll learn how to represent words using word embeddings, and how to use basic trigonometry to play around with them. Of course, we’ll use Julia.\nWord Embeddings Word embeddings is a way to represent words as a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\nOk, let’s unwrap the above definition. First, a real-valued vector is any vector which its elements belong to the real numbers. Generally we denote vectors with a bold lower-case letter, and we denote its elements (also called components) using square brackets. Hence, a vector $\\bold{v}$ that has 3 elements, $1$, $2$, and $3$, can be written as\n$$\\bold{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\nNext, what “close” means for vectors? We can use distance functions to get a measurable value. The most famous and commonly used distance function is the Euclidean distance, in honor of Euclid, the “father of geometry”, and the guy pictured in the image at the top of this post. The Euclidean distance is defined in trigonometry for 2-D and 3-D spaces. However, it can be generalized to any dimension $n \u003e 1$ by using vectors.\nSince every word is represented by an $n$-dimensional vector, we can use distances to compute a metric that represent similarity between vectors. And, more interesting, we can add and subtract words (or any other linear combination of one or more words) to generate new words.\nBefore we jump to code and examples, a quick note about how word embeddings are constructed. They are trained like a regular machine learning algorithm, where the cost function measures the difference between some vector distance between the vectors and a “semantic distance”. The goal is to iteratively find good vector values that minimize the cost. So, if a vector is close to another vector measured by a distance function, but far apart measured by some semantic distance on the words that these vectors represent, then the cost function will be higher. The algorithm cannot change the semantic distance, it is treated as a fixed value. However, it can change the vector elements’ values so that the vector distance function closely resembles the semantic distance function. Lastly, generally the dimensionality of the vectors used in word embeddings are high, $n \u003e 50$, since it needs a proper amount of dimensions in order to represent all the semantic information of words with vectors.\nPre-Trained Word Embeddings Generally we don’t train our own word embeddings from scratch, we use pre-trained ones. Here is a list of some of the most popular ones:\nWord2Vec: One of the first public available word embeddings, made by Google in 2013. Only supports English. GloVe: made by Stanford in 2014. Only supports English. FastText: From Facebook, released in 2016. Supports hundreds of languages. Julia Code We will use the Embeddings.jl package to easily load word embeddings as vectors, and the Distances.jl package for the convenience of several distance functions. This is a nice example of the Julia package ecosystem composability, where one package can define types, another can define functions, and another can define custom behavior of these functions on types that are defined in other packages.\njulia\u003e using Embeddings julia\u003e using Distances Let’s load the GloVe word embeddings. First, let’s check what we have in store to choose from GloVe’s English language embeddings:\njulia\u003e language_files(GloVe{:en}) 20-element Vector{String}: \"glove.6B/glove.6B.50d.txt\" \"glove.6B/glove.6B.100d.txt\" \"glove.6B/glove.6B.200d.txt\" \"glove.6B/glove.6B.300d.txt\" \"glove.42B.300d/glove.42B.300d.txt\" \"glove.840B.300d/glove.840B.300d.txt\" \"glove.twitter.27B/glove.twitter.27B.25d.txt\" \"glove.twitter.27B/glove.twitter.27B.50d.txt\" \"glove.twitter.27B/glove.twitter.27B.100d.txt\" \"glove.twitter.27B/glove.twitter.27B.200d.txt\" \"glove.6B/glove.6B.50d.txt\" \"glove.6B/glove.6B.100d.txt\" \"glove.6B/glove.6B.200d.txt\" \"glove.6B/glove.6B.300d.txt\" \"glove.42B.300d/glove.42B.300d.txt\" \"glove.840B.300d/glove.840B.300d.txt\" \"glove.twitter.27B/glove.twitter.27B.25d.txt\" \"glove.twitter.27B/glove.twitter.27B.50d.txt\" \"glove.twitter.27B/glove.twitter.27B.100d.txt\" \"glove.twitter.27B/glove.twitter.27B.200d.txt\" I’ll use the \"glove.6B/glove.6B.50d.txt\". This means that it was trained with 6 billion tokens, and it provides embeddings with 50-dimensional vectors. The load_embeddings function takes an optional second positional argument as an Int to choose from which index of the language_files to use. Finally, I just want the words “king”, “queen”, “man”, “woman”; so I am passing these words as a Set to the keep_words keyword argument:\njulia\u003e const glove = load_embeddings(GloVe{:en}, 1; keep_words=Set([\"king\", \"queen\", \"man\", \"woman\"])); Embeddings.EmbeddingTable{Matrix{Float32}, Vector{String}}(Float32[-0.094386 0.50451 -0.18153 0.37854; 0.43007 0.68607 0.64827 1.8233; … ; 0.53135 -0.64426 0.48764 0.0092753; -0.11725 -0.51042 -0.10467 -0.60284], [\"man\", \"king\", \"woman\", \"queen\"]) Watch out with the order that we get back. If you see the output of load_embeddings, the order is \"man\", \"king\", \"woman\", \"queen\"] Let’s see how a word is represented:\njulia\u003e queen = glove.embeddings[:, 4] 50-element Vector{Float32}: 0.37854 1.8233 -1.2648 ⋮ -2.2839 0.0092753 -0.60284 They are 50-dimensional vectors of Float32.\nNow, here’s the fun part: let’s add words and check the similarity between the result and some other word. A classical example is to start with the word “king”, subtract the word “men”, add the word “woman”, and check the distance of the result to the word “queen”:\njulia\u003e man = glove.embeddings[:, 1]; julia\u003e king = glove.embeddings[:, 2]; julia\u003e woman = glove.embeddings[:, 3]; julia\u003e cosine_dist(king - man + woman, queen) 0.13904202f0 This is less than 1/4 of the distance of “woman” to “king”:\njulia\u003e cosine_dist(woman, king) 0.58866215f0 Feel free to play around with others words. If you want suggestions, another classical example is:\ncosine_dist(Madrid - Spain + France, Paris) Conclusion I think that by allying interesting applications to abstract math topics like trigonometry is the vital missing piece in STEM education. I wish every new kid that is learning math could have the opportunity to contemplate how new and exciting technologies have some amazing simple math under the hood. If you liked this post, you would probably like linear algebra. I would highly recommend Gilbert Strang’s books and 3blue1brown series on linear algebra.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","wordCount":"1024","inLanguage":"en","datePublished":"2023-11-19T22:49:51-03:00","dateModified":"2024-02-11T15:55:02-03:00","author":{"@type":"Person","name":"Jose Storopoli"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://storopoli.io/2023-11-20-word_embeddings/"},"publisher":{"@type":"Organization","name":"Jose Storopoli, PhD","logo":{"@type":"ImageObject","url":"https://storopoli.io/assets/favicon.svg"}}}</script></head><body class=dark id=top><script crossorigin=anonymous src=/assets/js/theme.b20f95bb4da41ef90a2610a557a7000b2649a3f47282ec571676da6fc0427200.js integrity="sha256-sg+Vu02kHvkKJhClV6cACyZJo/RyguxXFnbab8BCcgA="></script><header class=header><div id=progressBar></div><nav class=nav><div class=logo><a href=https://storopoli.io/ accesskey=h title="Jose Storopoli, PhD (Alt + H)">Jose Storopoli, PhD</a><div class=logo-switches><button type=button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><input name=hamburger-input id=hamburger-input type=checkbox aria-label="Navigation Menu">
<label id=hamburger-menu for=hamburger-input></label><div class=overlay></div><ul id=menu><li><a href=https://storopoli.io/about/ title=About><span>About</span></a></li><li><a href=https://storopoli.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://storopoli.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://storopoli.io/>Home</a>&nbsp;»&nbsp;<a href=https://storopoli.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Word Embeddings</h1><div class=post-meta><span title='2023-11-19 22:49:51 -0300 -0300'>November 19, 2023</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Jose Storopoli&nbsp;|&nbsp;<a href=https://github.com/storopoli/storopoli.github.io/blob/main/content/posts/2023-11-20-word_embeddings/index.md rel="noopener noreferrer">Source code</a></div><div class=post-meta><span title="2024-02-11 15:55:02 -0300 -0300"><i>Last updated on February 11, 2024</i></span></div></header><div class="toc side"><details id=toc><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#word-embeddings aria-label="Word Embeddings">Word Embeddings</a></li><li><a href=#pre-trained-word-embeddings aria-label="Pre-Trained Word Embeddings">Pre-Trained Word Embeddings</a></li><li><a href=#julia-code aria-label="Julia Code">Julia Code</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#license aria-label=License>License</a></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=euclid.jpg#center alt="Euclid of Alexandria"></p><blockquote><p>Warning: This post has <a href=https://katex.org/>KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p></blockquote><p>I wish I could go back in time and tell my younger self
that you can make a machine understand human language with trigonometry.
That would definitely have made me more aware and interested in the
subject during my school years.
I would have looked at triangles, circles, sines, cosines, and tangents
in a whole different way.
Alas, better late than never.</p><p>In this post, we&rsquo;ll learn how to represent words using word embeddings,
and how to use basic trigonometry to play around with them.
Of course, we&rsquo;ll use <a href=https://julialang.org>Julia</a>.</p><h2 id=word-embeddings>Word Embeddings<a hidden class=anchor aria-hidden=true href=#word-embeddings>#</a></h2><p><strong><a href=https://en.wikipedia.org/wiki/Word_embedding>Word embeddings</a> is a way to
represent words as a real-valued vector that encodes the meaning of the word
in such a way that words that are closer in the vector space are expected
to be similar in meaning</strong>.</p><p>Ok, let&rsquo;s unwrap the above definition.
First, a <strong>real-valued vector</strong> is any vector which its elements belong to the real
numbers.
Generally we denote vectors with a bold lower-case letter,
and we denote its elements (also called components) using square brackets.
Hence, a vector $\bold{v}$ that has 3 elements, $1$, $2$, and $3$,
can be written as</p><p>$$\bold{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$$</p><p>Next, what &ldquo;close&rdquo; means for vectors?
We can use distance functions to get a measurable value.
The most famous and commonly used distance function is the <strong>Euclidean distance</strong>,
in honor of <a href=https://en.wikipedia.org/wiki/Euclid>Euclid</a>, the &ldquo;father of geometry&rdquo;,
and the guy pictured in the image at the top of this post.
The Euclidean distance is defined in trigonometry for 2-D and 3-D spaces.
However, it can be generalized to any dimension $n > 1$ by using vectors.</p><p>Since every word is represented by an $n$-dimensional vector,
we can use distances to compute a metric that represent similarity between vectors.
And, more interesting, we can add and subtract words
(or any other linear combination of one or more words) to generate new words.</p><p>Before we jump to code and examples, a quick note about how word embeddings
are constructed.
They are trained like a regular machine learning algorithm,
where the cost function measures the difference between
some vector distance between the vectors and a &ldquo;semantic distance&rdquo;.
The goal is to iteratively find good vector values that minimize the cost.
So, if a vector is close to another vector measured by a distance function,
but far apart measured by some semantic distance on the words that these
vectors represent, then the cost function will be higher.
The algorithm cannot change the semantic distance, it is treated as a fixed value.
However, it can change the vector elements&rsquo; values so that the vector distance function
closely resembles the semantic distance function.
Lastly, generally the dimensionality of the vectors used in word embeddings
are high, $n > 50$, since it needs a proper amount of dimensions in order to
represent all the semantic information of words with vectors.</p><h2 id=pre-trained-word-embeddings>Pre-Trained Word Embeddings<a hidden class=anchor aria-hidden=true href=#pre-trained-word-embeddings>#</a></h2><p>Generally we don&rsquo;t train our own word embeddings from scratch,
we use pre-trained ones.
Here is a list of some of the most popular ones:</p><ul><li><a href=https://code.google.com/archive/p/word2vec/>Word2Vec</a>:
One of the first public available word embeddings,
made by Google in 2013.
Only supports English.</li><li><a href=https://nlp.stanford.edu/projects/glove/>GloVe</a>:
made by Stanford in 2014.
Only supports English.</li><li><a href=https://fasttext.cc/>FastText</a>:
From Facebook, released in 2016.
Supports hundreds of languages.</li></ul><h2 id=julia-code>Julia Code<a hidden class=anchor aria-hidden=true href=#julia-code>#</a></h2><p>We will use the <a href=https://github.com/JuliaText/Embeddings.jl><code>Embeddings.jl</code></a>
package to easily load word embeddings as vectors,
and the <a href=https://github.com/JuliaStats/Distances.jl><code>Distances.jl</code></a>
package for the convenience of several distance functions.
This is a nice example of the Julia package ecosystem composability,
where one package can define types, another can define functions,
and another can define custom behavior of these functions on types that
are defined in other packages.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-jl data-lang=jl><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=k>using</span> <span class=n>Embeddings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=k>using</span> <span class=n>Distances</span>
</span></span></code></pre></div><p>Let&rsquo;s load the <a href=https://nlp.stanford.edu/projects/glove/>GloVe</a>
word embeddings.
First, let&rsquo;s check what we have in store to choose from
GloVe&rsquo;s English language embeddings:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-jl data-lang=jl><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>language_files</span><span class=p>(</span><span class=kt>GloVe</span><span class=p>{</span><span class=ss>:en</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=mi>20</span><span class=o>-</span><span class=n>element</span> <span class=kt>Vector</span><span class=p>{</span><span class=kt>String</span><span class=p>}</span><span class=o>:</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.50d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.100d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.200d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.300d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.42B.300d/glove.42B.300d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.840B.300d/glove.840B.300d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.25d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.50d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.100d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.200d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.50d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.100d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.200d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.6B/glove.6B.300d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.42B.300d/glove.42B.300d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.840B.300d/glove.840B.300d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.25d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.50d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.100d.txt&#34;</span>
</span></span><span class=line><span class=cl> <span class=s>&#34;glove.twitter.27B/glove.twitter.27B.200d.txt&#34;</span>
</span></span></code></pre></div><p>I&rsquo;ll use the <code>"glove.6B/glove.6B.50d.txt"</code>.
This means that it was trained with 6 billion tokens,
and it provides embeddings with 50-dimensional vectors.
The <code>load_embeddings</code> function takes an optional second positional
argument as an <code>Int</code> to choose from which index of the <code>language_files</code> to use.
Finally, I just want the words &ldquo;king&rdquo;, &ldquo;queen&rdquo;, &ldquo;man&rdquo;, &ldquo;woman&rdquo;;
so I am passing these words as a <code>Set</code> to the <code>keep_words</code> keyword argument:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-jl data-lang=jl><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=k>const</span> <span class=n>glove</span> <span class=o>=</span> <span class=n>load_embeddings</span><span class=p>(</span><span class=kt>GloVe</span><span class=p>{</span><span class=ss>:en</span><span class=p>},</span> <span class=mi>1</span><span class=p>;</span> <span class=n>keep_words</span><span class=o>=</span><span class=kt>Set</span><span class=p>([</span><span class=s>&#34;king&#34;</span><span class=p>,</span> <span class=s>&#34;queen&#34;</span><span class=p>,</span> <span class=s>&#34;man&#34;</span><span class=p>,</span> <span class=s>&#34;woman&#34;</span><span class=p>]));</span>
</span></span><span class=line><span class=cl><span class=n>Embeddings</span><span class=o>.</span><span class=kt>EmbeddingTable</span><span class=p>{</span><span class=kt>Matrix</span><span class=p>{</span><span class=kt>Float32</span><span class=p>},</span> <span class=kt>Vector</span><span class=p>{</span><span class=kt>String</span><span class=p>}}(</span><span class=kt>Float32</span><span class=p>[</span><span class=o>-</span><span class=mf>0.094386</span> <span class=mf>0.50451</span> <span class=o>-</span><span class=mf>0.18153</span> <span class=mf>0.37854</span><span class=p>;</span> <span class=mf>0.43007</span> <span class=mf>0.68607</span> <span class=mf>0.64827</span> <span class=mf>1.8233</span><span class=p>;</span> <span class=o>…</span> <span class=p>;</span> <span class=mf>0.53135</span> <span class=o>-</span><span class=mf>0.64426</span> <span class=mf>0.48764</span> <span class=mf>0.0092753</span><span class=p>;</span> <span class=o>-</span><span class=mf>0.11725</span> <span class=o>-</span><span class=mf>0.51042</span> <span class=o>-</span><span class=mf>0.10467</span> <span class=o>-</span><span class=mf>0.60284</span><span class=p>],</span> <span class=p>[</span><span class=s>&#34;man&#34;</span><span class=p>,</span> <span class=s>&#34;king&#34;</span><span class=p>,</span> <span class=s>&#34;woman&#34;</span><span class=p>,</span> <span class=s>&#34;queen&#34;</span><span class=p>])</span>
</span></span></code></pre></div><p>Watch out with the order that we get back.
If you see the output of <code>load_embeddings</code>,
the order is <code>"man", "king", "woman", "queen"]</code>
Let&rsquo;s see how a word is represented:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-jl data-lang=jl><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>queen</span> <span class=o>=</span> <span class=n>glove</span><span class=o>.</span><span class=n>embeddings</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=mi>4</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=mi>50</span><span class=o>-</span><span class=n>element</span> <span class=kt>Vector</span><span class=p>{</span><span class=kt>Float32</span><span class=p>}</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.37854</span>
</span></span><span class=line><span class=cl>  <span class=mf>1.8233</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>1.2648</span>
</span></span><span class=line><span class=cl>  <span class=o>⋮</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>2.2839</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.0092753</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>0.60284</span>
</span></span></code></pre></div><p>They are 50-dimensional vectors of <code>Float32</code>.</p><p>Now, here&rsquo;s the fun part:
let&rsquo;s add words and check the similarity between the
result and some other word.
A classical example is to start with the word &ldquo;king&rdquo;,
subtract the word &ldquo;men&rdquo;,
add the word &ldquo;woman&rdquo;,
and check the distance of the result to the word &ldquo;queen&rdquo;:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-jl data-lang=jl><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>man</span> <span class=o>=</span> <span class=n>glove</span><span class=o>.</span><span class=n>embeddings</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=mi>1</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>king</span> <span class=o>=</span> <span class=n>glove</span><span class=o>.</span><span class=n>embeddings</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>woman</span> <span class=o>=</span> <span class=n>glove</span><span class=o>.</span><span class=n>embeddings</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=mi>3</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>cosine_dist</span><span class=p>(</span><span class=n>king</span> <span class=o>-</span> <span class=n>man</span> <span class=o>+</span> <span class=n>woman</span><span class=p>,</span> <span class=n>queen</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=mf>0.13904202f0</span>
</span></span></code></pre></div><p>This is less than 1/4 of the distance of &ldquo;woman&rdquo; to &ldquo;king&rdquo;:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-jl data-lang=jl><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>cosine_dist</span><span class=p>(</span><span class=n>woman</span><span class=p>,</span> <span class=n>king</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=mf>0.58866215f0</span>
</span></span></code></pre></div><p>Feel free to play around with others words.
If you want suggestions, another classical example is:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>cosine_dist</span><span class=p>(</span><span class=n>Madrid</span> <span class=o>-</span> <span class=n>Spain</span> <span class=o>+</span> <span class=n>France</span><span class=p>,</span> <span class=n>Paris</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>I think that by allying interesting applications to abstract math topics
like trigonometry is the vital missing piece in STEM education.
I wish every new kid that is learning math could have the opportunity to contemplate
how new and exciting technologies have some amazing simple math under the hood.
If you liked this post, you would probably like <a href=https://en.wikipedia.org/wiki/Linear_algebra>linear algebra</a>.
I would highly recommend <a href=https://math.mit.edu/~gs/>Gilbert Strang&rsquo;s books</a>
and <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3blue1brown series on linear algebra</a>.</p><h2 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h2><p>This post is licensed under <a href=http://creativecommons.org/licenses/by-nc-sa/4.0/>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p><p><a href=http://creativecommons.org/licenses/by-nc-sa/4.0/><img loading=lazy src=https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png alt="CC BY-NC-SA 4.0"></a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://storopoli.io/tags/julia/>Julia</a></li><li><a href=https://storopoli.io/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://storopoli.io/2023-11-10-2023-11-13-soydev/><span class=title>« Prev</span><br><span>What is soydev? And why do I hate it</span>
</a><a class=next href=https://storopoli.io/2023-11-28-zero_cost_abstractions/><span class=title>Next »</span><br><span>Zero-cost Abstractions</span></a></nav></footer></article></main><footer class=footer><span><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></span>
<span>- Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer">Hugo</a> &
        <a href=https://github.com/Wonderfall/hugo-WonderMod/ rel=noopener>WonderMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script defer crossorigin=anonymous src=/assets/js/papermod.7ea300eda6d3653624a576fbc095ccd8a0c2977756acbe5de4114132a72cc7fa.js integrity="sha256-fqMA7abTZTYkpXb7wJXM2KDCl3dWrL5d5BFBMqcsx/o="></script></body></html>