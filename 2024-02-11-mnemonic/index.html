<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Seed Phrases and Entropy | Jose Storopoli, PhD</title>
<meta name=keywords content="bitcoin,cryptography,probability"><meta name=description content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
In this post, let&rsquo;s dive into a topic that is very important for anyone who uses the internet: passwords. We&rsquo;ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin &ldquo;seed phrases&rdquo;1.
Entropy Before we go into passwords, I&rsquo;ll introduce the concept of Entropy."><meta name=author content="Jose Storopoli"><link rel=canonical href=https://storopoli.io/2024-02-11-mnemonic/><link crossorigin=anonymous href=/assets/css/stylesheet.5d45b8bd1a3cf526e72959d51f1bdc688d8e97fa0df2a697a93df6bdc746feb4.css integrity="sha256-XUW4vRo89SbnKVnVHxvcaI2Ol/oN8qaXqT32vcdG/rQ=" rel="preload stylesheet" as=style><noscript><link crossorigin=anonymous href=/css/includes/noscript.30127fa68e36d08f5dd7f9d4e717dac42e729b844672afd0fbcacb0d9e508595.css integrity="sha256-MBJ/po420I9d1/nU5xfaxC5ym4RGcq/Q+8rLDZ5QhZU=" rel="preload stylesheet" as=style></noscript><link rel=icon href=https://storopoli.io/assets/favicon.svg><link rel=icon type=image/png sizes=16x16 href=https://storopoli.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://storopoli.io/favicon-32x32.png><link rel=apple-touch-icon href=https://storopoli.io/apple-touch-icon.png><link rel=mask-icon href=https://storopoli.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://storopoli.io/2024-02-11-mnemonic/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="Seed Phrases and Entropy"><meta property="og:description" content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
In this post, let&rsquo;s dive into a topic that is very important for anyone who uses the internet: passwords. We&rsquo;ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin &ldquo;seed phrases&rdquo;1.
Entropy Before we go into passwords, I&rsquo;ll introduce the concept of Entropy."><meta property="og:type" content="article"><meta property="og:url" content="https://storopoli.io/2024-02-11-mnemonic/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-11T15:59:02+00:00"><meta property="article:modified_time" content="2024-02-11T17:11:57+00:00"><meta property="og:site_name" content="Jose Storopoli, PhD"><meta name=twitter:card content="summary"><meta name=twitter:title content="Seed Phrases and Entropy"><meta name=twitter:description content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
In this post, let&rsquo;s dive into a topic that is very important for anyone who uses the internet: passwords. We&rsquo;ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin &ldquo;seed phrases&rdquo;1.
Entropy Before we go into passwords, I&rsquo;ll introduce the concept of Entropy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://storopoli.io/posts/"},{"@type":"ListItem","position":2,"name":"Seed Phrases and Entropy","item":"https://storopoli.io/2024-02-11-mnemonic/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Seed Phrases and Entropy","name":"Seed Phrases and Entropy","description":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nIn this post, let\u0026rsquo;s dive into a topic that is very important for anyone who uses the internet: passwords. We\u0026rsquo;ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin \u0026ldquo;seed phrases\u0026rdquo;1.\nEntropy Before we go into passwords, I\u0026rsquo;ll introduce the concept of Entropy.","keywords":["bitcoin","cryptography","probability"],"articleBody":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you’ll have to unfortunately enable JavaScript.\nIn this post, let’s dive into a topic that is very important for anyone who uses the internet: passwords. We’ll cover what the hell is Entropy, good password practices, and how it relates to Bitcoin “seed phrases”1.\nEntropy Before we go into passwords, I’ll introduce the concept of Entropy.\nEntropy is a measure of the amount of disorder in a system. It has its origins in Thermodynamics, where it’s used to measure the amount of energy in a system that is not available to do work.\nThe etymology of the word “Entropy” is after the Greek word for “transformation”.\nIt was given a proper statistical definition by Ludwig Boltzmann in 1870s. while establishing the field of Statistical Dynamics, a field of physics that studies the behavior of large collections of particles.\nLudwig Boltzmann In the context of Statistical Dynamics, Entropy is a measure of the number of ways a system can be arranged. The more ways a system can be arranged, the higher its Entropy. Specifically, Entropy is a logarithmic measure of the number of system states with significant probability of being occupied:\n$$S = -k \\cdot \\sum_i p_i \\ln p_i$$\nWhere:\n$S$: Entropy. $k$: Boltzmann’s constant, a physical constant that relates temperature to energy. $p_i$: probability of the system being in state $i$. In this formula, if all states are equally likely, i.e $p_i = \\frac{1}{N}$, where $N$ is the number of states, then the entropy is maximized. You can see this since a probability $p$ is a real number between 0 and 1, and as $N$ approaches infinity, the sum of the logarithms approaches negative infinity. Then, multiplying by $-k$ yields positive infinity.\nHow the hell Physics came to Passwords? There’s once a great men called Claude Shannon, who single-handedly founded the field of Information Theory, invented the concept of a Bit, and was the first to think about Boolean algebra in the context of electrical circuits. He laid the foundation for the Digital Revolution.\nIf you are happy using your smartphone, laptop, or any other digital device, in you high speed fiber internet connection, through a wireless router to send cats pictures to your friends, then you should thank Claude Shannon.\nClaude Shannon He was trying to find a formula to quantify the amount of information in a message. He wanted three things:\nThe measure should be a function of the probability of the message. Messages that are more likely should have less information. The measure should be additive. The information in a message should be the sum of the information in its parts. The measure should be continuous. Small changes in the message should result in small changes in the measure. He pretty much found that the formula for Entropy in statistical mechanics was a good measure of information. He called it Entropy to honor Boltzmann’s work. To differentiate it from the Statistical Dynamics’ Entropy, he changed the letter to $H$, in honor of Boltzmann’s $H$-theorem. So the formula for the Entropy of a message is:\n$$H(X) = −\\Sigma_{x \\in X} P(x_i​) \\log ​P(x_i​)$$\nWhere:\n$X$: random discrete variable. $H(X)$: Entropy of $X$ $P(x_i)$: probability of the random variable $X$ taking the value $x_i$. Also known as the probability mass function (PMF) of the discrete random variable $X$. $\\log$: base 2 logarithm, to measure the Entropy in bits. In information theory, the Entropy of a random variable is the average level of “information”, “surprise”, or “uncertainty” inherent to the variable’s possible outcomes2.\nLet’s take the simple example of a fair coin. The Entropy of the random variable $X$ that represents the outcome of a fair coin flip is:\n$$H(X) = −\\Sigma_{x \\in X} P(x_i​) \\log ​P(x_i​) = -\\left(\\frac{1}{2} \\log \\frac{1}{2} + \\frac{1}{2} \\log \\frac{1}{2}\\right) = 1 \\text{ bit}$$\nSo the outcome of a fair coin flip has 1 bit of Entropy. This means that the outcome of a fair coin flip has 1 bit of information, or 1 bit of uncertainty. Once the message is received, that the coin flip was heads or tails, the receiver has 1 bit of information about the outcome.\nAlternatively, we only need 1 bit to encode the outcome of a fair coin flip. Hence, there’s a connection between Entropy, search space, and information.\nAnother good example is the outcome of a fair 6-sided die. The Entropy of the random variable $X$ that represents the outcome of a fair 6-sided die is:\n$$H(X) = −\\Sigma_{x \\in X} P(x_i​) \\log ​P(x_i​) = - \\sum_{i=1}^6\\left(\\frac{1}{6} * \\log \\frac{1}{6} \\right) \\approx 2.58 \\text{ bits}$$\nThis means that the outcome of a fair 6-sided die has 2.58 bits of Entropy. we need $\\operatorname{ceil}(2.58) = 3$ bits to encode the outcome of a fair 6-sided die.\nEntropy and Passwords Ok now we come full circle. Let’s talk, finally, about passwords.\nIn the context of passwords, Entropy is a measure of how unpredictable a password is. The higher the Entropy, the harder it is to guess the password. The Entropy of a password is measured in bits, and it’s calculated using the formula:\n$$H = L \\cdot \\log_2(N)$$\nWhere:\n$H$: Entropy in bits $N$: number of possible characters in the password $L$: length of the password $\\log_2$:​ (N) calculates how many bits are needed to represent each character from the set. For example, if we have a password with 8 characters and each character can be any of the 26 lowercase letters, the standard english alphabet, the Entropy would be:\n$$H = 8 \\cdot \\log_2(26) \\approx 37.6 \\text{ bits}$$\nThis means that an attacker would need to try $2^{37.6} \\approx 2.01 \\cdot 10^{11}$ combinations3 to guess the password.\nIf the password were to include uppercase letters, numbers, and symbols (let’s assume 95 possible characters in total), the Entropy for an 8-character password would be:\n$$H = 8 \\cdot \\log_2(95) \\approx 52.6 \\text{ bits}$$\nThis means that an attacker would need to try $2^{52.6} \\approx 6.8 \\cdot 10^{15}$ combinations to guess the password.\nThis sounds a lot but it’s not that much.\nFor the calculations below, we’ll assume that the attacker now your dictionary set, i.e. the set of characters you use to create your password, and the password length.\nIf an attacker get a hold of an NVIDIA RTX 4090, MSRP USD 1,599, which can do 300 GH/s (300,000,000,000 hashes/second), i.e. $3 \\cdot 10^{11}$ hashes/second, it would take:\n8-length lowercase-only password: $$\\frac{2.01 \\cdot 10^{11}}{3 \\cdot 10^{11}} \\approx 0.67 \\text{ seconds}$$\n8-length password with uppercase letters, numbers, and symbols: $$\\frac{6.8 \\cdot 10^{15}}{3 \\cdot 10^{11}} \\approx 22114 \\text{ seconds} \\approx 6.14 \\text{ hours}$$\nSo, the first password would be cracked in less than a second, while the second would take a few hours. This with just one 1.5k USD GPU.\nBitcoin Seed Phrases Now that we understand Entropy and how it relates to passwords, let’s talk about bitcoin seed phrases1.\nRemember that our private key is a big-fucking number? If not, check my post on cryptographics basics.\nBIP-39 specifies how to use easy-to-remember seed phrases to store and recover private keys. The wordlist adheres to the following principles:\nsmart selection of words: the wordlist is created in such a way that it’s enough to type the first four letters to unambiguously identify the word. similar words avoided: word pairs like “build” and “built”, “woman” and “women”, or “quick” and “quickly” not only make remembering the sentence difficult but are also more error prone and more difficult to guess. Here is a simple 7-word seed phrase: brave sadness grocery churn wet mammal tube. Surprisingly enough, this badboy here gives you $77$ bits of Entropy, while also being easy to remember. This is due to the fact that the wordlist has 2048 words, so each word gives you $\\log_2(2048) = 11$ bits of Entropy4.\nThere’s a minor caveat to cover here. The last word in the seed phrase is a checksum, which is used to verify that the phrase is valid.\nSo, if you have a 12-word seed phrase, you have $11 \\cdot 11 = 121$ bits of Entropy. And for a 24-word seed phrase, you have $23 \\cdot 11 = 253$ bits of Entropy.\nThe National Institute of Standards and Technology (NIST) recommends a minimum of 112 bits of Entropy for all things cryptographic. And Bitcoin has a minimum of 128 bits of Entropy.\nDepending on your threat model, “Assume that your adversary is capable of a trillion guesses per second”, it can take a few years to crack a 121-bit Entropy seed phrase:\n$$\\frac{2^{121}}{10^{12}} \\approx 2.66 \\cdot 10^{24} \\text{ seconds} \\approx 3.08 \\cdot 10^{19} \\text{ days} \\approx 8.43 \\cdot 10^{16} \\text{ years}$$\nThat’s a lot of years. Now for a 253-bit Entropy seed phrase:\n$$\\frac{2^{253}}{10^{12}} \\approx 1.45 \\cdot 10^{64} \\text{ seconds} \\approx 1.68 \\cdot 10^{59} \\text{ days} \\approx 4.59 \\cdot 10^{56} \\text{ years}$$\nThat’s another huge number of years.\nSeed Phrases and Passwords You can also use a seed phrase as a password. The bonus point is that you don’t need to use the last word as a checksum, so you get 11 bits of Entropy free, compared to a Bitcoin seed phrase.\nRemember the 7-words badboy seed phrase we generated earlier? brave sadness grocery churn wet mammal tube.\nIt has $66$ bits of Entropy. This would take, assuming “that your adversary is capable of a trillion guesses per second”:\n$$\\frac{2^{77}}{10^{12}} \\approx 1.51 \\cdot 10^{11} \\text{ seconds} \\approx 1.75 \\cdot 10^{6} \\text{ days} \\approx 4.79 \\cdot 10^{3} \\text{ years}$$\nThat’s why tons of people use seed phrases as passwords. Even if you know the dictionary set and the length of the password, i.e. the number of words in the seed phrase, it would take a lot of years to crack it.\nConclusion Entropy is a measure of the amount of disorder in a system. In the context of passwords, it’s a measure of how unpredictable a password is. The higher the Entropy, the harder it is to guess the password.\nBitcoin seed phrases are a great way to store and recover private keys. They are easy to remember and have a high amount of Entropy. You can even use a seed phrase as a password.\nEven it your attacker is capable of a trillion guesses per second, like the NSA, it would take them a lot of years to crack even a 7-word seed phrase.\nIf you want to generate a seed phrase, you can use KeePassXC, which is a great open-source offline password manager that supports seed phrases5.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nseed phrases are technically called “mnemonic phrases”, but I’ll use the term “seed phrases” for the rest of the post. ↩︎ ↩︎\nthere is a Bayesian argument about the use of priors that should adhere to the Principle of Maximal Entropy ↩︎\ntechnically, we need to divide the number of combinations by 2, since we are assuming that the attacker is using a brute-force attack, which means that the attacker is trying all possible combinations, and the password could be at the beginning or at the end of the search space. This is called the birthday paradox, and it assumes that the password is uniformly distributed in the search space. ↩︎\nremember that $2^{11} = 2048$. ↩︎\ntechnically, KeePassXC uses the EFF wordlist, which has 7,776 words, so each word gives you $\\log_2(7776) \\approx 12.9$ bits of Entropy. They were created to be easy to use with 6-sided dice. ↩︎\n","wordCount":"1907","inLanguage":"en","datePublished":"2024-02-11T15:59:02Z","dateModified":"2024-02-11T17:11:57Z","author":{"@type":"Person","name":"Jose Storopoli"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://storopoli.io/2024-02-11-mnemonic/"},"publisher":{"@type":"Organization","name":"Jose Storopoli, PhD","logo":{"@type":"ImageObject","url":"https://storopoli.io/assets/favicon.svg"}}}</script></head><body class=dark id=top><script crossorigin=anonymous src=/assets/js/theme.b20f95bb4da41ef90a2610a557a7000b2649a3f47282ec571676da6fc0427200.js integrity="sha256-sg+Vu02kHvkKJhClV6cACyZJo/RyguxXFnbab8BCcgA="></script><header class=header><div id=progressBar></div><nav class=nav><div class=logo><a href=https://storopoli.io/ accesskey=h title="Jose Storopoli, PhD (Alt + H)">Jose Storopoli, PhD</a><div class=logo-switches><button type=button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><input name=hamburger-input id=hamburger-input type=checkbox aria-label="Navigation Menu">
<label id=hamburger-menu for=hamburger-input></label><div class=overlay></div><ul id=menu><li><a href=https://storopoli.io/about/ title=About><span>About</span></a></li><li><a href=https://storopoli.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://storopoli.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://storopoli.io/>Home</a>&nbsp;»&nbsp;<a href=https://storopoli.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Seed Phrases and Entropy</h1><div class=post-meta><span title='2024-02-11 15:59:02 +0000 UTC'>February 11, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Jose Storopoli&nbsp;|&nbsp;<a href=https://github.com/storopoli/storopoli.github.io/blob/main/content/posts/2024-02-11-mnemonic/index.md rel="noopener noreferrer">Source code</a></div><div class=post-meta></div></header><div class="toc side"><details id=toc><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#entropy aria-label=Entropy>Entropy</a><ul><li><a href=#how-the-hell-physics-came-to-passwords aria-label="How the hell Physics came to Passwords?">How the hell Physics came to Passwords?</a></li><li><a href=#entropy-and-passwords aria-label="Entropy and Passwords">Entropy and Passwords</a></li></ul></li><li><a href=#bitcoin-seed-phrases aria-label="Bitcoin Seed Phrases">Bitcoin Seed Phrases</a></li><li><a href=#seed-phrases-and-passwords aria-label="Seed Phrases and Passwords">Seed Phrases and Passwords</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#license aria-label=License>License</a></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=password_strength.png#center alt="Password Meme"></p><blockquote><p>Warning: This post has <a href=https://katex.org/>KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p></blockquote><p>In this post, let&rsquo;s dive into a topic that is very important for anyone who uses the internet: <strong>passwords</strong>.
We&rsquo;ll cover what the hell is <strong>Entropy</strong>,
good <strong>password practices</strong>,
and how it relates to <strong>Bitcoin &ldquo;seed phrases&rdquo;</strong><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><h2 id=entropy>Entropy<a hidden class=anchor aria-hidden=true href=#entropy>#</a></h2><p>Before we go into passwords,
I&rsquo;ll introduce the concept of <strong><em>Entropy</em></strong>.</p><p><a href=https://en.wikipedia.org/wiki/Entropy>Entropy</a>
is a measure of the <strong>amount of disorder in a system</strong>.
It has its origins in <strong>Thermodynamics</strong>,
where it&rsquo;s used to measure the amount of energy in a system that is not available to do work.</p><p>The etymology of the word &ldquo;Entropy&rdquo; is after the Greek word for &ldquo;transformation&rdquo;.</p><p>It was given a proper statistical definition by <a href=https://en.wikipedia.org/wiki/Ludwig_Boltzmann>Ludwig Boltzmann</a> in 1870s.
while establishing the field of <a href=https://en.wikipedia.org/wiki/Statistical_dynamics>Statistical Dynamics</a>,
a field of physics that studies the behavior of large collections of particles.</p><figure><img loading=lazy src=boltzmann.jpg#center alt="Ludwig Boltzmann" width=300><figcaption>Ludwig Boltzmann</figcaption></figure><p>In the context of Statistical Dynamics,
<strong>Entropy is a measure of the number of ways a system can be arranged</strong>.
The more ways a system can be arranged,
the higher its Entropy.
Specifically, <strong>Entropy is a logarithmic measure of the number of system states with significant probability of being occupied</strong>:</p><p>$$S = -k \cdot \sum_i p_i \ln p_i$$</p><p>Where:</p><ul><li>$S$: Entropy.</li><li>$k$: Boltzmann&rsquo;s constant, a physical constant that relates temperature to energy.</li><li>$p_i$: probability of the system being in state $i$.</li></ul><p>In this formula, if all states are equally likely,
i.e $p_i = \frac{1}{N}$,
where $N$ is the number of states,
then the entropy is maximized.
You can see this since a probability $p$ is a real number between 0 and 1,
and as $N$ approaches infinity,
the sum of the logarithms approaches negative infinity.
Then, multiplying by $-k$ yields positive infinity.</p><h3 id=how-the-hell-physics-came-to-passwords>How the hell Physics came to Passwords?<a hidden class=anchor aria-hidden=true href=#how-the-hell-physics-came-to-passwords>#</a></h3><p>There&rsquo;s once a great men called <a href=https://en.wikipedia.org/wiki/Claude_Shannon>Claude Shannon</a>,
who single-handedly founded the field of <a href=https://en.wikipedia.org/wiki/Information_theory><strong>Information Theory</strong></a>,
invented the concept of a <a href=https://en.wikipedia.org/wiki/Bit><strong>Bit</strong></a>,
and was the first to think about Boolean algebra in the context of electrical circuits.
He laid the foundation for the <a href=https://en.wikipedia.org/wiki/Digital_Revolution><strong>Digital Revolution</strong></a>.</p><p>If you are happy using your smartphone, laptop, or any other digital device,
in you high speed fiber internet connection,
through a wireless router to send cats pictures to your friends,
then you should thank Claude Shannon.</p><figure><img loading=lazy src=shannon.jpg#center alt="Claude Shannon" width=300><figcaption>Claude Shannon</figcaption></figure><p>He was trying to find a formula to quantify the amount of information in a message.
He wanted three things:</p><ol><li>The measure should be a <strong>function of the probability of the message</strong>.
Messages that are more likely should have less information.</li><li>The measure should be <strong>additive</strong>.
The information in a message should be the sum of the information in its parts.</li><li>The measure should be <strong>continuous</strong>.
Small changes in the message should result in small changes in the measure.</li></ol><p>He pretty much found that the formula for Entropy in statistical mechanics
was a good measure of information.
He called it <em>Entropy</em> to honor Boltzmann&rsquo;s work.
To differentiate it from the Statistical Dynamics&rsquo; Entropy,
he changed the letter to $H$,
in honor of <a href=https://en.wikipedia.org/wiki/H-theorem>Boltzmann&rsquo;s $H$-theorem</a>.
So the formula for the Entropy of a message is:</p><p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​)$$</p><p>Where:</p><ul><li>$X$: random discrete variable.</li><li>$H(X)$: Entropy of $X$</li><li>$P(x_i)$: probability of the random variable $X$ taking the value $x_i$.
Also known as the probability mass function (PMF) of the discrete random variable $X$.</li><li>$\log$: base 2 logarithm, to measure the Entropy in bits.</li></ul><p>In information theory,
the <strong>Entropy of a random variable is the average level of &ldquo;information&rdquo;, &ldquo;surprise&rdquo;,
or &ldquo;uncertainty&rdquo; inherent to the variable&rsquo;s possible outcomes</strong><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>Let&rsquo;s take the simple example of a fair coin.
The Entropy of the random variable $X$ that represents the outcome of a fair coin flip is:</p><p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = -\left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{2}\right) = 1 \text{ bit}$$</p><p>So the outcome of a fair coin flip has 1 bit of Entropy.
This means that the outcome of a fair coin flip has 1 bit of information,
or 1 bit of uncertainty.
Once the message is received,
that the coin flip was heads or tails,
the receiver has 1 bit of information about the outcome.</p><p>Alternatively, we only need 1 bit to encode the outcome of a fair coin flip.
Hence, there&rsquo;s a connection between Entropy, search space, and information.</p><p>Another good example is the outcome of a fair 6-sided die.
The Entropy of the random variable $X$ that represents the outcome of a fair 6-sided die is:</p><p>$$H(X) = −\Sigma_{x \in X} P(x_i​) \log ​P(x_i​) = - \sum_{i=1}^6\left(\frac{1}{6} * \log \frac{1}{6} \right) \approx 2.58 \text{ bits}$$</p><p>This means that the outcome of a fair 6-sided die has 2.58 bits of Entropy.
we need $\operatorname{ceil}(2.58) = 3$ bits to encode the outcome of a fair 6-sided die.</p><h3 id=entropy-and-passwords>Entropy and Passwords<a hidden class=anchor aria-hidden=true href=#entropy-and-passwords>#</a></h3><p>Ok now we come full circle.
Let&rsquo;s talk, finally, about passwords.</p><p>In the context of passwords, <strong>Entropy</strong> is a measure of how unpredictable a password is.
The higher the Entropy, the harder it is to guess the password.
The Entropy of a password is measured in bits,
and it&rsquo;s calculated using the formula:</p><p>$$H = L \cdot \log_2(N)$$</p><p>Where:</p><ul><li>$H$: Entropy in bits</li><li>$N$: number of possible characters in the password</li><li>$L$: length of the password</li><li>$\log_2$:​ (N) calculates how many bits are needed to represent each character from the set.</li></ul><p>For example,
if we have a password with 8 characters and each character can be any of the 26 lowercase letters,
the standard english alphabet,
the Entropy would be:</p><p>$$H = 8 \cdot \log_2(26) \approx 37.6 \text{ bits}$$</p><p>This means that an attacker would need to try $2^{37.6} \approx 2.01 \cdot 10^{11}$ combinations<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> to guess the password.</p><p>If the password were to include uppercase letters, numbers, and symbols
(let&rsquo;s assume 95 possible characters in total),
the Entropy for an 8-character password would be:</p><p>$$H = 8 \cdot \log_2(95) \approx 52.6 \text{ bits}$$</p><p>This means that an attacker would need to try $2^{52.6} \approx 6.8 \cdot 10^{15}$ combinations to guess the password.</p><p>This sounds a lot but it&rsquo;s not that much.</p><p>For the calculations below, we&rsquo;ll assume that the attacker now your dictionary set,
i.e. the set of characters you use to create your password,
and the password length.</p><p>If an attacker get a hold of an NVIDIA RTX 4090,
MSRP USD 1,599, which can do
<a href=https://www.tomshardware.com/news/rtx-4090-password-cracking-comparison>300 GH/s (300,000,000,000 hashes/second)</a>,
i.e. $3 \cdot 10^{11}$ hashes/second,
it would take:</p><ol><li>8-length lowercase-only password:</li></ol><p>$$\frac{2.01 \cdot 10^{11}}{3 \cdot 10^{11}} \approx 0.67 \text{ seconds}$$</p><ol><li>8-length password with uppercase letters, numbers, and symbols:</li></ol><p>$$\frac{6.8 \cdot 10^{15}}{3 \cdot 10^{11}} \approx 22114 \text{ seconds} \approx 6.14 \text{ hours}$$</p><p>So, the first password would be cracked in less than a second,
while the second would take a few hours.
This with just one 1.5k USD GPU.</p><h2 id=bitcoin-seed-phrases>Bitcoin Seed Phrases<a hidden class=anchor aria-hidden=true href=#bitcoin-seed-phrases>#</a></h2><p>Now that we understand Entropy and how it relates to passwords,
let&rsquo;s talk about bitcoin seed phrases<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>Remember that our private key is a big-fucking number?
If not, check my <a href=../2024-02-05-crypto-basics/>post on cryptographics basics</a>.</p><p><a href=https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki>BIP-39</a>
specifies how to use easy-to-remember seed phrases to store and recover
private keys.
The <a href=https://github.com/bitcoin/bips/blob/master/bip-0039/english.txt>wordlist</a>
adheres to the following principles:</p><ol><li><strong>smart selection of words</strong>:
the wordlist is created in such a way that it&rsquo;s enough to type the first four
letters to unambiguously identify the word.</li><li><strong>similar words avoided</strong>:
word pairs like &ldquo;build&rdquo; and &ldquo;built&rdquo;, &ldquo;woman&rdquo; and &ldquo;women&rdquo;, or &ldquo;quick&rdquo; and &ldquo;quickly&rdquo;
not only make remembering the sentence difficult but are also more error
prone and more difficult to guess.</li></ol><p>Here is a simple 7-word seed phrase: <code>brave sadness grocery churn wet mammal tube</code>.
Surprisingly enough, this badboy here gives you $77$ bits of Entropy,
while also being easy to remember.
This is due to the fact that the wordlist has 2048 words,
so each word gives you $\log_2(2048) = 11$ bits of Entropy<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><p>There&rsquo;s a minor caveat to cover here.
The last word in the seed phrase is a checksum,
which is used to verify that the phrase is valid.</p><p>So, if you have a 12-word seed phrase,
you have $11 \cdot 11 = 121$ bits of Entropy.
And for a 24-word seed phrase,
you have $23 \cdot 11 = 253$ bits of Entropy.</p><p>The National Institute of Standards and Technology (NIST) recommends a
<a href=https://crypto.stackexchange.com/a/87059>minimum of 112 bits of Entropy for all things cryptographic</a>.
And Bitcoin has a <a href=https://bitcoin.stackexchange.com/a/118929>minimum of 128 bits of Entropy</a>.</p><p>Depending on your threat model,
<a href=https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html>&ldquo;Assume that your adversary is capable of a trillion guesses per second&rdquo;</a>,
it can take a few years to crack a 121-bit Entropy seed phrase:</p><p>$$\frac{2^{121}}{10^{12}} \approx 2.66 \cdot 10^{24} \text{ seconds} \approx 3.08 \cdot 10^{19} \text{ days} \approx 8.43 \cdot 10^{16} \text{ years}$$</p><p>That&rsquo;s a lot of years.
Now for a 253-bit Entropy seed phrase:</p><p>$$\frac{2^{253}}{10^{12}} \approx 1.45 \cdot 10^{64} \text{ seconds} \approx 1.68 \cdot 10^{59} \text{ days} \approx 4.59 \cdot 10^{56} \text{ years}$$</p><p>That&rsquo;s another huge number of years.</p><h2 id=seed-phrases-and-passwords>Seed Phrases and Passwords<a hidden class=anchor aria-hidden=true href=#seed-phrases-and-passwords>#</a></h2><p>You can also use a seed phrase as a password.
The bonus point is that you don&rsquo;t need to use the last word as a checksum,
so you get 11 bits of Entropy free, compared to a Bitcoin seed phrase.</p><p>Remember the 7-words badboy seed phrase we generated earlier?
<code>brave sadness grocery churn wet mammal tube</code>.</p><p>It has $66$ bits of Entropy.
This would take, assuming
<a href=https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html>&ldquo;that your adversary is capable of a trillion guesses per second&rdquo;</a>:</p><p>$$\frac{2^{77}}{10^{12}} \approx 1.51 \cdot 10^{11} \text{ seconds} \approx 1.75 \cdot 10^{6} \text{ days} \approx 4.79 \cdot 10^{3} \text{ years}$$</p><p>That&rsquo;s why tons of people use seed phrases as passwords.
Even if you know the dictionary set and the length of the password,
i.e. the number of words in the seed phrase,
it would take a lot of years to crack it.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Entropy is a measure of the amount of disorder in a system.
In the context of passwords, it&rsquo;s a measure of how unpredictable a password is.
The higher the Entropy, the harder it is to guess the password.</p><p>Bitcoin seed phrases are a great way to store and recover private keys.
They are easy to remember and have a high amount of Entropy.
You can even use a seed phrase as a password.</p><p>Even it your attacker is capable of a trillion guesses per second,
like the <a href=https://www.nytimes.com/2013/08/18/magazine/laura-poitras-snowden.html>NSA</a>,
it would take them a lot of years to crack even a 7-word seed phrase.</p><p>If you want to generate a seed phrase,
you can use <a href=https://keepassxc.org/>KeePassXC</a>,
which is a great open-source <strong><em>offline</em></strong> password manager that supports seed phrases<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</p><h2 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h2><p>This post is licensed under <a href=http://creativecommons.org/licenses/by-nc-sa/4.0/>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p><p><a href=http://creativecommons.org/licenses/by-nc-sa/4.0/><img loading=lazy src=https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png alt="CC BY-NC-SA 4.0"></a></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>seed phrases are technically called &ldquo;mnemonic phrases&rdquo;,
but I&rsquo;ll use the term &ldquo;seed phrases&rdquo; for the rest of the post.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>there is a Bayesian argument about
the use of priors that should adhere to the
<a href=https://en.wikipedia.org/wiki/Principle_of_maximum_entropy>Principle of Maximal Entropy</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>technically, we need to divide the number of combinations by 2,
since we are assuming that the attacker is using a brute-force attack,
which means that the attacker is trying all possible combinations,
and the password could be at the beginning or at the end of the search space.
This is called the <a href=https://en.wikipedia.org/wiki/Birthday_problem>birthday paradox</a>,
and it assumes that the password is uniformly distributed in the search space.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>remember that $2^{11} = 2048$.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>technically, KeePassXC uses the <a href=https://www.eff.org/files/2016/07/18/eff_large_wordlist.txt>EFF wordlist</a>,
which has 7,776 words, so each word gives you $\log_2(7776) \approx 12.9$ bits of Entropy.
They were created to be easy to use with 6-sided dice.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://storopoli.io/tags/bitcoin/>Bitcoin</a></li><li><a href=https://storopoli.io/tags/cryptography/>Cryptography</a></li><li><a href=https://storopoli.io/tags/probability/>Probability</a></li></ul><nav class=paginav><a class=prev href=https://storopoli.io/2023-11-23-lindley_paradox/><span class=title>« Prev</span><br><span>Lindley's Paradox, or The consistency of Bayesian Thinking</span>
</a><a class=next href=https://storopoli.io/2023-12-04-bayesian_models_ci/><span class=title>Next »</span><br><span>Testing Bayesian Models with Nix and GitHub Actions</span></a></nav></footer></article></main><footer class=footer><span><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></span>
<span>- Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer">Hugo</a> &
        <a href=https://github.com/Wonderfall/hugo-WonderMod/ rel=noopener>WonderMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script defer crossorigin=anonymous src=/assets/js/papermod.7ea300eda6d3653624a576fbc095ccd8a0c2977756acbe5de4114132a72cc7fa.js integrity="sha256-fqMA7abTZTYkpXb7wJXM2KDCl3dWrL5d5BFBMqcsx/o="></script></body></html>