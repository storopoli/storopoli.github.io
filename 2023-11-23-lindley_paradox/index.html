<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lindley's Paradox, or The consistency of Bayesian Thinking | Jose Storopoli, PhD</title>
<meta name=keywords content="bayesian,probability,julia"><meta name=description content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
Dennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: &ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell&rsquo;s Rule2."><meta name=author content="Jose Storopoli"><link rel=canonical href=https://storopoli.io/2023-11-23-lindley_paradox/><link crossorigin=anonymous href=/assets/css/stylesheet.5d45b8bd1a3cf526e72959d51f1bdc688d8e97fa0df2a697a93df6bdc746feb4.css integrity="sha256-XUW4vRo89SbnKVnVHxvcaI2Ol/oN8qaXqT32vcdG/rQ=" rel="preload stylesheet" as=style><noscript><link crossorigin=anonymous href=/css/includes/noscript.30127fa68e36d08f5dd7f9d4e717dac42e729b844672afd0fbcacb0d9e508595.css integrity="sha256-MBJ/po420I9d1/nU5xfaxC5ym4RGcq/Q+8rLDZ5QhZU=" rel="preload stylesheet" as=style></noscript><link rel=icon href=https://storopoli.io/assets/favicon.svg><link rel=icon type=image/png sizes=16x16 href=https://storopoli.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://storopoli.io/favicon-32x32.png><link rel=apple-touch-icon href=https://storopoli.io/apple-touch-icon.png><link rel=mask-icon href=https://storopoli.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="Lindley's Paradox, or The consistency of Bayesian Thinking"><meta property="og:description" content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
Dennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: &ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell&rsquo;s Rule2."><meta property="og:type" content="article"><meta property="og:url" content="https://storopoli.io/2023-11-23-lindley_paradox/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-22T07:06:59-03:00"><meta property="article:modified_time" content="2024-02-11T15:55:02-03:00"><meta property="og:site_name" content="Jose Storopoli, PhD"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lindley's Paradox, or The consistency of Bayesian Thinking"><meta name=twitter:description content="Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you&rsquo;ll have to unfortunately enable JavaScript.
Dennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: &ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell&rsquo;s Rule2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://storopoli.io/posts/"},{"@type":"ListItem","position":2,"name":"Lindley's Paradox, or The consistency of Bayesian Thinking","item":"https://storopoli.io/2023-11-23-lindley_paradox/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lindley's Paradox, or The consistency of Bayesian Thinking","name":"Lindley\u0027s Paradox, or The consistency of Bayesian Thinking","description":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2.","keywords":["bayesian","probability","julia"],"articleBody":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you’ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: “Inside every non-Bayesian there is a Bayesian struggling to get out”; and one of my favorite heuristics around prior probabilities: Cromwell’s Rule2. Lindley predicted in 1975 that “Bayesian methods will indeed become pervasive, enabled by the development of powerful computing facilities” (Lindley, 1975). You can find more about all of Lindley’s achievements in his obituary.\nLindley’s Paradox Lindley’s paradox3 is a counterintuitive situation in statistics in which the Bayesian and frequentist approaches to a hypothesis testing problem give different results for certain choices of the prior distribution.\nMore formally, the paradox is as follows. We have some parameter $\\theta$ that we are interested in. Then, we proceed with an experiment to test two competing hypotheses:\n$H_0$ (also known as null hypothesis): there is no “effect”, or, more specifically, $\\theta = 0$. $H_a$ (also known as alternative hypothesis): there is an “effect”, or, more specifically, $\\theta \\ne 0$. The paradox occurs when two conditions are met:\nThe result of the experiment is significant by a frequentist test of $H_0$, which indicates sufficient evidence to reject $H_0$, at a certain threshold of probability4. The posterior probability (Bayesian approach) of $H_0 \\mid \\theta$ (null hypothesis given $\\theta$) is high, which indicates strong evidence that $H_0$ should be favored over $H_a$, that is, to not reject $H_0$. These results can occur at the same time when $H_0$ is very specific, $H_a$ more diffuse, and the prior distribution does not strongly favor one or the other. These conditions are pervasive across science and common in traditional null-hypothesis significance testing approaches.\nThis is a duel of frequentist versus Bayesian approaches, and one of the many in which Bayesian emerges as the most coherent. Let’s give a example and go over the analytical result with a ton of math, but also a computational result with Julia.\nExample Here’s the setup for the example. In a certain city 49,581 boys and 48,870 girls have been born over a certain time period. The observed proportion of male births is thus $\\frac{49,581}{98,451} \\approx 0.5036$.\nWe assume that the birth of a child is independent with a certain probability $\\theta$. Since our data is a sequence of $n$ independent Bernoulli trials, i.e., $n$ independent random experiments with exactly two possible outcomes: “success” and “failure”, in which the probability of success is the same every time the experiment is conducted. We can safely assume that it follows a binomial distribution with parameters:\n$n$: the number of “trials” (or the total number of births). $\\theta$: the probability of male births. We then set up our two competing hypotheses:\n$H_0$: $\\theta = 0.5$. $H_a$: $\\theta \\ne 0.5$. Analytical Solution This is a toy-problem and, like most toy problems, we can solve it analytically5 for both the frequentist and the Bayesian approaches.\nAnalytical Solutions – Frequentist Approach The frequentist approach to testing $H_0$ is to compute a $p$-value4, the probability of observing births of boys at least as large as 49,581 assuming $H_0$ is true. Because the number of births is very large, we can use a normal approximation6 for the binomial-distributed number of male births. Let’s define $X$ as the total number of male births, then $X$ follows a normal distribution:\n$$X \\sim \\text{Normal}(\\mu, \\sigma)$$\nwhere $\\mu$ is the mean parameter, $n \\theta$ in our case, and $\\sigma$ is the standard deviation parameter, $\\sqrt{n \\theta (1 - \\theta)}$. We need to calculate the conditional probability of $X \\geq \\frac{49,581}{98,451} \\approx 0.5036$ given $\\mu = n \\theta = 98,451 \\cdot \\frac{1}{2} = 49,225.5$ and $\\sigma = \\sqrt{n \\theta (1 - \\theta)} = \\sqrt{98,451 \\cdot \\frac{1}{2} \\cdot (1 - \\frac{1}{2})}$:\n$$P(X \\ge 0.5036 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75})$$\nThis is basically a cumulative distribution function (CDF) of $X$ on the interval $[49,225.5, 98,451]$:\n$$\\int_{49,225.5}^{98,451} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{\\left( \\frac{x - \\mu}{\\sigma} \\right)^2}{2}} dx$$\nAfter inserting the values and doing some arithmetic, our answer is approximately $0.0117$. Note that this is a one-sided test, since it is symmetrical, the two-sided test would be $0.0117 \\cdot 2 = 0.0235$. Since we don’t deviate from the Fisher’s canon, this is well below the 5% threshold. Hooray! We rejected the null hypothesis! Quick! Grab a frequentist celebratory cigar! But, wait. Let’s check the Bayesian approach.\nAnalytical Solutions – Bayesian Approach For the Bayesian approach, we need to set prior probabilities on both hypotheses. Since we do not favor one from another, let’s set equal prior probabilities:\n$$P(H_0) = P(H_a) = \\frac{1}{2}$$\nAdditionally, all parameters of interest need a prior distribution. So, let’s put a prior distribution on $\\theta$. We could be fancy here, but let’s not. We’ll use a uniform distribution on $[0, 1]$.\nWe have everything we need to compute the posterior probability of $H_0$ given $\\theta$. For this, we’ll use Bayes theorem7:\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\nNow again let’s plug in all the values:\n$$P(H_0 \\mid \\theta) = \\frac{P(\\theta \\mid H_0) P(H_0)}{P(\\theta)}$$\nNote that by the axioms of probability and by the product rule of probability we can decompose $P(\\theta)$ into:\n$$P(\\theta) = P(\\theta \\mid H_0) P(H_0) + P(\\theta \\mid H_a) P(H_a)$$\nAgain, we’ll use the normal approximation:\n$$ \\begin{aligned} \u0026P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \\\\ \u0026= \\frac{ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\left( \\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 } { \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{ \\left( -\\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 + \\int_0^1 \\frac {1}{\\sqrt{2 \\pi \\sigma^2} } e^{- \\left( \\frac{\\mu - \\mu \\cdot \\theta)}{2 \\sigma} \\right)^2}d \\theta \\cdot 0.5 } \\\\ \u0026= 0.9505 \\end{aligned} $$\nThe likelihood of the alternative hypothesis, $P(\\theta \\mid H_a)$, is just the CDF of all possible values of $\\theta \\ne 0.5$.\n$$P(H_0 \\mid \\text{data}) = P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \u003e 0.95$$\nAnd we fail to reject the null hypothesis, in frequentist terms. However, we can also say in Bayesian terms, that we strongly favor $H_0$ over $H_a$.\nQuick! Grab the Bayesian celebratory cigar! The null is back on the game!\nComputational Solutional For the computational solution, we’ll use Julia and the following packages:\nHypothesisTest.jl Turing.jl Computational Solutions – Frequentist Approach We can perform a BinomialTest with HypothesisTest.jl:\njulia\u003e using HypothesisTests julia\u003e BinomialTest(49_225, 98_451, 0.5036) Binomial test ------------- Population details: parameter of interest: Probability of success value under h_0: 0.5036 point estimate: 0.499995 95% confidence interval: (0.4969, 0.5031) Test summary: outcome with 95% confidence: reject h_0 two-sided p-value: 0.0239 Details: number of observations: 98451 number of successes: 49225 This is the two-sided test, and I had to round $49,225.5$ to $49,225$ since BinomialTest do not support real numbers. But the results match with the analytical solution, we still reject the null.\nComputational Solutions – Bayesian Approach Now, for the Bayesian computational approach, I’m going to use a generative modeling approach, and one of my favorites probabilistic programming languages, Turing.jl:\njulia\u003e using Turing julia\u003e @model function birth_rate() θ ~ Uniform(0, 1) total_births = 98_451 male_births ~ Binomial(total_births, θ) end; julia\u003e model = birth_rate() | (; male_births = 49_225); julia\u003e chain = sample(model, NUTS(1_000, 0.8), MCMCThreads(), 1_000, 4) Chains MCMC chain (1000×13×4 Array{Float64, 3}): Iterations = 1001:1:2000 Number of chains = 4 Samples per chain = 1000 Wall duration = 0.2 seconds Compute duration = 0.19 seconds parameters = θ internals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size Summary Statistics parameters mean std mcse ess_bulk ess_tail rhat ess_per_sec Symbol Float64 Float64 Float64 Float64 Float64 Float64 Float64 θ 0.4999 0.0016 0.0000 1422.2028 2198.1987 1.0057 7368.9267 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 θ 0.4969 0.4988 0.4999 0.5011 0.5031 We can see from the output of the quantiles that the 95% quantile for $\\theta$ is the interval $(0.4969, 0.5031)$. Although it overlaps zero, that is not the equivalent of a hypothesis test. For that, we’ll use the highest posterior density interval (HPDI), which is defined as “choosing the narrowest interval” that captures a certain posterior density threshold value. In this case, we’ll use a threshold interval of 95%, i.e. an $\\alpha = 0.05$:\njulia\u003e hpd(chain; alpha=0.05) HPD parameters lower upper Symbol Float64 Float64 θ 0.4970 0.5031 We see that we fail to reject the null, $\\theta = 0.5$ at $\\alpha = 0.05$ which is in accordance with the analytical solution.\nWhy the Frequentist and Bayesian Approaches Disagree Why do the approaches disagree? What is going on under the hood?\nThe answer is disappointing8. The main problem is that the frequentist approach only allows fixed significance levels with respect to sample size. Whereas the Bayesian approach is consistent and robust to sample size variations.\nTaken to extreme, in some cases, due to huge sample sizes, the $p$-value is pretty much a proxy for sample size and have little to no utility on hypothesis testing. This is known as $p$-hacking9.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nReferences Lindley, Dennis V. “The future of statistics: A Bayesian 21st century”. Advances in Applied Probability 7 (1975): 106-115.\nas far as I know there’s only one coherent approach to uncertainty, and it is the Bayesian approach. Otherwise, as de Finetti and Ramsey proposed, you are susceptible to a Dutch book. This is a topic for another blog post… ↩︎\nCromwell’s rule states that the use of prior probabilities of 1 (“the event will definitely occur”) or 0 (“the event will definitely not occur”) should be avoided, except when applied to statements that are logically true or false. Hence, anything that is not a math theorem should have priors in $(0,1)$. The reference comes from Oliver Cromwell, asking, very politely, for the Church of Scotland to consider that their prior probability might be wrong. This footnote also deserves a whole blog post… ↩︎\nStigler’s law of eponymy states that no scientific discovery is named after its original discoverer. The paradox was already was discussed in Harold Jeffreys' 1939 textbook. Also, fun fact, Stigler’s is not the original creator of such law… Now that’s a self-referential paradox, and a broad version of the Halting problem, which should earn its own footnote. Nevertheless, we are getting into self-referential danger zone here with footnotes’ of footnotes’ of footnotes’… ↩︎\nthis is called $p$-value and can be easily defined as “the probability of sampling data from a target population given that $H_0$ is true as the number of sampling procedures $\\to \\infty$”. Yes, it is not that intuitive, and it deserves not a blog post, but a full curriculum to hammer it home. ↩︎ ↩︎\nthat is not true for most of the real-world problems. For Bayesian approaches, we need to run computational asymptotic exact approximations using a class of methods called Markov chain Monte Carlo (MCMC). Furthermore, for some nasty problems we need to use different set of methods called variational inference (VI) or approximate Bayesian computation (ABC). ↩︎\nif you are curious about how this approximation works, check the backup slides of my open access and open source graduate course on Bayesian statistics. ↩︎\nBayes’ theorem is officially called Bayes-Price-Laplace theorem. Bayes was trying to disprove David Hume’s argument that miracles did not exist (How dare he?). He used the probabilistic approach of trying to quantify the probability of a parameter (god exists) given data (miracles happened). He died without publishing any of his ideas. His wife probably freaked out when she saw the huge pile of notes that he had and called his buddy Richard Price to figure out what to do with it. Price struck gold and immediately noticed the relevance of Bayes’ findings. He read it aloud at the Royal Society. Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes, used the same probabilistic approach to perform statistical inference using France’s first census data in the early-Napoleonic era. Somehow we had the answer to statistical inference back then, and we had to rediscover everything again in the late-20th century… ↩︎\ndisappointing because most of published scientific studies suffer from this flaw. ↩︎\nand, like all footnotes here, it deserves its own blog post… ↩︎\n","wordCount":"2063","inLanguage":"en","datePublished":"2023-11-22T07:06:59-03:00","dateModified":"2024-02-11T15:55:02-03:00","author":{"@type":"Person","name":"Jose Storopoli"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://storopoli.io/2023-11-23-lindley_paradox/"},"publisher":{"@type":"Organization","name":"Jose Storopoli, PhD","logo":{"@type":"ImageObject","url":"https://storopoli.io/assets/favicon.svg"}}}</script></head><body class=dark id=top><script crossorigin=anonymous src=/assets/js/theme.b20f95bb4da41ef90a2610a557a7000b2649a3f47282ec571676da6fc0427200.js integrity="sha256-sg+Vu02kHvkKJhClV6cACyZJo/RyguxXFnbab8BCcgA="></script><header class=header><div id=progressBar></div><nav class=nav><div class=logo><a href=https://storopoli.io/ accesskey=h title="Jose Storopoli, PhD (Alt + H)">Jose Storopoli, PhD</a><div class=logo-switches><button type=button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><input name=hamburger-input id=hamburger-input type=checkbox aria-label="Navigation Menu">
<label id=hamburger-menu for=hamburger-input></label><div class=overlay></div><ul id=menu><li><a href=https://storopoli.io/about/ title=About><span>About</span></a></li><li><a href=https://storopoli.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://storopoli.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://storopoli.io/>Home</a>&nbsp;»&nbsp;<a href=https://storopoli.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Lindley's Paradox, or The consistency of Bayesian Thinking</h1><div class=post-meta><span title='2023-11-22 07:06:59 -0300 -0300'>November 22, 2023</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Jose Storopoli&nbsp;|&nbsp;<a href=https://github.com/storopoli/storopoli.github.io/blob/main/content/posts/2023-11-23-lindley_paradox/index.md rel="noopener noreferrer">Source code</a></div><div class=post-meta><span title="2024-02-11 15:55:02 -0300 -0300"><i>Last updated on February 11, 2024</i></span></div></header><div class="toc side"><details id=toc><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#lindleys-paradox aria-label="Lindley&rsquo;s Paradox">Lindley&rsquo;s Paradox</a></li><li><a href=#example aria-label=Example>Example</a><ul><li><a href=#analytical-solution aria-label="Analytical Solution">Analytical Solution</a><ul><li><a href=#analytical-solutions----frequentist-approach aria-label="Analytical Solutions &ndash; Frequentist Approach">Analytical Solutions &ndash; Frequentist Approach</a></li><li><a href=#analytical-solutions----bayesian-approach aria-label="Analytical Solutions &ndash; Bayesian Approach">Analytical Solutions &ndash; Bayesian Approach</a></li></ul></li><li><a href=#computational-solutional aria-label="Computational Solutional">Computational Solutional</a><ul><li><a href=#computational-solutions----frequentist-approach aria-label="Computational Solutions &ndash; Frequentist Approach">Computational Solutions &ndash; Frequentist Approach</a></li><li><a href=#computational-solutions----bayesian-approach aria-label="Computational Solutions &ndash; Bayesian Approach">Computational Solutions &ndash; Bayesian Approach</a></li></ul></li></ul></li><li><a href=#why-the-frequentist-and-bayesian-approaches-disagree aria-label="Why the Frequentist and Bayesian Approaches Disagree">Why the Frequentist and Bayesian Approaches Disagree</a></li><li><a href=#license aria-label=License>License</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=lindley.jpg#center alt="Dennis Lindley"></p><blockquote><p>Warning: This post has <a href=https://katex.org/>KaTeX</a> enabled,
so if you want to view the rendered math formulas,
you&rsquo;ll have to unfortunately enable JavaScript.</p></blockquote><p><a href=https://en.wikipedia.org/wiki/Dennis_Lindley>Dennis Lindley</a>,
one of my many heroes,
was an English statistician,
decision theorist and leading advocate of Bayesian statistics.
He published a pivotal book,
<a href=https://onlinelibrary.wiley.com/doi/book/10.1002/9781118650158>Understanding Uncertainty</a>,
that changed my view on what is and how to handle uncertainty in a
coherent<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> way.
He is responsible for one of my favorites quotes:
&ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out&rdquo;;
and one of my favorite heuristics around prior probabilities:
<a href=https://en.wikipedia.org/wiki/Cromwell%27s_rule>Cromwell&rsquo;s Rule</a><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.
Lindley predicted in 1975 that &ldquo;Bayesian methods will indeed become pervasive,
enabled by the development of powerful computing facilities&rdquo; (Lindley, 1975).
You can find more about all of Lindley&rsquo;s achievements in his <a href=https://www.theguardian.com/science/2014/mar/16/dennis-lindley>obituary</a>.</p><h2 id=lindleys-paradox>Lindley&rsquo;s Paradox<a hidden class=anchor aria-hidden=true href=#lindleys-paradox>#</a></h2><p>Lindley&rsquo;s paradox<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> is a counterintuitive situation in statistics
in which the Bayesian and frequentist approaches to a hypothesis testing problem
give different results for certain choices of the prior distribution.</p><p>More formally, the paradox is as follows.
We have some parameter $\theta$ that we are interested in.
Then, we proceed with an experiment to test two competing hypotheses:</p><ol><li>$H_0$ (also known as <em>null hypothesis</em>):
there is no &ldquo;effect&rdquo;, or, more specifically,
$\theta = 0$.</li><li>$H_a$ (also known as <em>alternative hypothesis</em>):
there is an &ldquo;effect&rdquo;, or, more specifically,
$\theta \ne 0$.</li></ol><p>The paradox occurs when two conditions are met:</p><ol><li>The result of the experiment is <em>significant</em> by a frequentist test of $H_0$,
which indicates sufficient evidence to reject $H_0$, at a certain threshold of
probability<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</li><li>The posterior probability (Bayesian approach) of $H_0 \mid \theta$
(null hypothesis given $\theta$) is high,
which indicates strong evidence that $H_0$ should be favored over $H_a$,
that is, to <em>not</em> reject $H_0$.</li></ol><p>These results can occur at the same time when $H_0$ is very specific,
$H_a$ more diffuse,
and the prior distribution does not strongly favor one or the other.
These conditions are pervasive across science
and common in traditional null-hypothesis significance testing approaches.</p><p>This is a duel of frequentist versus Bayesian approaches,
and one of the many in which Bayesian emerges as the most coherent.
Let&rsquo;s give a example and go over the analytical result with a ton of math,
but also a computational result with <a href=https://julialang.org>Julia</a>.</p><h2 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h2><p>Here&rsquo;s the setup for the example.
In a certain city 49,581 boys and 48,870 girls have been
born over a certain time period.
The observed proportion of male births is thus
$\frac{49,581}{98,451} \approx 0.5036$.</p><p>We assume that the birth of a child is independent with a certain probability
$\theta$.
Since our data is a sequence of $n$ independent <a href=https://en.wikipedia.org/wiki/Bernoulli_trial>Bernoulli trials</a>,
i.e., $n$ independent random experiments with exactly two possible outcomes:
&ldquo;success&rdquo; and &ldquo;failure&rdquo;,
in which the probability of success is the same every time the
experiment is conducted.
We can safely assume that it follows a <a href=https://en.wikipedia.org/wiki/Binomial_distribution>binomial distribution</a>
with parameters:</p><ul><li>$n$: the number of &ldquo;trials&rdquo; (or the total number of births).</li><li>$\theta$: the probability of male births.</li></ul><p>We then set up our two competing hypotheses:</p><ol><li>$H_0$: $\theta = 0.5$.</li><li>$H_a$: $\theta \ne 0.5$.</li></ol><h3 id=analytical-solution>Analytical Solution<a hidden class=anchor aria-hidden=true href=#analytical-solution>#</a></h3><p>This is a toy-problem and, like most toy problems,
we can solve it analytically<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> for both the frequentist and the Bayesian approaches.</p><h4 id=analytical-solutions----frequentist-approach>Analytical Solutions &ndash; Frequentist Approach<a hidden class=anchor aria-hidden=true href=#analytical-solutions----frequentist-approach>#</a></h4><p>The frequentist approach to testing $H_0$ is to compute a $p$-value<sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>,
the probability of observing births of boys at least as large as 49,581
assuming $H_0$ is true.
Because the number of births is very large,
we can use a normal approximation<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> for the
binomial-distributed number of male births.
Let&rsquo;s define $X$ as the total number of male births,
then $X$ follows a normal distribution:</p><p>$$X \sim \text{Normal}(\mu, \sigma)$$</p><p>where $\mu$ is the mean parameter,
$n \theta$ in our case,
and $\sigma$ is the standard deviation parameter,
$\sqrt{n \theta (1 - \theta)}$.
We need to calculate the conditional probability of
$X \geq \frac{49,581}{98,451} \approx 0.5036$
given $\mu = n \theta = 98,451 \cdot \frac{1}{2} = 49,225.5$
and
$\sigma = \sqrt{n \theta (1 - \theta)} = \sqrt{98,451 \cdot \frac{1}{2} \cdot (1 - \frac{1}{2})}$:</p><p>$$P(X \ge 0.5036 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75})$$</p><p>This is basically a
<a href=https://en.wikipedia.org/wiki/Cumulative_distribution_function>cumulative distribution function (CDF)</a>
of $X$ on the interval $[49,225.5, 98,451]$:</p><p>$$\int_{49,225.5}^{98,451} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{\left( \frac{x - \mu}{\sigma} \right)^2}{2}} dx$$</p><p>After inserting the values and doing some arithmetic,
our answer is approximately $0.0117$.
Note that this is a one-sided test,
since it is symmetrical,
the two-sided test would be
$0.0117 \cdot 2 = 0.0235$.
Since we don&rsquo;t deviate from the Fisher&rsquo;s canon,
this is well below the 5% threshold.
Hooray! We rejected the null hypothesis!
Quick! Grab a frequentist celebratory cigar!
But, wait. Let&rsquo;s check the Bayesian approach.</p><h4 id=analytical-solutions----bayesian-approach>Analytical Solutions &ndash; Bayesian Approach<a hidden class=anchor aria-hidden=true href=#analytical-solutions----bayesian-approach>#</a></h4><p>For the Bayesian approach, we need to set prior probabilities on both hypotheses.
Since we do not favor one from another, let&rsquo;s set equal prior probabilities:</p><p>$$P(H_0) = P(H_a) = \frac{1}{2}$$</p><p>Additionally, all parameters of interest need a prior distribution.
So, let&rsquo;s put a prior distribution on $\theta$.
We could be fancy here, but let&rsquo;s not.
We&rsquo;ll use a uniform distribution on $[0, 1]$.</p><p>We have everything we need to compute the posterior probability of $H_0$ given
$\theta$.
For this, we&rsquo;ll use <a href=https://en.wikipedia.org/wiki/Bayes%27_theorem>Bayes theorem</a><sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>:</p><p>$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$$</p><p>Now again let&rsquo;s plug in all the values:</p><p>$$P(H_0 \mid \theta) = \frac{P(\theta \mid H_0) P(H_0)}{P(\theta)}$$</p><p>Note that by the <a href=https://en.wikipedia.org/wiki/Probability_axioms>axioms of probability</a>
and by the <a href=https://en.wikipedia.org/wiki/Chain_rule_(probability)>product rule of probability</a>
we can decompose $P(\theta)$ into:</p><p>$$P(\theta) = P(\theta \mid H_0) P(H_0) + P(\theta \mid H_a) P(H_a)$$</p><p>Again, we&rsquo;ll use the normal approximation:</p><p>$$
\begin{aligned}
&amp;P \left( \theta = 0.5 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75} \right) \\
&= \frac{
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \left( \frac{(\mu - \mu \cdot 0.5)}{2 \sigma} \right)^2} \cdot 0.5
}
{
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{ \left( -\frac{(\mu - \mu \cdot 0.5)}{2 \sigma} \right)^2} \cdot 0.5 +
\int_0^1 \frac {1}{\sqrt{2 \pi \sigma^2} } e^{- \left( \frac{\mu - \mu \cdot \theta)}{2 \sigma} \right)^2}d \theta \cdot 0.5
} \\
&= 0.9505
\end{aligned}
$$</p><p>The likelihood of the alternative hypothesis,
$P(\theta \mid H_a)$,
is just the CDF of all possible values of $\theta \ne 0.5$.</p><p>$$P(H_0 \mid \text{data}) = P \left( \theta = 0.5 \mid \mu = 49,225.5, \sigma = \sqrt{24.612.75} \right) > 0.95$$</p><p>And we fail to reject the null hypothesis, in frequentist terms.
However, we can also say in Bayesian terms, that we strongly favor $H_0$
over $H_a$.</p><p>Quick! Grab the Bayesian celebratory cigar!
The null is back on the game!</p><h3 id=computational-solutional>Computational Solutional<a hidden class=anchor aria-hidden=true href=#computational-solutional>#</a></h3><p>For the computational solution, we&rsquo;ll use <a href=https://julialang.org>Julia</a>
and the following packages:</p><ul><li><a href=https://github.com/JuliaStats/HypothesisTests.jl><code>HypothesisTest.jl</code></a></li><li><a href=https://turinglang.org/><code>Turing.jl</code></a></li></ul><h4 id=computational-solutions----frequentist-approach>Computational Solutions &ndash; Frequentist Approach<a hidden class=anchor aria-hidden=true href=#computational-solutions----frequentist-approach>#</a></h4><p>We can perform a <a href=https://juliastats.org/HypothesisTests.jl/stable/nonparametric/#Binomial-test><code>BinomialTest</code></a>
with <code>HypothesisTest.jl</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=k>using</span> <span class=n>HypothesisTests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>BinomialTest</span><span class=p>(</span><span class=mi>49_225</span><span class=p>,</span> <span class=mi>98_451</span><span class=p>,</span> <span class=mf>0.5036</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Binomial</span> <span class=n>test</span>
</span></span><span class=line><span class=cl><span class=o>-------------</span>
</span></span><span class=line><span class=cl><span class=n>Population</span> <span class=n>details</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=n>parameter</span> <span class=n>of</span> <span class=n>interest</span><span class=o>:</span>   <span class=n>Probability</span> <span class=n>of</span> <span class=n>success</span>
</span></span><span class=line><span class=cl>    <span class=n>value</span> <span class=n>under</span> <span class=n>h_0</span><span class=o>:</span>         <span class=mf>0.5036</span>
</span></span><span class=line><span class=cl>    <span class=n>point</span> <span class=n>estimate</span><span class=o>:</span>          <span class=mf>0.499995</span>
</span></span><span class=line><span class=cl>    <span class=mi>95</span><span class=o>%</span> <span class=n>confidence</span> <span class=n>interval</span><span class=o>:</span> <span class=p>(</span><span class=mf>0.4969</span><span class=p>,</span> <span class=mf>0.5031</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Test</span> <span class=n>summary</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=n>outcome</span> <span class=n>with</span> <span class=mi>95</span><span class=o>%</span> <span class=n>confidence</span><span class=o>:</span> <span class=n>reject</span> <span class=n>h_0</span>
</span></span><span class=line><span class=cl>    <span class=n>two</span><span class=o>-</span><span class=n>sided</span> <span class=n>p</span><span class=o>-</span><span class=n>value</span><span class=o>:</span>           <span class=mf>0.0239</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Details</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=n>number</span> <span class=n>of</span> <span class=n>observations</span><span class=o>:</span> <span class=mi>98451</span>
</span></span><span class=line><span class=cl>    <span class=n>number</span> <span class=n>of</span> <span class=n>successes</span><span class=o>:</span>    <span class=mi>49225</span>
</span></span></code></pre></div><p>This is the two-sided test,
and I had to round $49,225.5$ to $49,225$
since <code>BinomialTest</code> do not support real numbers.
But the results match with the analytical solution,
we still reject the null.</p><h4 id=computational-solutions----bayesian-approach>Computational Solutions &ndash; Bayesian Approach<a hidden class=anchor aria-hidden=true href=#computational-solutions----bayesian-approach>#</a></h4><p>Now, for the Bayesian computational approach,
I&rsquo;m going to use a generative modeling approach,
and one of my favorites probabilistic programming languages,
<code>Turing.jl</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=k>using</span> <span class=n>Turing</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=nd>@model</span> <span class=k>function</span> <span class=n>birth_rate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>           <span class=n>θ</span> <span class=o>~</span> <span class=n>Uniform</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>           <span class=n>total_births</span> <span class=o>=</span> <span class=mi>98_451</span>
</span></span><span class=line><span class=cl>           <span class=n>male_births</span> <span class=o>~</span> <span class=n>Binomial</span><span class=p>(</span><span class=n>total_births</span><span class=p>,</span> <span class=n>θ</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=k>end</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>model</span> <span class=o>=</span> <span class=n>birth_rate</span><span class=p>()</span> <span class=o>|</span> <span class=p>(;</span> <span class=n>male_births</span> <span class=o>=</span> <span class=mi>49_225</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>chain</span> <span class=o>=</span> <span class=n>sample</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>NUTS</span><span class=p>(</span><span class=mi>1_000</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>),</span> <span class=n>MCMCThreads</span><span class=p>(),</span> <span class=mi>1_000</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Chains</span> <span class=n>MCMC</span> <span class=n>chain</span> <span class=p>(</span><span class=mi>1000</span><span class=o>×</span><span class=mi>13</span><span class=o>×</span><span class=mi>4</span> <span class=kt>Array</span><span class=p>{</span><span class=kt>Float64</span><span class=p>,</span> <span class=mi>3</span><span class=p>})</span><span class=o>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Iterations</span>        <span class=o>=</span> <span class=mi>1001</span><span class=o>:</span><span class=mi>1</span><span class=o>:</span><span class=mi>2000</span>
</span></span><span class=line><span class=cl><span class=kt>Number</span> <span class=n>of</span> <span class=n>chains</span>  <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>Samples</span> <span class=n>per</span> <span class=n>chain</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=n>Wall</span> <span class=n>duration</span>     <span class=o>=</span> <span class=mf>0.2</span> <span class=n>seconds</span>
</span></span><span class=line><span class=cl><span class=n>Compute</span> <span class=n>duration</span>  <span class=o>=</span> <span class=mf>0.19</span> <span class=n>seconds</span>
</span></span><span class=line><span class=cl><span class=n>parameters</span>        <span class=o>=</span> <span class=n>θ</span>
</span></span><span class=line><span class=cl><span class=n>internals</span>         <span class=o>=</span> <span class=n>lp</span><span class=p>,</span> <span class=n>n_steps</span><span class=p>,</span> <span class=n>is_accept</span><span class=p>,</span> <span class=n>acceptance_rate</span><span class=p>,</span> <span class=n>log_density</span><span class=p>,</span> <span class=n>hamiltonian_energy</span><span class=p>,</span> <span class=n>hamiltonian_energy_error</span><span class=p>,</span> <span class=n>max_hamiltonian_energy_error</span><span class=p>,</span> <span class=n>tree_depth</span><span class=p>,</span> <span class=n>numerical_error</span><span class=p>,</span> <span class=n>step_size</span><span class=p>,</span> <span class=n>nom_step_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Summary</span> <span class=n>Statistics</span>
</span></span><span class=line><span class=cl>  <span class=n>parameters</span>      <span class=n>mean</span>       <span class=n>std</span>      <span class=n>mcse</span>    <span class=n>ess_bulk</span>    <span class=n>ess_tail</span>      <span class=n>rhat</span>   <span class=n>ess_per_sec</span>
</span></span><span class=line><span class=cl>      <span class=kt>Symbol</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>     <span class=kt>Float64</span>     <span class=kt>Float64</span>   <span class=kt>Float64</span>       <span class=kt>Float64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>θ</span>    <span class=mf>0.4999</span>    <span class=mf>0.0016</span>    <span class=mf>0.0000</span>   <span class=mf>1422.2028</span>   <span class=mf>2198.1987</span>    <span class=mf>1.0057</span>     <span class=mf>7368.9267</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Quantiles</span>
</span></span><span class=line><span class=cl>  <span class=n>parameters</span>      <span class=mf>2.5</span><span class=o>%</span>     <span class=mf>25.0</span><span class=o>%</span>     <span class=mf>50.0</span><span class=o>%</span>     <span class=mf>75.0</span><span class=o>%</span>     <span class=mf>97.5</span><span class=o>%</span>
</span></span><span class=line><span class=cl>      <span class=kt>Symbol</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>θ</span>    <span class=mf>0.4969</span>    <span class=mf>0.4988</span>    <span class=mf>0.4999</span>    <span class=mf>0.5011</span>    <span class=mf>0.5031</span>
</span></span></code></pre></div><p>We can see from the output of the quantiles that the 95% quantile for $\theta$ is
the interval $(0.4969, 0.5031)$.
Although it overlaps zero, that is not the equivalent of a hypothesis test.
For that, we&rsquo;ll use the
<a href=https://en.wikipedia.org/wiki/highest_posterior_density_interval>highest posterior density interval (HPDI)</a>,
which is defined as &ldquo;choosing the narrowest interval&rdquo; that
captures a certain posterior density threshold value.
In this case, we&rsquo;ll use a threshold interval of 95%,
i.e. an $\alpha = 0.05$:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>julia</span><span class=o>&gt;</span> <span class=n>hpd</span><span class=p>(</span><span class=n>chain</span><span class=p>;</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.05</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>HPD</span>
</span></span><span class=line><span class=cl>  <span class=n>parameters</span>     <span class=n>lower</span>     <span class=n>upper</span>
</span></span><span class=line><span class=cl>      <span class=kt>Symbol</span>   <span class=kt>Float64</span>   <span class=kt>Float64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>θ</span>    <span class=mf>0.4970</span>    <span class=mf>0.5031</span>
</span></span></code></pre></div><p>We see that we fail to reject the null,
$\theta = 0.5$ at $\alpha = 0.05$ which is in accordance with the analytical
solution.</p><h2 id=why-the-frequentist-and-bayesian-approaches-disagree>Why the Frequentist and Bayesian Approaches Disagree<a hidden class=anchor aria-hidden=true href=#why-the-frequentist-and-bayesian-approaches-disagree>#</a></h2><p>Why do the approaches disagree?
What is going on under the hood?</p><p>The answer is disappointing<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.
The main problem is that the frequentist approach only allows fixed significance
levels with respect to sample size.
Whereas the Bayesian approach is consistent and robust to sample size variations.</p><p>Taken to extreme, in some cases, due to huge sample sizes,
the $p$-value is pretty much a <em>proxy</em> for sample size
and have little to no utility on hypothesis testing.
This is known as $p$-hacking<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>.</p><h2 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h2><p>This post is licensed under <a href=http://creativecommons.org/licenses/by-nc-sa/4.0/>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a>.</p><p><a href=http://creativecommons.org/licenses/by-nc-sa/4.0/><img loading=lazy src=https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png alt="CC BY-NC-SA 4.0"></a></p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>Lindley, Dennis V. &ldquo;The future of statistics: A Bayesian 21st century&rdquo;.
<em>Advances in Applied Probability</em> 7 (1975): 106-115.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>as far as I know there&rsquo;s only one coherent approach to uncertainty,
and it is the Bayesian approach.
Otherwise, as de Finetti and Ramsey proposed,
you are susceptible to a <a href=https://en.wikipedia.org/wiki/Dutch_book>Dutch book</a>.
This is a topic for another blog post&mldr;&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Cromwell&rsquo;s rule states that the use of prior probabilities of 1
(&ldquo;the event will definitely occur&rdquo;) or 0 (&ldquo;the event will definitely not occur&rdquo;)
should be avoided, except when applied to statements that are logically true or false.
Hence, anything that is not a math theorem should have priors in $(0,1)$.
The reference comes from <a href=https://en.wikipedia.org/wiki/Oliver_Cromwell>Oliver Cromwell</a>,
asking, very politely, for the Church of Scotland to consider that their prior probability
might be wrong.
This footnote also deserves a whole blog post&mldr;&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy>Stigler&rsquo;s law of eponymy</a>
states that no scientific discovery is named after its original discoverer.
The paradox was already was discussed in <a href=https://en.wikipedia.org/wiki/Harold_Jeffreys>Harold Jeffreys</a>'
1939 textbook.
Also, fun fact, Stigler&rsquo;s is not the original creator of such law&mldr;
Now that&rsquo;s a self-referential paradox, and a broad version of the <a href=https://en.wikipedia.org/wiki/Halting_problem>Halting problem</a>,
which should earn its own footnote.
Nevertheless, we are getting into self-referential danger zone here with
footnotes&rsquo; of footnotes&rsquo; of footnotes&rsquo;&mldr;&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>this is called $p$-value and can be easily defined as
&ldquo;the probability of sampling data from a target population given that $H_0$
is true as the number of sampling procedures $\to \infty$&rdquo;.
Yes, it is not that intuitive, and it deserves not a blog post,
but a full curriculum to hammer it home.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>that is not true for most of the real-world problems.
For Bayesian approaches,
we need to run computational asymptotic exact approximations using a class
of methods called <a href=https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo>Markov chain Monte Carlo (MCMC)</a>.
Furthermore, for some nasty problems we need to use different set of methods
called <a href=https://en.wikipedia.org/wiki/Variational_Inference>variational inference (VI)</a>
or <a href=https://en.wikipedia.org/wiki/Approximate_Bayesian_computation>approximate Bayesian computation (ABC)</a>.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>if you are curious about how this approximation works,
check the backup slides of my
<a href=https://github.com/storopoli/Bayesian-Statistics>open access and open source graduate course on Bayesian statistics</a>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Bayes&rsquo; theorem is officially called Bayes-Price-Laplace theorem.
Bayes was trying to disprove David Hume&rsquo;s argument that miracles did not exist
(How dare he?).
He used the probabilistic approach of trying to quantify the probability of a parameter
(god exists) given data (miracles happened).
He died without publishing any of his ideas.
His wife probably freaked out when she saw the huge pile of notes that he had
and called his buddy Richard Price to figure out what to do with it.
Price struck gold and immediately noticed the relevance of Bayes&rsquo; findings.
He read it aloud at the Royal Society.
Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes,
used the same probabilistic approach to perform statistical inference using France&rsquo;s
first census data in the early-Napoleonic era.
Somehow we had the answer to statistical inference back then,
and we had to rediscover everything again in the late-20th century&mldr;&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>disappointing because most of
published scientific studies suffer from this flaw.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>and, like all footnotes here, it deserves its own blog post&mldr;&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://storopoli.io/tags/bayesian/>bayesian</a></li><li><a href=https://storopoli.io/tags/probability/>probability</a></li><li><a href=https://storopoli.io/tags/julia/>julia</a></li></ul><nav class=paginav><a class=prev href=https://storopoli.io/2024-01-14-htmx/><span class=title>« Prev</span><br><span>htmx: an Oasis in a Desert of Soy</span>
</a><a class=next href=https://storopoli.io/2024-02-11-mnemonic/><span class=title>Next »</span><br><span>Seed Phrases and Entropy</span></a></nav></footer></article></main><footer class=footer><span><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a></span>
<span>- Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer">Hugo</a> &
        <a href=https://github.com/Wonderfall/hugo-WonderMod/ rel=noopener>WonderMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script defer crossorigin=anonymous src=/assets/js/papermod.7ea300eda6d3653624a576fbc095ccd8a0c2977756acbe5de4114132a72cc7fa.js integrity="sha256-fqMA7abTZTYkpXb7wJXM2KDCl3dWrL5d5BFBMqcsx/o="></script></body></html>