[{"content":" Euclid\u0026rsquo;s one-way function\nWarning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nThis is the companion post to the cryptography workshop that I gave at a local BitDevs. Let\u0026rsquo;s explore the basics of cryptography. We\u0026rsquo;ll go through the following topics:\nOne-way functions Hash functions Public-key cryptography DSA Schnorr Why we don\u0026rsquo;t reuse nonces? Why we can combine Schnorr Signatures and not DSA? One-way functions A one-way function is a function that is easy to compute on every input, but hard to invert given the image1 of a random input. For example, imagine an omelet. It\u0026rsquo;s easy to make an omelet from eggs, but it\u0026rsquo;s hard to make eggs from an omelet. In a sense we can say that the function $\\text{omelet}$ is a one-way function\n$$\\text{omelet}^{-1}(x) = \\ldots$$\nThat is, we don\u0026rsquo;t know how to invert the function $\\text{omelet}$ to get the original eggs back. Or, even better, the benefit we get from reverting the omelet to eggs is not worth the effort, either in time or money.\nNot all functions are one-way functions. The exponential function, $f(x) = e^x$, is not a one-way function. It is easy to undo the exponential function by taking the natural logarithm,\n$$f^{-1}(x) = \\ln(x)$$\nTo showcase one-way functions, let\u0026rsquo;s take a look at the following example. Let\u0026rsquo;s play around with some numbers. Not any kind of numbers, but very special numbers called primes. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\nIf I give you a big number $n$ and ask you to find its prime factors, and point a gun at your head, you\u0026rsquo;ll pretty much screwed. There\u0026rsquo;s no known efficient algorithm2 to factorize a big number into its prime factors. You\u0026rsquo;ll be forced to test all numbers from 2 to $\\sqrt{n}$ to see if they divide $n$.\nHere\u0026rsquo;s a number:\n$$90809$$\nWhat are its prime factors? It\u0026rsquo;s $1279 \\cdot 71$. Easy to check, right? Hard to find. That\u0026rsquo;s because prime factorization, if you choose a fucking big number, is a one-way function.\nHash Functions Let\u0026rsquo;s spice things up. There is a special class of one-way functions called hash functions.\nA hash function is any function that can be used to map data of arbitrary size to fixed-size values.\nBut we are most interested in cryptographic hash functions, which are hash functions that have statistical properties desirable for cryptographic application:\nOne-way function: easy to compute $y = f(x)$, hard as fuck to do the opposite, $x = f^{-1}(y)$. Deterministic: given a function that maps elements from set $X$ to set $Y$, $f: X \\to Y$, for every $x \\in X$ there\u0026rsquo;s at least one $y \\in Y$3. This means that if I give you a certain input, it will always map to the same output. It is deterministic. Collision resistance: the possible values of $f: X \\to Y$ follows a uniform distribution, that is, given the size of the set $Y$, it is hard to find two $x_1, x_2 \\in X$ that have the same $y \\in Y$ value4. This property is really important because if an attacker wants to brute-force the hash function, there\u0026rsquo;s no option than searching uniformly across the whole possible space of possible values that the hash function outputs. These properties make enable cryptographic hash functions to be used in a wide range of applications, including but not limited to:\nDigital signatures: Hash functions are used to create a digest of the message to be signed. The digital signature is then generated using the hash, rather than the message itself, to ensure integrity and non-repudiation.\nPassword hashing: Storing passwords as hash values instead of plain text. Even if the hash values are exposed, the original passwords remain secure due to the pre-image resistance property.\nBlockchain and cryptocurrency: Hash functions are used to maintain the integrity of the blockchain. Each block contains the hash of the previous block, creating a secure link. Cryptographic hashes also underpin various aspects of cryptocurrency transactions.\nData integrity verification: Hash functions are used to ensure that files, messages, or data blocks have not been altered. By comparing hash values computed before and after transmission or storage, any changes in the data can be detected.\nWe\u0026rsquo;ll cover just the digital signatures part in this post.\nSHA-2 and its variants The Secure Hash Algorithm 2 (SHA-2) is a set of cryptographic hash functions designed by the National Security Agency (NSA). It was first published in 2001.\nIt is composed of six hash functions with digests that are 224, 256, 384, 512, 512/224, and 512/256 bits long:\nSHA-224 SHA-256 SHA-384 SHA-512 SHA-512/224 SHA-512/256 Amongst these, let\u0026rsquo;s focus on SHA-256, which is the most widely used while also being notoriously adopted by bitcoin.\nSHA-256 does not have any known vulnerabilities and is considered secure. It comprises of 32-bit words and operates on 64-byte blocks. The algorithm does 64 rounds of the following operations:\nAND: bitwise boolean AND XOR: bitwise boolean XOR OR: bitwise boolean OR ROT: right rotation bit shift ADD: addition modulo $2^{32}$ You can check SHA-256 Pseudocode on Wikipedia. It really scrambles the input message in a way that is very hard to reverse.\nThese operations are non-linear and very difficult to keep track of. In other words, you can\u0026rsquo;t reverse-engineer the hash to find the original message. There\u0026rsquo;s no \u0026ldquo;autodiff\u0026rdquo; for hash functions.\nSince it is a cryptographic hash function, if we change just one bit of the input, the output will be completely different. Check this example:\n$ echo \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34; | shasum -a 256 c03905fcdab297513a620ec81ed46ca44ddb62d41cbbd83eb4a5a3592be26a69 - $ echo \u0026#34;The quick brown fox jumps over the lazy dog.\u0026#34; | shasum -a 256 b47cc0f104b62d4c7c30bcd68fd8e67613e287dc4ad8c310ef10cbadea9c4380 - Here we are only adding a period at the end of the sentence, and the hash is completely different. This is due to the property of collision resistance that we mentioned earlier.\nFields Before we dive into public-key cryptography, we need a brief interlude on fields.\nFields are sets with two binary operations, called addition $+$ and multiplication $\\times$. We write\n$$F = (F, +, \\times)$$\nto denote a field, where $F$ is the set, $+$ is the addition operation, and $\\times$ is the multiplication operation.\nAddition and multiplication behave similar to the addition and multiplication of real numbers. For example, addition is commutative and associative\n$$a + b = b + a,$$\nand multiplication is distributive\n$$a \\times (b + c) = a \\times b + a \\times c.$$\nAlso, there are two special elements in the field, called the additive identity $-a$ and the multiplicative identity $a^{-1}$, such that\n$$a + (-a) = I,$$\nand\n$$a \\times a^{-1} = I,$$\nwhere $I$ is the identity element.\nNote that this allows us to define subtraction\n$$a - b = a + (-b),$$\nand division\n$$a \\div b = a \\times b^{-1}.$$\nFinite Fields Now we are ready for finite fields. A finite field, also called a Galois field (in honor of Ã‰variste Galois), is a field with a finite number of elements. As with any field, a finite field is a set on which the operations of multiplication, addition, subtraction and division are defined and satisfy the rules above for fields.\nFinite fields is a very rich topic in mathematics, and there are many ways to construct them. The easiest way to construct a finite field is to take the integers modulo a prime number $p$. For example $\\mathbb{Z}_5$ is a finite field with 5 elements:\n$$\\mathbb{Z}_5 = \\lbrace 0, 1, 2, 3, 4 \\rbrace.$$\nIn general, $\\mathbb{Z}_n$ is a finite field with $n$ elements:\n$$\\mathbb{Z}_n = \\lbrace 0, 1, 2, \\ldots, n - 1 \\rbrace.$$\nThe number of elements in a finite field is called the order of the field. The order of a finite field is always a prime number $p$. The $\\mathbb{Z}_5$ example above is a finite field of order 5. However, $\\mathbb{Z}_4$ is not a finite field, because 4 is not a prime number, but rather a composite number.\n$$4 = 2 \\times 2.$$\nAnd we can write $\\mathbb{Z}_4$ as\n$$\\mathbb{Z}_4 = 2 \\times \\mathbb{Z}_2.$$\nThis means that every element in $a \\in \\mathbb{Z}_4$ can be written as\n$$a = 2 \\times b,$$\nwhere $b$ is an element in $\\mathbb{Z}_2$.\nHence, not every element of $\\mathbb{Z}_4$ is unique, and they are equivalent to the elements in $\\mathbb{Z}_2$.\nIn general if $n$ is a composite number, then $\\mathbb{Z}_n$ is not a finite field. However, if $n = r \\times s$ where $r$ and $s$ are prime numbers, and $r \u0026lt; s$, then $\\mathbb{Z}_n$ is a finite field of order $r$.\nOperations in Finite Fields Addition in finite fields is defined as the remainder of the sum of two elements modulo the order of the field.\nFor example, in $\\mathbb{Z}_3$,\n$$1 + 2 = 3 \\mod 3 = 0.$$\nWe can also define subtraction in finite fields as the remainder of the difference of two elements modulo the order of the field.\nFor example, in $\\mathbb{Z}_3$,\n$$1 - 2 = -1 \\mod 3 = 2.$$\nMultiplication in finite fields can be written as multiple additions. For example, in $\\mathbb{Z}_3$,\n$$2 \\times 2 = 2 + 2 = 4 \\mod 3 = 1.$$\nExponentiation in finite fields can be written as multiple multiplications. For example, in $\\mathbb{Z}_3$,\n$$2^2 = 2 \\times 2 = 4 \\mod 3 = 1.$$\nAs you can see addition, subtraction, and multiplication becomes linear operations. This is very trivial for any finite field.\nHowever, for division we are pretty much screwed. It is really hard to find the multiplicative inverse of an element in a finite field. For example, suppose that we have numbers $a,b$ in a very large finite field $\\mathbb{Z}_p$, such that\n$$c = a \\times b \\mod p.$$\nThen we can write division as\n$$a = c \\div b = c \\times b^{-1} \\mod p.$$\nNow we need to find $b^{-1}$, which is the multiplicative inverse of $b$. This is called the discrete logarithm problem. Because we need to find $b^{-1}$ such that\n$$b^{-1} = \\log_b c \\mod p.$$\nSince this number is a discrete number and not a real number, that\u0026rsquo;s why it\u0026rsquo;s called the discrete logarithm problem.\nGood luck my friend, no efficient method is known for computing them in general. You can try brute force, but that\u0026rsquo;s not efficient.\nWhy the Discrete Logarithm Problem is Hard as Fuck To get a feeling why the discrete logarithm problem is difficult, let\u0026rsquo;s add one more concept to our bag of knowledge. Every finite field has generators, also known as primitive roots, which is also a member of the group, such that applying multiplication to this one single element makes possible to generate the whole finite field.\nLet\u0026rsquo;s illustrate this with an example. Below we have a table of all the results of the following operation\n$$b^x \\mod 7$$\nfor every possible value of $x$. As you\u0026rsquo;ve guessed right this is the $\\mathbb{Z}_7$ finite field.\n$b$ $b^1 \\mod 7$ $b^2 \\mod 7$ $b^3 \\mod 7$ $b^4 \\mod 7$ $b^5 \\mod 7$ $b^6 \\mod 7$ $1$ $1$ $1$ $1$ $1$ $1$ $1$ $2$ $2$ $4$ $1$ $2$ $4$ $1$ $3$ $3$ $2$ $6$ $4$ $5$ $1$ $4$ $4$ $2$ $1$ $4$ $2$ $1$ $5$ $5$ $4$ $6$ $2$ $3$ $1$ $6$ $6$ $1$ $6$ $1$ $1$ $1$ You see that something interesting is happening here. For specific values of $b$, such as $b = 3$, and $b = 5$, we are able to generate the whole finite field. Hence, say that $3$ and $5$ are generators or primitive roots of $\\mathbb{Z}_7$.\nNow suppose I ask you to find $x$ in the following equation\n$$3^x \\mod p = 11$$\nwhere $p$ is a very large prime number. Then you don\u0026rsquo;t have any other option than brute forcing it. You\u0026rsquo;ll need to try each exponent $x \\in \\mathbb{Z}_p$ until you find the one that satisfies the equation.\nNotice that this operation is very asymmetric. It is very easy to compute $3^x \\mod p$ for any $x$, but it is very hard to find $x$ given $3^x \\mod p$.\nNow we are ready to dive into public-key cryptography.\nNumerical Example of the Discrete Logarithm Problem Let\u0026rsquo;s illustrate the discrete logarithm problem with a numerical example.\nChoose a prime number $p$. Let\u0026rsquo;s pick $p = 17$. Choose a generator $g$ of the group. For $p = 17$, we can choose $g = 3$ because $3$ is a primitive root of $\\mathbb{Z}_{17}$. Choose an element $x$. Let\u0026rsquo;s pick $x = 15$. The discrete logarithm problem is to find $x$ given $g^x \\mod p$. So let\u0026rsquo;s plug in the numbers; find $x$ in\n$$3^x = 15 \\mod 17 $$\nTry to find it. Good luck5.\nPublic-key cryptography Public-key cryptography, or asymmetric cryptography, is a cryptographic system that uses pairs of keys: private and public. The public key you can share with anyone, but the private key you must keep secret. The keys are related mathematically, but it is computationally infeasible to derive the private key from the public key. In other words, the public key is a one-way function of the private key.\nBefore we dive into the details of the public-key cryptography, and signing and verifying messages, let me introduce some notation:\n$p$: big fucking huge prime number (4096 bits or more) $\\mathbb{Z}_p$: the finite field of order $p$ $g$: a generator of $\\mathbb{Z}_p$ $S_k$: secret key, a random integer in the finite field $\\mathbb{Z}_p$ $P_k$: public key derived by $P_k = g^{S_k}$ If you know $S_k$ and $g$ (which is almost always part of the spec), then it\u0026rsquo;s easy to derive the $P_k$. However, if you only know $g$ and $P_k$, good luck finding $S_k$. It\u0026rsquo;s the discrete log problem again. And as long $p$ is HUGE you are pretty confident that no one will find your secret key from your public key.\nNow what we can do with these keys and big prime numbers? We\u0026rsquo;ll we can sign a message with our secret key and everyone can verify the authenticity of the message using our public key. The message in our case it is commonly a hash function of the \u0026ldquo;original message\u0026rdquo;. Due to the collision resistance property, we can definitely assert that:\nthe message has not been altered the message was signed by the owner of the private key Fun fact, I once gave a recommendation letter to a very bright student, that was only a plain text file signed with my private key. I could rest assured that the letter was not altered, and the student and other people could verify that I was the author of the letter.\nNext, we\u0026rsquo;ll dive into the details of the Digital Signature Algorithm (DSA) and the Schnorr signature algorithm.\nDSA DSA stands for Digital Signature Algorithm. It was first proposed by the National Institute of Standards and Technology (NIST) in 1991. Note that OpenSSH announced that DSA is scheduled for removal in 2025.\nHere\u0026rsquo;s how you can sign a message using DSA:\nChoose two prime numbers $p, q$ such that $p - 1 \\mod q = 0$ (e.g., 1279 and 71). Choose your private key $S_k$ as a random integer $\\in [1, q-1]$. Choose a generator $g$. Compute your public key $P_k$: $g^{S_k} \\mod p$. Choose your nonce $k$: as a random integer $\\in [1, q-1]$. Compute your \u0026ldquo;public nonce\u0026rdquo; $K$: $(g^k \\mod p) \\mod q$ (also known as $r$). Get your message ($m$) through a cryptographic hash function $H$: $H(m)$. Compute your signature $s$: $(k^{-1} (H(m) + S_k K)) \\mod q$. Send to your buddy $(p, q, g)$, $P_k$, and $(K, s)$. And here\u0026rsquo;s how you can verify the signature:\nCompute $w = s^{-1} \\mod q$. Compute $u_1 = H{m} \\cdot w \\mod q$. Compute $u_2 = K \\cdot w \\mod q$. Compute $K^* = {g^{u_1} P^{u_2}_k \\mod p} \\mod q$. Assert $K = K^*$. How this works? Let\u0026rsquo;s go through a proof of correctness. I added some comments to every operation in parentheses to make it easier to follow.\n$s = k^{-1} \\cdot {H + S_k K} \\mod q$ ($\\mod p$ and $H(m)$ implicit). $k = s^{-1} \\cdot {H + S_k K} \\mod q$ (move $s$ to $k$). $k = H \\cdot s^{-1} + S_k K \\cdot s^{-1} \\mod q$ (distribute $s^{-1}$). $k = H \\cdot w + S_k K \\cdot w \\mod q$ ($w = s^{-1}$). $g^k = g^{H \\cdot w + S_k K \\cdot w \\mod q}$ (put $g$ in both sides). $g^k = g^{H \\cdot w \\mod q} \\cdot g^{S_k K \\cdot w \\mod q}$ (product of the exponents). $g^k = g^{H \\cdot w \\mod q} \\cdot P^{K \\cdot w \\mod q}_k$ ($P_k = g^{S_k}$). $g^k = g^{u_1} \\cdot P^{u_2}_k$ (replace $u_1$ and $u_2$). $K = K^*$ (replace $K$ and $K^*$). There you go. This attest that the signature is correct and the message was signed by the owner of the private key.\nSchnorr Schnorr signature algorithm is a very similar algorithm to DSA. It was proposed by Claus-Peter Schnorr in 1989. It is considered to be more secure than DSA and is also more efficient. The patent for Schnorr signatures expired in 2008, just in time for Satoshi to include it in Bitcoin. However, it was probably not included due to the fact that there wasn\u0026rsquo;t good battle-tested software implementations of it at the time. However, it was added to Bitcoin in the Taproot upgrade6.\nSchnorr is a marvelous algorithm. It is so much simpler than DSA. Here\u0026rsquo;s how you sign a message using Schnorr:\nChoose a prime number $p$. Choose your private key $S_k$ as a random integer $\\in [1, p-1]$. Choose a generator $g$. Compute your public key $P_k$: $g^{S_k}$. Choose your nonce $k$: as a random integer $\\in [1, p-1]$. Compute your \u0026ldquo;public nonce\u0026rdquo; $K$: $g^k \\mod p$ (also known as $r$). Get your message ($m$) through a cryptographic hash function $H$ concatenating with $K$: $e = H(K || m)$. Compute your signature $s$: $k - S_k e$. Send to your buddy $(p, g)$, $P_k$, and $(K, s)$. And here\u0026rsquo;s how you can verify the signature:\nCompute $e = H(K || m)$. Compute $K^* = g^s P_k^e$. Compute $e^* = H(K^* || m)$. Assert $e = e^*$. How this works? Let\u0026rsquo;s go through a proof of correctness. As before, I added some comments to every operation in parentheses to make it easier to follow.\n$K^* = g^s P_k^e$ ($\\mod p$ implicit). $K^* = g^{k - S_k e} g^{S_k e}$ ($s = k - S_k e$ and $P_k = g^{S_k}$). $K^* = g^k$ (cancel $S_k e$ in the exponent of $g$). $K^* = K$ ($K = g^k$). Hence $H(K^* || m) = H(K || m)$. There you go. This attest that the signature is correct and the message was signed by the owner of the private key.\nWhy we don\u0026rsquo;t reuse nonces? Never, ever, reuse a nonce. Why? First, because nonce is short for \u0026ldquo;number used once\u0026rdquo;. It is supposed to be used only once. Because if you reuse a nonce, then you are pretty much screwed. An attacker can derive your private key from two signatures with the same nonce. This is called the \u0026ldquo;nonce reuse attack\u0026rdquo;.\nFun fact: this is what happened to the PlayStation 3.\nLet\u0026rsquo;s see how we can derive the private key from two signatures with the same nonce. Here we are in a context that we have two signatures $s$ and $s^\\prime$, both using the same nonce $k = k^\\prime$.\nFirst, let\u0026rsquo;s do the ugly DSA math:\n$$\\begin{aligned} s^\\prime - s \u0026amp;= (k^{\\prime {-1}} (H(m_1) + S_k K\u0026rsquo;)) - (k^{-1} (H(m_2) + S_k K)) \\\\ s^\\prime - s \u0026amp;= k^{-1} (H(m_1) - H(m_2)) \\\\ k \u0026amp;= (H(m_1) - H(m_2)) (s^\\prime - s)^{-1} \\end{aligned}$$\nNow remember you know $s$, $s^\\prime$, $H(m_1)$, $H(m_2)$ $K$, and $K^\\prime$. Let\u0026rsquo;s do the final step and solve for $S_k$:\n$$S_k = K^{-1} (k s - H(m_1))$$\nNow let\u0026rsquo;s do the Schnorr math. But in Schnorr, everything is simpler. Even nonce reuse attacks.\n$$s^\\prime - s = (k^\\prime - k) - S_k (e^\\prime - e)$$\nIf $k^\\prime = k$ (nonce reuse) then you can easily isolate $S_k$ with simple algebra.\nRemember: you know $s^\\prime, s, e, e^\\prime$ and $k^\\prime - k = 0$.\nWhy we can combine Schnorr Signatures and not DSA? In Bitcoin, we can combine Schnorr signatures and not DSA. Why? Because Schnorr signatures are linear. This means that you can add two Schnorr signatures and get a valid signature for the sum of the messages. This is not possible with DSA. This is called the \u0026ldquo;linearity property\u0026rdquo; of Schnorr signatures.\nRemember that in $Z_p$ addition, multiplication, and exponentiation, i.e anything with $+, \\cdot, -$, are linear operations However, division (modular inverse), .i.e anything that is $^{-1}$, is not linear. That is:\n$$x^{-1} + y^{-1} != (x + y)^{-1}.$$\nHere\u0026rsquo;s a trivial python code that shows that modular inverse is not linear:\n\u0026gt;\u0026gt;\u0026gt; p = 71; x = 13; y = 17; \u0026gt;\u0026gt;\u0026gt; pow(x, -1, p) + pow(y, -1, p) == pow(x + y, -1, p) False Let\u0026rsquo;s revisit the signature step of DSA and Schnorr:\nDSA: $s = k^{-1} (H(m) + S_k K)$ Schnorr: $s = k - S_k H(K || m)$ So if you have two Schnorr signatures $s_1$ and $s_2$ for two messages $m_1$ and $m_2$, then you can easily compute a valid signature for the sum of the messages $m_1 + m_2$:\n$$s = s_1 + s_2$$\nAlso note that we can combine Schnorr public keys:\n$$P^\\prime_k + P_k = g^{S^\\prime_k} + g^{S_k} = g^{S_k^\\prime + S_k}$$\nAnd the signature $s$ for the sum of the messages $m_1 + m_2$ can be verified with the public key $P^\\prime_k + P_k$.\nThis is not possible with DSA.\nBecause the signature step in DSA is not linear, it has a $k^{-1}$ in it.\nTechnical Interlude: Elliptic Curves Technically speaking, Bitcoin uses the Elliptic Curve Digital Signature Algorithm (ECDSA), and the Schnorr signature algorithm is based on the same elliptic curve (EC) as ECDSA.\nAnd trivially speaking EC public-key cryptography in the end is just a finite field on $\\mathbb{Z}_p$. It has everything that we\u0026rsquo;ve seen so far:\nAddition Subtraction Multiplication Division Exponentiation Generators Discrete Logarithm Problem Conclusion I hope you enjoyed this companion post to the cryptography workshop. Remember don\u0026rsquo;t reuse nonces.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nthe image of a function $f$ is the set of all values that $f$ may produce.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthe problem of factoring a number into its prime factors is not known to be in the class of problems that can be solved in polynomial time, P. It is not known to be NP-complete, NP, either. Actually to find it P is NP or not is the hardest way to earn a million dollars, the P vs NP problem.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthis is called surjection.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nat least $\\frac{1}{N}$ where $N$ is the size of $Y$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe answer is $x = 6$. This means that $3^6 = 15 \\mod 17$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTaproot is a proposed Bitcoin protocol upgrade that was deployed as a forward-compatible soft fork. The validation of Taproot is based on Schnorr signatures. You can find more in BIPS 340, 341, and 342.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2024-02-05-crypto-basics/","summary":"Euclid\u0026rsquo;s one-way function\nWarning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nThis is the companion post to the cryptography workshop that I gave at a local BitDevs. Let\u0026rsquo;s explore the basics of cryptography. We\u0026rsquo;ll go through the following topics:\nOne-way functions Hash functions Public-key cryptography DSA Schnorr Why we don\u0026rsquo;t reuse nonces? Why we can combine Schnorr Signatures and not DSA?","title":"Cryptography Basics"},{"content":" It all started when I had to accompany my mom to the hospital. It was just a routine checkup, but I had to wait for a few hours. I brought my laptop with me, since they have good WiFi and I could work on my projects. Then I realized that my mom was playing a Sudoku1 game on her phone. I couln\u0026rsquo;t help but notice that the game was full of ads and it was asking for a lot of permissions, like location and sensor data. So I decided to make a Sudoku game for her, without ads or using any permission. It wouldn\u0026rsquo;t even need to ask for the blessing of Google or Tim Apple since it was a Progressive Web App (PWA) and it would work offline.\nYou can play the game at storopoli.io/sudoku or check the source code at storopoli/sudoku.\nHere\u0026rsquo;s a screenshot of the game:\nTools of Choice So what would I use to build this game? Only one thing: Dioxus. Dioxus is a fullstack framework for Rust, that allows you to build web applications with Rust. You can benefit from the safety and performance of Rust, powerful type system and borrow checker, along with the low memory footprint.\nThat\u0026rsquo;s it. Just Rust and HTML with some raw CSS. No \u0026ldquo;YavaScript\u0026rdquo;. No Node.js. No npm. No webpack. No Tailwind CSS. Just cargo run --release and you\u0026rsquo;re done.\nPackage Management Using Rust for fullstack development is an amazing thing. First, package management is a breeze with Cargo. Second, you don\u0026rsquo;t have to worry about \u0026ldquo;npm vulnerabilities\u0026rdquo;. Have you ever gone into your project and ran npm audit?\nThis is solvable with Rust.\nRuntime Errors An additional advantage is that you don\u0026rsquo;t have to worry about common runtime errors like undefined is not a function or null is not an object. These are all picked-up by Rust on compile time. So you can focus on the logic of your application knowing that it will work as expected.\nA common workflow in Rust fullstack applications is to use Rust\u0026rsquo;s powerful type system to parse any user input into a type that you can trust, and then propagate that type throughout your application. This way you can be sure that you\u0026rsquo;re not going to have any runtime errors due to invalid input. This is not the case with \u0026ldquo;YavaScript\u0026rdquo;. You need to validate the input at every step of the way, and you can\u0026rsquo;t be sure that the input is valid at any point in time.\nYou can sleep soundly at night knowing that your application won\u0026rsquo;t crash and as long as the host machine has electricity and internet access, your app is working as expected2.\nPerformance Rust is known for its performance. This is due to the fact that Rust gives you control over deciding on which type you\u0026rsquo;ll use for a variable. This is not the case with \u0026ldquo;YavaScript\u0026rdquo;, where you can\u0026rsquo;t decide if a variable is a number or a string. Also you can use references and lifetimes to avoid copying data around.\nSo, if you make sane decisions, like u8 (unsigned 8-bit integer) instead of i32 (signed 32-bit integer) for a number that will never be greater than 255, you can have a very low memory footprint. Also you can use \u0026amp;str (string slice) instead of String to avoid copying strings around.\nYou just don\u0026rsquo;t have this level of control with \u0026ldquo;YavaScript\u0026rdquo;. You get either strings or numbers and you can\u0026rsquo;t decide on the size of the number. And all of your strings will be heap-allocated and copied around.\nProgressive Web Apps Progressive Web Apps (PWAs) are web applications that are regular web pages or websites, but can appear to the user like traditional applications or native mobile applications. Since they use the device\u0026rsquo;s browser, they don\u0026rsquo;t need to be installed through an app store. This is a great advantage, since you don\u0026rsquo;t have to ask for permissions to Google or Tim Apple.\nIn Dioxus making a PWA was really easy. There is a PWA template in the examples/ directory in their repository. You just have to follow the instructions in the README and you\u0026rsquo;re done. In my case, I only had to change the metadata in the manifest.json file and add what I wanted to cache in the service worker .js file. These were only the favicon icon and the CSS style file.\nSudoku Algorithm I didn\u0026rsquo;t have to worry about the algorithm to generate the Sudoku board. This was already implemented in the sudoku crate. But I had to implement some Sudoku logic to make the user interface work.\nSome things that I had to implement were:\nfind the related cells. Given a cell, find the cells in the same row, column and sub-grid. find the conflicting cells. Given a cell, find the cells in the same row, column and sub-grid that have the same value. Find the Related Cells This was a simple task, yet it was very fun to implement.\nTo get the related cells, you need to find the row and column of the cell. Then you can find the start row and start column of the 3x3 sub-grid. After that, you can add the cells in the same row, column and sub-grid to a vector. Finally, you can remove the duplicates and the original cell from the vector.\nHere\u0026rsquo;s the code:\npub fn get_related_cells(index: u8) -\u0026gt; Vec\u0026lt;u8\u0026gt; { let mut related_cells = Vec::new(); let row = index / 9; let col = index % 9; let start_row = row / 3 * 3; let start_col = col / 3 * 3; // Add cells in the same row for i in 0..9 { related_cells.push(row * 9 + i); } // Add cells in the same column for i in 0..9 { related_cells.push(i * 9 + col); } // Add cells in the same 3x3 sub-grid for i in start_row..start_row + 3 { for j in start_col..start_col + 3 { related_cells.push(i * 9 + j); } } // Remove duplicates and the original cell related_cells.sort_unstable(); related_cells.dedup(); related_cells.retain(|\u0026amp;x| x != index); related_cells } Find the Conflicting Cells To find the conflicting cells, you need to get the value of the target cell. Then you can get the related cells and filter the ones that have the same value as the target cell. Easy peasy.\nHere\u0026rsquo;s the code:\npub fn get_conflicting_cells(board: \u0026amp;SudokuState, index: u8) -\u0026gt; Vec\u0026lt;u8\u0026gt; { // Get the value of the target cell let value = board[index as usize]; // Ignore if the target cell is empty (value 0) if value == 0 { return Vec::new(); } // Get related cells let related_cells = get_related_cells(index); // Find cells that have the same value as the target cell related_cells .into_iter() .filter(|\u0026amp;index| board[index as usize] == value) .collect() } Note that I am using 0 to represent empty cells.\nBut if the user ignores the conflicting cells and adds a number to the board, there will be more conflicting cells than the ones related to the target cell. This can be done with another helper function.\nHere\u0026rsquo;s the code, and I took the liberty of adding the docstrings (the /// comments that renders as documentation):\n/// Get all the conflictings cells for all filled cells in a Sudoku board /// /// ## Parameters /// /// - `current_sudoku: SudokuState` - A reference to the current [`SudokuState`] /// /// ## Returns /// /// Returns a `Vec\u0026lt;u8\u0026gt;` representing all cell\u0026#39;s indices that are conflicting /// with the current Sudoku board. pub fn get_all_conflicting_cells(current_sudoku: \u0026amp;SudokuState) -\u0026gt; Vec\u0026lt;u8\u0026gt; { let filled: Vec\u0026lt;u8\u0026gt; = current_sudoku .iter() .enumerate() .filter_map(|(idx, \u0026amp;value)| { if value != 0 { u8::try_from(idx).ok() } else { None // Filter out the item if the value is 0 } }) .collect(); // Get all conflicting cells for the filled cells let mut conflicting: Vec\u0026lt;u8\u0026gt; = filled .iter() .flat_map(|\u0026amp;v| get_conflicting_cells(current_sudoku, v)) .collect::\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;(); // Retain unique conflicting.sort_unstable(); conflicting.dedup(); conflicting } The trick here is that we are using a flat_map since a naive map would return a nested Vec\u0026lt;Vec\u0026lt;Vec\u0026lt;...\u0026gt;\u0026gt;\u0026gt; of u8s, and we don\u0026rsquo;t want that. We want a flat Vec\u0026lt;u8\u0026gt; of all conflicting cells. Recursion is always tricky, go ask Alan Turing.\nSudoku App State As you can see, I used a SudokuState type to represent the state of the game. This is just a type alias for a [u8; 81] array. This is a very simple and efficient way to represent the state of the game.\nHere\u0026rsquo;s the code:\npub type SudokuState = [u8; 81]; The Sudoku app has also an undo button. This is implemented by using a Vec\u0026lt;SudokuState\u0026gt; to store the history of the game. Every time that the user adds a number to the board, the new update state is pushed to the history vector. When the user clicks the undo button, the last state is popped from the history vector and the board is updated.\nThere\u0026rsquo;s one additional problem with the undo button. It needs to switch the clicked cell to the one that was clicked before. Yet another simple, but fun, task. First you need to find the index at which two given SudokuState, the current and the last, differ by exactly one item.\nAgain I\u0026rsquo;ll add the docstrings since they incorporate some good practices that are worth mentioning:\n/// Finds the index at which two given [`SudokuState`] /// differ by exactly one item. /// /// This function iterates over both arrays in lockstep and checks for a /// pair of elements that are not equal. /// It assumes that there is exactly one such pair and returns its index. /// /// ## Parameters /// /// * `previous: SudokuState` - A reference to the first [`SudokuState`] to compare. /// * `current: SudokuState` - A reference to the second [`SudokuState`] to compare. /// /// ## Returns /// /// Returns `Some(usize)` with the index of the differing element if found, /// otherwise returns `None` if the arrays are identical (which should not /// happen given the problem constraints). /// /// ## Panics /// /// The function will panic if cannot convert any of the Sudoku\u0026#39;s board cells /// indexes from `usize` into a `u8` /// /// ## Examples /// /// ``` /// let old_board: SudokuState = [0; 81]; /// let mut new_boad: SudokuState = [0; 81]; /// new_board[42] = 1; // Introduce a change /// /// let index = find_changed_cell(\u0026amp;old_board, \u0026amp;new_board); /// assert_eq!(index, Some(42)); /// ``` pub fn find_changed_cell(previous: \u0026amp;SudokuState, current: \u0026amp;SudokuState) -\u0026gt; Option\u0026lt;u8\u0026gt; { for (index, (\u0026amp;cell1, \u0026amp;cell2)) in previous.iter().zip(current.iter()).enumerate() { if cell1 != cell2 { return Some(u8::try_from(index).expect(\u0026#34;cannot convert from u8\u0026#34;)); } } None // Return None if no change is found (which should not happen in your case) } The function find_changed_cell can panic if it cannot convert any of the Sudoku\u0026rsquo;s board cells indexes from usize into a u8. Hence, we add a ## Panics section to the docstring to inform the user of this possibility. Additionally, we add an ## Examples section to show how to use the function. These are good practices that are worth mentioning3 and I highly encourage you to use them in your Rust code.\nTests Another advantage of using Rust is that you can write tests for your code without needing to use a third-party library. It is baked into the language and you can run your tests with cargo test.\nHere\u0026rsquo;s an example of a test for the get_conflicting_cells function:\n#[test] fn test_conflicts_multiple() { let board = [ 1, 0, 0, 0, 0, 0, 0, 0, 1, // Row 1 with conflict 0, 1, 0, 0, 0, 0, 0, 0, 0, // Row 2 with conflict 0, 0, 0, 0, 0, 0, 0, 0, 0, // Row 3 0, 0, 0, 0, 0, 0, 0, 0, 0, // Row 4 0, 0, 0, 0, 0, 0, 0, 0, 0, // Row 5 0, 0, 0, 0, 0, 0, 0, 0, 0, // Row 6 0, 0, 0, 0, 0, 0, 0, 0, 0, // Row 7 0, 0, 0, 0, 0, 0, 0, 0, 0, // Row 8 1, 0, 0, 0, 0, 0, 0, 0, 0, // Row 9 with conflict ]; assert_eq!(get_conflicting_cells(\u0026amp;board, 0), vec![8, 10, 72]); } And also two tests for the find_changed_cell function:\n#[test] fn test_find_changed_cell_single_difference() { let old_board: SudokuState = [0; 81]; let mut new_board: SudokuState = [0; 81]; new_board[42] = 1; // Introduce a change assert_eq!(find_changed_cell(\u0026amp;old_board, \u0026amp;new_board), Some(42)); } #[test] fn test_find_changed_cell_no_difference() { let old_board: SudokuState = [0; 81]; // This should return None since there is no difference assert_eq!(find_changed_cell(\u0026amp;old_board, \u0026amp;old_board), None); } Conclusion I had a lot of fun building this game. I gave my mother an amazing gift that she\u0026rsquo;ll treasure forever. Her smartphone has one less spyware now. I deployed a fullstack web app with Rust that is fast, safe and efficient; with the caveat that I didn\u0026rsquo;t touched any \u0026ldquo;YavaScript\u0026rdquo; or complexes build tools.\nI hope you enjoyed this post and that you\u0026rsquo;ll give Rust a try in your next fullstack project.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nAccording to Wikipedia, Sudoku is a logic-based, combinatorial number-placement puzzle. The objective is to fill a 9Ã—9 grid with digits so that each column, each row, and each of the nine 3Ã—3 subgrids that compose the grid contain all of the digits from 1 to 9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nin my case I am sending the bill to Bill Gates, since it is using the GitHub Pages to host the app.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe clippy linter can warn you if you don\u0026rsquo;t add these sections to your docstrings. Just add pedantic = \u0026quot;deny\u0026quot; inside your Cargo.toml file in the [lints.clippy] section and you\u0026rsquo;re good to go.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2024-01-30-sudoku/","summary":"It all started when I had to accompany my mom to the hospital. It was just a routine checkup, but I had to wait for a few hours. I brought my laptop with me, since they have good WiFi and I could work on my projects. Then I realized that my mom was playing a Sudoku1 game on her phone. I couln\u0026rsquo;t help but notice that the game was full of ads and it was asking for a lot of permissions, like location and sensor data.","title":"Fullstack and Progressive Web Apps in Rust: A Tale of a Sudoku Spyware"},{"content":" Warning: This post has mermaid.js enabled, so if you want to view the rendered diagrams, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI love to learn new things and I\u0026rsquo;m passionate about Stoic philosophy. So, when I acquired the domain stoicquotes.io1, I\u0026rsquo;ve decided to give htmx a try.\nWhat is htmx? htmx is a small JavaScript library that allows you to enhance your HTML with attributes to perform AJAX (Asynchronous JavaScript and XML) without writing JavaScript2. It focuses on extending HTML by adding custom attributes that describe how to perform common dynamic web page behaviors like partial page updates, form submission, etc. htmx is designed to be easy to use, requiring minimal JavaScript knowledge, so that you can add interactivity3 to web pages with just HTML.\nLet\u0026rsquo;s contrast this with the Soy stuff like the notorious React framework. React, on the other hand, is a JavaScript library for building user interfaces, primarily through a component-based architecture. It manages the creation of user interface elements, updates the UI efficiently when data changes, and helps keep your UI in sync with the state of your application. React requires a deeper knowledge of JavaScript and understanding of its principles, such as components, state, and props.\nIn simple terms:\nhtmx enhances plain HTML by letting you add attributes for dynamic behaviors, so you can make webpages interactive with no JavaScript coding; you can think of it as boosting your HTML to do more. React is more like building a complex machine from customizable parts that you program with JavaScript, giving you full control over how your application looks and behaves but also requiring more from you in terms of code complexity and architecture. Additionally, React can be slower and less performant than htmx. This is due to htmx manipulating the actual DOM itself, while React updates objects in the Virtual DOM. Afterward, React compares the new Virtual DOM with a pre-update version and calculates the most efficient way to make these changes to the real DOM. So React has to do this whole trip around diff\u0026rsquo;ing all the time the Virtual DOM against the actual DOM for every fucking change.\nFinally, htmx receives pure HTML from the server. React needs to the JSON busboy thing: the server sends JSON, React parses JSON into JavaScript code, then it parses it again to HTML for the browser.\nHere are some mermaid.js diagrams to illustrate what is going on under the hood:\n--- title: htmx --- flowchart LR HTML --\u003e DOM --- title: React --- flowchart LR JSON --\u003e JavaScript --\u003e HTML --\u003e VDOM[Virtual DOM] --\u003e DOM A consequence of these different paradigms is that htmx don\u0026rsquo;t care about what the server sends back and will happily include in the DOM. Hence, front-end and back-end are decoupled and less complex. Whereas in Reactland, we need to have a tight synchronicity between front-end and back-end. If the JSON that the server sends doesn\u0026rsquo;t conform to the exact specifications of the front-end, the application becomes a dumpster fire breaks.\nHypermedia When the web was created it was based on the concept of Hypermedia. Hypermedia refers to a system of interconnected multimedia elements, which can include text, graphics, audio, video, and hyperlinks. It allows users to navigate between related pieces of content across the web or within applications, creating a non-linear way of accessing information.\nHTML follows the Hypermedia protocol. HTML is the native language of browsers4. That\u0026rsquo;s why all the React-like frameworks have to convert JavaScript into HTML. So it\u0026rsquo;s only natural to rely primarily on HTML to deliver content and sprinkle JavaScript sparingly when you need something that HTML cannot offer.\nUnfortunately, HTML has stopped in time. Despite all the richness of HTTP with the diverse request methods: GET, HEAD, POST, PUT, DELETE, CONNECT, OPTIONS, TRACE, PATCH; HTML only has two elements that interact with the server:\n\u0026lt;a\u0026gt;: sends a GET request to fetch new data. \u0026lt;form\u0026gt;: sends a POST request to create new data. That\u0026rsquo;s the main purpose of htmx: allowing HTML elements to leverage all the capabilities of HTTP.\nhtmx in Practice OK, enough of abstract and theoretical concepts. Let\u0026rsquo;s see how htmx works in practice.\nFirst, the only thing you need to do enable htmx is to insert this \u0026lt;script\u0026gt; tag in your HTML:\n\u0026lt;script src=\u0026#34;https://unpkg.com/htmx.org@{version}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; where {version} is the desired htmx version that you\u0026rsquo;ll want to use. I has around 40kb of size.\nInside the code behind stoicquotes.io1, we have the following HTML5:\n\u0026lt;div\u0026gt; \u0026lt;blockquote id=\u0026#34;quote\u0026#34;\u0026gt; Some nice Stoic quote... \u0026lt;/blockquote\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button hx-get=\u0026#34;/quote\u0026#34; hx-trigger=\u0026#34;click\u0026#34; hx-target=\u0026#34;#quote\u0026#34; hx-swap=\u0026#34;outerHTML\u0026#34;\u0026gt; New \u0026lt;/button\u0026gt; When the user clicks (hx-trigger) in the \u0026ldquo;New\u0026rdquo; button, htmx sends a GET request to the /quote endpoint (hx-get). Then it swaps the whole HTML (hx-swap) of the element that has id \u0026ldquo;quote\u0026rdquo; (hx-target). This is accomplished without a single character of JavaScript. Instead we extend HTML by adding new attributes to the HTML elements:\nhx-get hx-trigger hx-target hx-swap The server replies with a new \u0026lt;blockquote\u0026gt; element every time it gets a GET request in the /quote endpoint.\nThis is truly amazing. We just used one line of htmx.\nhtmx adheres to my trifecta of amazing tools6:\npowerful expressive concise Here\u0026rsquo;s a breakdown of what the trifecta of amazing tools means:\nPowerful: A powerful tool has the capability to handle complex, demanding tasks with relative ease. It possesses the strength, performance, and features necessary to accomplish a wide range of functions.\nExpressive: An expressive tool gives users the ability to articulate complex ideas, designs, or concepts with simplicity and nuance. It provides a rich set of capabilities that allow for diverse and sophisticated forms of expression.\nConcise: A concise tool allows for achieving goals with minimal effort or complexity. It focuses on efficiency and effectiveness, often through simplification and the removal of unnecessary components. It should be capable of performing tasks without requiring verbose instructions or processes.\nHow we would do this in React? Now compare this with React.\nFirst, we need to install React. This is not simple, but here\u0026rsquo;s a breakdown:\ninstall Node.js\ninstall React: npm install react react-dom\ncreate an index.js file with some variant of:\nimport { createRoot } from \u0026#39;react-dom/client\u0026#39;; document.body.innerHTML = \u0026#39;\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;; const root = createRoot(document.getElementById(\u0026#39;app\u0026#39;)); root.render(\u0026lt;h1\u0026gt;Hello, world\u0026lt;/h1\u0026gt;); And now here\u0026rsquo;s the code for the Quote component:\nimport React, { useState } from \u0026#39;react\u0026#39;; const Quote = () =\u0026gt; { const [quote, setQuote] = useState(\u0026#39;Some nice Stoic quote...\u0026#39;); const fetchNewQuote = async () =\u0026gt; { try { const response = await fetch(\u0026#39;/quote\u0026#39;); const newQuote = await response.text(); setQuote(newQuote); } catch (error) { console.error(\u0026#39;Error fetching new quote:\u0026#39;, error); } }; return ( \u0026lt;div\u0026gt; \u0026lt;blockquote id=\u0026#34;quote\u0026#34;\u0026gt; {quote} \u0026lt;/blockquote\u0026gt; \u0026lt;button onClick={fetchNewQuote}\u0026gt; New \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } export default Quote; That\u0026rsquo;s a LOT of JavaScript code. The Soy Gods must be smiling upon you, my friend.\nConclusion I highly recommend that you check out htmx, especially the free Hypermedia systems book which goes into details and it is way more comprehensive than this short blog post.\nhtmx is a fresh and elegant approach to build simple reactive web pages. It extends HTML to be able to use all of the capabilities of any JavaScript-based reactive framework without a single drop of JavaScript. You just add some new HTML attributes to your HTML elements.\nI\u0026rsquo;ve had such joy using htmx lately. It made me go back into my early teens, when I was doing HTML pages in GeoCities. Good times, no JavaScript-bloated code.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nyou can find the source code at storopoli/stoic-quotes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYES, yes, no YavaScript. Hooray!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhtmx can do much more, such as lazy loading, infinite scroll, or submitting forms without a full page reload, etc.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nactually we also have WASM.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;ve simplified a bit removing some styling for the purpose of clarity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthere are some other tools that I use that adhere to the trifecta. Most notoriously is Julia and Rust.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2024-01-14-htmx/","summary":"Warning: This post has mermaid.js enabled, so if you want to view the rendered diagrams, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI love to learn new things and I\u0026rsquo;m passionate about Stoic philosophy. So, when I acquired the domain stoicquotes.io1, I\u0026rsquo;ve decided to give htmx a try.\nWhat is htmx? htmx is a small JavaScript library that allows you to enhance your HTML with attributes to perform AJAX (Asynchronous JavaScript and XML) without writing JavaScript2.","title":"htmx: an Oasis in a Desert of Soy"},{"content":" I have an open access and open source1 graduate-level course on Bayesian statistics. It is available in GitHub through the repo storopoli/Bayesian-Statistics. I\u0026rsquo;ve taught it many times and every time was such a joy. It is composed of:\na set of 300+ slides2 covering the theoretical part Stan3 models Turing.jl4 models Now and then I receive emails from someone saying that the materials helped them to understand Bayesian statistics. These kind messages really make my day, and that\u0026rsquo;s why I strive to keep the content up-to-date and relevant.\nI decided to make the repository fully reproducible and testable in CI5 using Nix and GitHub actions.\nHere\u0026rsquo;s what I am testing on every new change to the main repository and every new pull request (PR):\nslides in LaTeX are built and released as PDF in CI typos in content and code are tested Turing.jl models are run and tested in CI using the latest version of Julia, Turing.jl and dependencies Stan models are run and test in CI using the latest version of Stan Nix All of these tests demand a highly reproducible and intricate development environment. That\u0026rsquo;s where Nix comes in. Nix can be viewed as a package manager, operating system, build tool, immutable system, and many things.\nNix is purely functional. Everything is described as an expression/function, taking some inputs and producing deterministic outputs. This guarantees reproducible results and makes caching everything easy. Nix expressions are lazy. Anything described in Nix code will only be executed if some other expression needs its results. This is very powerful but somewhat unnatural for developers not familiar with functional programming.\nI enjoy Nix so much that I use it as the operating system and package manager in all of my computers. Feel free to check my setup at storopoli/flakes.\nThe main essence of the repository setup is the flake.nix file. A Flake is a collection of recipes (Nix derivations) that the repository provides. From the NixOS Wiki article on Flakes:\nFlakes is a feature of managing Nix packages to simplify usability and improve reproducibility of Nix installations. Flakes manages dependencies between Nix expressions, which are the primary protocols for specifying packages. Flakes implements these protocols in a consistent schema with a common set of policies for managing packages.\nI use the Nix\u0026rsquo;s Flakes to not only setup the main repository package, defined in the Flake as just package.default which is the PDF build of the LaTeX slides, but also to setup the development environment, defined in the Flake as the devShell.default, to run the latest versions of Stan and Julia/Turing.jl.\nWe\u0026rsquo;ll go over the Flake file in detail. However, let me show the full Flake file:\n{ description = \u0026#34;A basic flake with a shell\u0026#34;; inputs.nixpkgs.url = \u0026#34;github:NixOS/nixpkgs/nixpkgs-unstable\u0026#34;; inputs.flake-utils.url = \u0026#34;github:numtide/flake-utils\u0026#34;; inputs.pre-commit-hooks.url = \u0026#34;github:cachix/pre-commit-hooks.nix\u0026#34;; outputs = { self, nixpkgs, flake-utils, pre-commit-hooks }: flake-utils.lib.eachDefaultSystem (system: let pkgs = nixpkgs.legacyPackages.${system}; tex = pkgs.texlive.combine { inherit (pkgs.texlive) scheme-small; inherit (pkgs.texlive) latexmk pgf pgfplots tikzsymbols biblatex beamer; inherit (pkgs.texlive) silence appendixnumberbeamer fira fontaxes mwe; inherit (pkgs.texlive) noto csquotes babel helvetic transparent; inherit (pkgs.texlive) xpatch hyphenat wasysym algorithm2e listings; inherit (pkgs.texlive) lstbayes ulem subfigure ifoddpage relsize; inherit (pkgs.texlive) adjustbox media9 ocgx2 biblatex-apa wasy; }; julia = pkgs.julia-bin.overrideDerivation (oldAttrs: { doInstallCheck = false; }); in { checks = { pre-commit-check = pre-commit-hooks.lib.${system}.run { src = ./.; hooks = { typos.enable = true; }; }; }; devShells.default = pkgs.mkShell { packages = with pkgs;[ bashInteractive # pdfpc # FIXME: broken on darwin typos cmdstan julia ]; shellHook = \u0026#39;\u0026#39; export JULIA_NUM_THREADS=\u0026#34;auto\u0026#34; export JULIA_PROJECT=\u0026#34;turing\u0026#34; export CMDSTAN_HOME=\u0026#34;${pkgs.cmdstan}/opt/cmdstan\u0026#34; ${self.checks.${system}.pre-commit-check.shellHook} \u0026#39;\u0026#39;; }; packages.default = pkgs.stdenvNoCC.mkDerivation rec { name = \u0026#34;slides\u0026#34;; src = self; buildInputs = with pkgs; [ coreutils tex gnuplot biber ]; phases = [ \u0026#34;unpackPhase\u0026#34; \u0026#34;buildPhase\u0026#34; \u0026#34;installPhase\u0026#34; ]; buildPhase = \u0026#39;\u0026#39; export PATH=\u0026#34;${pkgs.lib.makeBinPath buildInputs}\u0026#34;; cd slides export HOME=$(pwd) latexmk -pdflatex -shell-escape slides.tex \u0026#39;\u0026#39;; installPhase = \u0026#39;\u0026#39; mkdir -p $out cp slides.pdf $out/ \u0026#39;\u0026#39;; }; }); } A flake is composed primarily of inputs and outputs. As inputs I have:\ninputs.nixpkgs.url = \u0026#34;github:NixOS/nixpkgs/nixpkgs-unstable\u0026#34;; inputs.flake-utils.url = \u0026#34;github:numtide/flake-utils\u0026#34;; inputs.pre-commit-hooks.url = \u0026#34;github:cachix/pre-commit-hooks.nix\u0026#34;; nixpkgs is responsible for providing all of the packages necessary for both package.default and devShell.default: cmdstan, julia-bin, typos, and a bunch of texlive LaTeX small packages. flake-utils are a bunch of Nix utility functions that creates tons of syntactic sugar to make the Flake easily accessible in all platforms, such as macOS and Linux. pre-commit-hooks is a nice Nix utility to create easy git hooks that do some checking at several steps of the git workflow. The only hook that I am using is the typos pre-commit hook that checks the whole commit changes for common typos and won\u0026rsquo;t let you commit successfully if you have typos: either correct or whitelist them in the _typos.toml file. The outputs are the bulk of the Flake file and it is a Nix function that takes all the above as inputs and outputs a couple of things:\noutputs = { self, nixpkgs, flake-utils, pre-commit-hooks }: flake-utils.lib.eachDefaultSystem (system: { checks = ... devShells = ... packages = ... }); checks things that are executed/built when you run nix flake check devShells things that are executed/built when you run nix develop packages things that are executed/built when you run nix build Let\u0026rsquo;s go over each one of the outputs that the repository Flake has.\npackages \u0026ndash; LaTeX slides We all know that LaTeX is a pain to make it work. If it builds in my machine definitely won\u0026rsquo;t build in yours. This is solved effortlessly in Nix. Take a look at the tex variable definition in the let ... in block:\nlet # ... tex = pkgs.texlive.combine { inherit (pkgs.texlive) scheme-small; inherit (pkgs.texlive) latexmk pgf pgfplots tikzsymbols biblatex beamer; inherit (pkgs.texlive) silence appendixnumberbeamer fira fontaxes mwe; inherit (pkgs.texlive) noto csquotes babel helvetic transparent; inherit (pkgs.texlive) xpatch hyphenat wasysym algorithm2e listings; inherit (pkgs.texlive) lstbayes ulem subfigure ifoddpage relsize; inherit (pkgs.texlive) adjustbox media9 ocgx2 biblatex-apa wasy; }; # ... in tex is a custom instantiation of the texlive.combine derivation with some overrides to specify which CTAN packages you need to build the slides. We use tex in the packages.default Flake output:\npackages.default = pkgs.stdenvNoCC.mkDerivation rec { name = \u0026#34;slides\u0026#34;; src = self; buildInputs = with pkgs; [ coreutils tex gnuplot biber ]; phases = [ \u0026#34;unpackPhase\u0026#34; \u0026#34;buildPhase\u0026#34; \u0026#34;installPhase\u0026#34; ]; buildPhase = \u0026#39;\u0026#39; export PATH=\u0026#34;${pkgs.lib.makeBinPath buildInputs}\u0026#34;; cd slides export HOME=$(pwd) latexmk -pdflatex -shell-escape slides.tex \u0026#39;\u0026#39;; installPhase = \u0026#39;\u0026#39; mkdir -p $out cp slides.pdf $out/ \u0026#39;\u0026#39;; }; Here we are declaring a Nix derivation with the stdenvNoCC.mkDerivation, the NoCC part means that we don\u0026rsquo;t need C/C++ build tools. The src is the Flake repository itself and I also specify the dependencies in buildInputs: I still need some fancy stuff to build my slides. Finally, I specify the several phases of the derivation. The most important part is that I cd into the slides/ directory and run latexmk in it, and copy the resulting PDF to the $out Nix special directory which serves as the output directory for the derivation.\nThis is really nice because anyone with Nix installed can run:\n$ nix build github:storopoli/Bayesian-Statistics and bingo! You have my slides as PDF built from LaTeX files without having to clone or download the repository. Fully reproducible in any machine or architecture.\nThe next step is to configure GitHub actions to run Nix and build the slides' PDF file in CI. I have two workflows for that and they are almost identical except for the last step. The first one is the build-slides.yml, which, of course, builds the slides. These are the relevant parts:\nname: Build Slides runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 - name: Install Nix uses: DeterminateSystems/nix-installer-action@v8 - name: Build Slides run: nix build -L - name: Copy result out of nix store run: cp -v result/slides.pdf slides.pdf - name: Upload Artifacts uses: actions/upload-artifact@v3 with: name: output path: ./slides.pdf if-no-files-found: error Here we use a set of actions to:\ninstall Nix build the slides\u0026rsquo; PDF file using nix build (the -L flag is to have more verbose logs) upload the built slides\u0026rsquo; PDF file as an artifact of the CI run. This is useful for inspection and debugging. There is also the caveat that if the PDF file is not found the whole workflow should error. The last one is the release-slides.yml, which releases the slides when I publish a new tag. It is almost the same as build-slides.yml, thus I will only highlight the relevant bits:\non: push: tags: - \u0026#34;*\u0026#34; # ... - name: Release uses: ncipollo/release-action@v1 id: release with: artifacts: ./slides.pdf The only change is the final step that we now use a release-action that automatically publishes a release with the slides\u0026rsquo; PDF file as one of the release artifacts. This is good since, once I achieve a milestone in the slides, I can easily tag a new version and have GitHub automatically publish a new release with the resulting PDF file attached in the release.\nThis is a very good workflow, both in GitHub but also locally. I don\u0026rsquo;t need to install tons of gigabytes of texlive stuff to build my slides locally. I just run nix build. Also, if someones contributes to the slides I don\u0026rsquo;t need to check the correctness of the LaTeX code, only the content and the output PDF artifact in the resulting CI from the PR. If it\u0026rsquo;s all good, just thank the blessed soul and merge it!\nTuring.jl Models The repository has a directory called turing/ which is a Julia project with .jl files and a Project.toml that lists the Julia dependencies and appropriate compat bounds. In order to test the Turing.jl models in the Julia files, I have the following things in the Nix Flake devShell:\nlet # ... julia = pkgs.julia-bin.overrideDerivation (oldAttrs: { doInstallCheck = false; }); # ... in # ... devShells.default = pkgs.mkShell { packages = with pkgs;[ # ... julia # ... ]; shellHook = \u0026#39;\u0026#39; # ... export JULIA_NUM_THREADS=\u0026#34;auto\u0026#34; export JULIA_PROJECT=\u0026#34;turing\u0026#34; # ... \u0026#39;\u0026#39;; }; Nix devShell lets you create a development environment by adding a transparent layer on top of your standard shell environment with additional packages, hooks, and environment variables. First, in the let ... in block, I am defining a variable called julia that is the julia-bin package with an attribute doInstallCheck being overridden to false. I don\u0026rsquo;t want the Nix derivation of the mkShell to run all Julia standard tests. Next, I define some environment variables in the shellHook, which, as the name implies, runs every time that I instantiate the default devShell with nix develop.\nWith the Nix Flake part covered, let\u0026rsquo;s check how we wrap everything in a GitHub action workflow file named models.yml. Again, I will only highlight the relevant parts for the Turing.jl model testing CI job:\njobs: test-turing: name: Test Turing Models runs-on: ubuntu-latest strategy: matrix: jl-file: [ \u0026#34;01-predictive_checks.jl\u0026#34;, # ... \u0026#34;13-model_comparison-roaches.jl\u0026#34;, ] steps: # ... - name: Test ${{ matrix.jl-file }} run: | nix develop -L . --command bash -c \u0026#34;julia -e \u0026#39;using Pkg; Pkg.instantiate()\u0026#39;\u0026#34; nix develop -L . --command bash -c \u0026#34;julia turing/${{ matrix.jl-file }}\u0026#34; I list all the Turing.jl model Julia files in a matrix.jl-file list to define variations for each job. Next, we install the latest Julia version. Finally, we run everything in parallel using the YAML string interpolation ${{ matrix.jl-file }}. This expands the expression into N parallel jobs, where N is the jl-file list length.\nIf any of these parallel jobs error out, then the whole workflow will error. Hence, we are always certain that the models are up-to-date with the latest Julia version in nixpkgs, and the latest Turing.jl dependencies.\nStan Models The repository has a directory called stan/ that holds a bunch of Stan models in .stan files. These models can be used with any Stan interface, such as RStan/CmdStanR, PyStan/CmdStanPy, or Stan.jl. However I am using CmdStan which only needs a shell environment and Stan, no additional dependencies like Python, R, or Julia. Additionally, nixpkgs has a cmdstan package that is well-maintained and up-to-date with the latest Stan release.\nIn order to test the Stan models, I have the following setup in the Nix Flake devShell:\ndevShells.default = pkgs.mkShell { packages = with pkgs;[ # ... cmdstan # ... ]; shellHook = \u0026#39;\u0026#39; # ... export CMDSTAN_HOME=\u0026#34;${pkgs.cmdstan}/opt/cmdstan\u0026#34; # ... \u0026#39;\u0026#39;; }; Here I am also defining an environment variable in the shellHook, CMDSTAN_HOME because that is useful for local development.\nIn the same GitHub action workflow models.yml file is defined the Stan model testing CI job:\njobs: test-stan: name: Test Stan Models runs-on: ubuntu-latest strategy: matrix: stan: [ { model: \u0026#34;01-predictive_checks-posterior\u0026#34;, data: \u0026#34;coin_flip.data.json\u0026#34;, }, # ... { model: \u0026#34;13-model_comparison-zero_inflated-poisson\u0026#34;, data: \u0026#34;roaches.data.json\u0026#34;, }, ] steps: # ... - name: Test ${{ matrix.stan.model }} run: | echo \u0026#34;Compiling: ${{ matrix.stan.model }}\u0026#34; nix develop -L . --command bash -c \u0026#34;stan stan/${{ matrix.stan.model }}\u0026#34; nix develop -L . --command bash -c \u0026#34;stan/${{ matrix.stan.model }} sample data file=stan/${{ matrix.stan.data }}\u0026#34; Now I am using a YAML dictionary as the entry for every element in the stan YAML list with two keys: model and data. model lists the Stan model file without the .stan extension, and data lists the JSON data file that the model needs to run. We\u0026rsquo;ll use both to run parallel jobs to test all the Stan models listed in the stan list. For that we use the following commands:\nnix develop -L . --command bash -c \u0026#34;stan stan/${{ matrix.stan.model }}\u0026#34; nix develop -L . --command bash -c \u0026#34;stan/${{ matrix.stan.model }} sample data file=stan/${{ matrix.stan.data }}\u0026#34; This instantiates the devShell.default shell environment, and uses the stan binary provided by the cmdstan Nix package to compile the model into an executable binary. Next, we run this model executable binary in sample mode while also providing the corresponding data file with data file=.\nAs before, if any of these parallel jobs error out, then the whole workflow will error. Hence, we are always certain that the models are up-to-date with the latest Stan/CmdStan version in nixpkgs.\nConclusion I am quite happy with this setup. It makes easy to run test in CI with GitHub Actions, while also being effortless to instantiate a development environment with Nix. If I want to get a new computer up and running, I don\u0026rsquo;t need to install a bunch of packages and go over \u0026ldquo;getting started\u0026rdquo; instructions to have all the necessary dependencies.\nThis setup also helps onboard new contributors since it is:\neasy to setup the dependencies necessary to develop and test trivial to check if contributions won\u0026rsquo;t break anything Speaking of \u0026ldquo;contributors\u0026rdquo;, if you are interested in Bayesian modeling, feel free to go over the contents of the repository storopoli/Bayesian-Statistics. Contributions are most welcomed. Don\u0026rsquo;t hesitate on opening an issue or pull request.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nthe code is MIT-licensed and the content is CreativeCommons Non-Commercial 4.0\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI am also planning to go over the slides for every lecture in a YouTube playlist in the near future. This would make it the experience complete: slides, lectures, and code.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\na probabilistic programming language and suite of MCMC samplers written in C++. It is today\u0026rsquo;s gold standard in Bayesian stats.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nis an ecosystem of Julia packages for Bayesian inference using probabilistic programming.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCI stands for continuous integration, sometimes also known as CI/CD, continuous integration and continuous delivery. CI/CD is a wide \u0026ldquo;umbrella\u0026rdquo; term for \u0026ldquo;everything that is tested in all parts of the development cicle\u0026rdquo;, and these tests commonly take place in a cloud machine.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2023-12-04-bayesian_models_ci/","summary":"I have an open access and open source1 graduate-level course on Bayesian statistics. It is available in GitHub through the repo storopoli/Bayesian-Statistics. I\u0026rsquo;ve taught it many times and every time was such a joy. It is composed of:\na set of 300+ slides2 covering the theoretical part Stan3 models Turing.jl4 models Now and then I receive emails from someone saying that the materials helped them to understand Bayesian statistics. These kind messages really make my day, and that\u0026rsquo;s why I strive to keep the content up-to-date and relevant.","title":"Testing Bayesian Models with Nix and GitHub Actions"},{"content":"In programming language circles there\u0026rsquo;s a recently trend of discussing a concept called zero-cost abstractions: the ability to use higher-levels abstractions without suffering any loss of performance.\nZero-cost abstractions allows you to write performant code without having to give up a single drop of convenience and expressiveness:\nYou want for-loops? You can have it. Generics? Yeah, why not? Data structures? Sure, keep\u0026rsquo;em coming. Async operations? You bet ya! Multi-threading? Hell yes!\nTo put more formally, I like this definition from StackOverflow:\nZero Cost Abstractions means adding higher-level programming concepts, like generics, collections and so on do not come with a run-time cost, only compile time cost (the code will be slower to compile). Any operation on zero-cost abstractions is as fast as you would write out matching functionality by hand using lower-level programming concepts like for loops, counters, ifs and using raw pointers.\nHere\u0026rsquo;s an analogy:\nImagine that you are going to buy a car. The sales person offers you a fancy car praising how easy it is to drive it, that you don\u0026rsquo;t need to think about RPM, clutch and stick shift, parking maneuver, fuel type, and other shenanigans. You just turn it on and drive. However, once you take a look at the car\u0026rsquo;s data sheet, you are horrified. The car is bad in every aspect except easy of use. It has dreadful fuel consumption, atrocious safety ratings, disastrous handling, and so on\u0026hellip;\nBelieve me, you wouldn\u0026rsquo;t want to own that car.\nMetaphors aside, that\u0026rsquo;s exactly what professional developers1 and whole teams choose to use every day: unacceptable inferior tools. Tools that, not only don\u0026rsquo;t have zero-cost abstractions, rather don\u0026rsquo;t allow you to even have non-zero-cost anything!\nLet\u0026rsquo;s do some Python bashing in the meantime. I know that\u0026rsquo;s easy to bash Python, but that\u0026rsquo;s not the point. If Python wasn\u0026rsquo;t used so widely in production, I would definitely leave it alone. Don\u0026rsquo;t get me wrong, Python is the second-best language for everything2.\nThe curious case of the Python boolean I wish this meme was a joke, but it isn\u0026rsquo;t. A boolean is one of the simplest data type taking only two possible values: true or false. Just grab your nearest Python REPL:\n\u0026gt;\u0026gt;\u0026gt; from sys import getsizeof \u0026gt;\u0026gt;\u0026gt; getsizeof(True) 28 The function sys.getsizeof returns the size of an object in bytes. How the hell Python needs 28 bytes to represent something that needs at most 1 byte3? Imagine incurring a 28x penalty in memory size requirements for every boolean that you use. Now multiply this by every operation that your code is going to run in production over time. Again: unacceptable.\nThat\u0026rsquo;s because all objects in Python, in the sense that everything that you can instantiate, i.e. everything that you can put on the left hand-side of the = assignment, is a PyObject:\nAll Python objects ultimately share a small number of fields at the beginning of the objectâ€™s representation in memory. These are represented by the PyObject and PyVarObject types.\nPython is dynamically-typed, which means that you don\u0026rsquo;t have primitives like 8-, 16-, 32-bit (un)signed integers and so on. Everything is a huge mess allocated in the heap that must carry not only its value, but also information about its type.\nMost important, everything that is fast in Python is not Python-based. Take a look at the image below, I grabbed some popular Python libraries from GitHub, namely NumPy (linear algebra package) and PyToch (deep learning package), and checked the language codebase percentage.\nSurprise, they are not Python libraries. They are C/C++ codebases. Even if Python is the main language used in these codebases4, I still think that this is not the case due to the nature of the Python code: all docstrings are written in Python. If you have a very fast C function in your codebase that takes 50 lines of code, followed by a Python wrapper function that calls it using 10 lines of code, but with a docstring that is 50 lines of code; you have a \u0026ldquo;Python\u0026rdquo;-majority codebase.\nIn a sense the most efficient Python programmer is a C/C++ programmer\u0026hellip;\nHere\u0026rsquo;s Julia, which is also dynamically-typed:\njulia\u0026gt; Base.summarysize(true) 1 And to your surprise, Julia is coded in \u0026hellip;. Julia! Check the image below for the language codebase percentage of Julia and Lux.jl5 (deep learning package).\nFinally, here\u0026rsquo;s Rust, which is not dynamically-, but static-typed:\n// main.rs use std::mem; fn main() { println!(\u0026#34;Size of bool: {} byte\u0026#34;, mem::size_of::\u0026lt;bool\u0026gt;()); } $ cargo run --release Compiling size_of_bool v0.1.0 Finished release [optimized] target(s) in 0.00s Running `target/release/size_of_bool` Size of bool: 1 byte More zero-costs abstractions Let\u0026rsquo;s cover two more zero-costs abstractions, both in Julia and in Rust: for-loops and enums.\nFor-loops A friend and a Julia-advocate once told me that Julia\u0026rsquo;s master plan is to secretly \u0026ldquo;make everyone aware about compilers\u0026rdquo;. The compiler is a program that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.\nPython uses CPython as the compiler. If you search around on why CPython/Python is so slow and inefficient, you\u0026rsquo;ll find that the culprits are:\nPython is dynamic-typed language. Python\u0026rsquo;s Global Interpreter Lock (GIL) restricts multi-threading capabilities. Python is interpreted, which means that Python code is executed sequentially: line-by-line. Python is garbage-collected: all memory its tracked, and allocated or deallocated which introduces overhead. I completely disagree with almost all the above reasons, except the GIL. Python is slow because of its design decisions, more specifically the way CPython works under the hood. It is not built for performance in mind. Actually, the main objective of Python was to be a \u0026ldquo;language that would be easy to read, write, and maintain\u0026rdquo;. I salute that: Python has remained true to its main objective.\nNow let\u0026rsquo;s switch to Julia:\nJulia is dynamic-typed language. Julia is interpreted, which means that Julia code is executed sequentially: line-by-line. Julia is garbage-collected: all memory its tracked, and allocated or deallocated which introduces overhead. I\u0026rsquo;ve copy-pasted all Python\u0026rsquo;s arguments for inefficiency, except the GIL. And, contrary to Python, Julia is fast! Sometimes even faster than C6. Actually, that was the goal all along since Julia\u0026rsquo;s inception. If you check the notorious Julia announcement blog post from 2012:\nWe want a language that\u0026rsquo;s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that\u0026rsquo;s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.\n(Did we mention it should be as fast as C?)\nIt mentions \u0026ldquo;speed\u0026rdquo; twice. Not only that, but also specifically says that it should match C\u0026rsquo;s speed.\nJulia is fast because of its design decisions. One of the major reasons why Julia is fast is because of the choice of compiler that it uses: LLVM.\nLLVM originally stood for low level virtual machine. Despite its name, LLVM has little to do with traditional virtual machines. LLVM can take intermediate representation (IR) code and compile it into machine-dependent instructions. It has support and sponsorship from a lot of big-tech corporations, such as Apple, Google, IBM, Meta, Arm, Intel, AMD, Nvidia, and so on. It is a pretty fast compiler that can do wonders in optimizing IR code to a plethora of computer architectures.\nIn a sense, Julia is a front-end for LLVM. It turns your easy-to-read and easy-to-write Julia code into LLVM IR code. Take this for-loop example inside a function:\nfunction sum_10() acc = 0 for i in 1:10 acc += i end return acc end Let\u0026rsquo;s check what Julia generates as LLVM IR code for this function. We can do that with the @code_llvm macro.\njulia\u0026gt; @code_llvm debuginfo=:none sum_10() define i64 @julia_sum_10_172() #0 { top: ret i64 55 } You can\u0026rsquo;t easily fool the compiler. Julia understands that the answer is 55, and the LLVM IR generated code is pretty much just \u0026ldquo;return 55 as a 64-bit integer\u0026rdquo;.\nLet\u0026rsquo;s also check the machine-dependent instructions with the @code_native macro. I am using an Apple Silicon machine, so these instructions might differ from yours:\njulia\u0026gt; @code_native debuginfo=:none sum_10() .section __TEXT,__text,regular,pure_instructions .build_version macos, 14, 0 .globl _julia_sum_10_214 ; -- Begin function julia_sum_10_214 .p2align 2 _julia_sum_10_214: ; @julia_sum_10_214 .cfi_startproc ; %bb.0: ; %top mov w0, #55 ret .cfi_endproc ; -- End function .subsections_via_symbols The only important instruction for our argument here is the mov w0, #55. This means \u0026ldquo;move the value 55 into the w0 register\u0026rdquo;, where w0 is one of registers available in ARM-based architectures (which Apple Silicon chips are).\nThis is a zero-cost abstraction! I don\u0026rsquo;t need to give up for-loops, because they might be slow and inefficient; like some Python users suggest newcomers. I can have the full convenience and expressiveness of for-loops without paying performance costs. Pretty much the definition of a zero-cost abstraction from above.\nUsing LLVM as a compiler backend is not something unique to Julia. Rust also uses LLVM under the hood. Take for example this simple Rust code:\n// main.rs pub fn sum_10() -\u0026gt; i32 { let mut acc = 0; for i in 1..=10 { acc += i } acc } fn main() { println!(\u0026#34;sum_10: {}\u0026#34;, sum_10()); } We can inspect both LLVM IR code and machine instructions with the cargo-show-asm crate:\n$ cargo asm --llvm \u0026#34;sum_10::main\u0026#34; | grep 55 Finished release [optimized] target(s) in 0.00s store i32 55, ptr %_9, align 4 $ cargo asm \u0026#34;sum_10::main\u0026#34; | grep 55 Finished release [optimized] target(s) in 0.00s mov w8, #55 No coincidence that the LLVM IR code is very similar, with the difference that integers, by default, in Julia are 64 bits and in Rust 32 bits. However, the machine code is identical: \u0026ldquo;move the value 55 into a w something register\u0026rdquo;.\nEnums Another zero-cost abstraction, in Julia and Rust, is enums.\nIn Julia all enums, by default have a BaseType of Int32: a signed 32-bit integer. However, we can override this with type annotations:\njulia\u0026gt; @enum Thing::Bool One Two julia\u0026gt; Base.summarysize(Thing(false)) 1 Here we have an enum Thing with two variants: One and Two. Since we can safely represent all the possible variant space of Thing with a boolean type, we override the BaseType of Thing to be the Bool type. Unsurprised, any object of Thing occupies 1 byte in memory.\nWe can achieve the same with Rust:\n// main.rs use std::mem; #[allow(dead_code)] enum Thing { One, Two, } fn main() { println!(\u0026#34;Size of Thing: {} byte\u0026#34;, mem::size_of::\u0026lt;Thing\u0026gt;()); } $ cargo run --release Compiling enum_size v0.1.0 Finished release [optimized] target(s) in 0.09s Running `target/release/enum_size` Size of Thing: 1 byte However, contrary to Julia, Rust compiler automatically detects the enum\u0026rsquo;s variant space size and adjust accordingly. So, no need of overrides.\nConclusion Zero-cost abstractions are a joy to have in a programming language. It enables you, as a programmer, to just focus on what\u0026rsquo;s important: write expressive code that is easy to read, maintain, debug, and build upon.\nIt is no wonder that zero-cost abstractions is a pervasive feature of two of my top-favorite languages: Julia and Rust.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nthis post is somehow connected to my soydev rant.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand that\u0026rsquo;s not a compliment.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ntechnically, we can represent a boolean with just one bit. However, the short answer is still one byte, because that\u0026rsquo;s smallest addressable unit of memory.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand modifying .gitattributes is cheating. Yes, I am talking to you NumPy!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLux.jl doesn\u0026rsquo;t even have a .gitattributes file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nif you compare runtime execution.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2023-11-28-zero_cost_abstractions/","summary":"In programming language circles there\u0026rsquo;s a recently trend of discussing a concept called zero-cost abstractions: the ability to use higher-levels abstractions without suffering any loss of performance.\nZero-cost abstractions allows you to write performant code without having to give up a single drop of convenience and expressiveness:\nYou want for-loops? You can have it. Generics? Yeah, why not? Data structures? Sure, keep\u0026rsquo;em coming. Async operations? You bet ya! Multi-threading? Hell yes!","title":"Zero-cost Abstractions"},{"content":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2. Lindley predicted in 1975 that \u0026ldquo;Bayesian methods will indeed become pervasive, enabled by the development of powerful computing facilities\u0026rdquo; (Lindley, 1975). You can find more about all of Lindley\u0026rsquo;s achievements in his obituary.\nLindley\u0026rsquo;s Paradox Lindley\u0026rsquo;s paradox3 is a counterintuitive situation in statistics in which the Bayesian and frequentist approaches to a hypothesis testing problem give different results for certain choices of the prior distribution.\nMore formally, the paradox is as follows. We have some parameter $\\theta$ that we are interested in. Then, we proceed with an experiment to test two competing hypotheses:\n$H_0$ (also known as null hypothesis): there is no \u0026ldquo;effect\u0026rdquo;, or, more specifically, $\\theta = 0$. $H_a$ (also known as alternative hypothesis): there is an \u0026ldquo;effect\u0026rdquo;, or, more specifically, $\\theta \\ne 0$. The paradox occurs when two conditions are met:\nThe result of the experiment is significant by a frequentist test of $H_0$, which indicates sufficient evidence to reject $H_0$, at a certain threshold of probability4. The posterior probability (Bayesian approach) of $H_0 \\mid \\theta$ (null hypothesis given $\\theta$) is high, which indicates strong evidence that $H_0$ should be favored over $H_a$, that is, to not reject $H_0$. These results can occur at the same time when $H_0$ is very specific, $H_a$ more diffuse, and the prior distribution does not strongly favor one or the other. These conditions are pervasive across science and common in traditional null-hypothesis significance testing approaches.\nThis is a duel of frequentist versus Bayesian approaches, and one of the many in which Bayesian emerges as the most coherent. Let\u0026rsquo;s give a example and go over the analytical result with a ton of math, but also a computational result with Julia.\nExample Here\u0026rsquo;s the setup for the example. In a certain city 49,581 boys and 48,870 girls have been born over a certain time period. The observed proportion of male births is thus $\\frac{49,581}{98,451} \\approx 0.5036$.\nWe assume that the birth of a child is independent with a certain probability $\\theta$. Since our data is a sequence of $n$ independent Bernoulli trials, i.e., $n$ independent random experiments with exactly two possible outcomes: \u0026ldquo;success\u0026rdquo; and \u0026ldquo;failure\u0026rdquo;, in which the probability of success is the same every time the experiment is conducted. We can safely assume that it follows a binomial distribution with parameters:\n$n$: the number of \u0026ldquo;trials\u0026rdquo; (or the total number of births). $\\theta$: the probability of male births. We then set up our two competing hypotheses:\n$H_0$: $\\theta = 0.5$. $H_a$: $\\theta \\ne 0.5$. Analytical Solution This is a toy-problem and, like most toy problems, we can solve it analytically5 for both the frequentist and the Bayesian approaches.\nAnalytical Solutions \u0026ndash; Frequentist Approach The frequentist approach to testing $H_0$ is to compute a $p$-value4, the probability of observing births of boys at least as large as 49,581 assuming $H_0$ is true. Because the number of births is very large, we can use a normal approximation6 for the binomial-distributed number of male births. Let\u0026rsquo;s define $X$ as the total number of male births, then $X$ follows a normal distribution:\n$$X \\sim \\text{Normal}(\\mu, \\sigma)$$\nwhere $\\mu$ is the mean parameter, $n \\theta$ in our case, and $\\sigma$ is the standard deviation parameter, $\\sqrt{n \\theta (1 - \\theta)}$. We need to calculate the conditional probability of $X \\geq \\frac{49,581}{98,451} \\approx 0.5036$ given $\\mu = n \\theta = 98,451 \\cdot \\frac{1}{2} = 49,225.5$ and $\\sigma = \\sqrt{n \\theta (1 - \\theta)} = \\sqrt{98,451 \\cdot \\frac{1}{2} \\cdot (1 - \\frac{1}{2})}$:\n$$P(X \\ge 0.5036 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75})$$\nThis is basically a cumulative distribution function (CDF) of $X$ on the interval $[49,225.5, 98,451]$:\n$$\\int_{49,225.5}^{98,451} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{\\left( \\frac{x - \\mu}{\\sigma} \\right)^2}{2}} dx$$\nAfter inserting the values and doing some arithmetic, our answer is approximately $0.0117$. Note that this is a one-sided test, since it is symmetrical, the two-sided test would be $0.0117 \\cdot 2 = 0.0235$. Since we don\u0026rsquo;t deviate from the Fisher\u0026rsquo;s canon, this is well below the 5% threshold. Hooray! We rejected the null hypothesis! Quick! Grab a frequentist celebratory cigar! But, wait. Let\u0026rsquo;s check the Bayesian approach.\nAnalytical Solutions \u0026ndash; Bayesian Approach For the Bayesian approach, we need to set prior probabilities on both hypotheses. Since we do not favor one from another, let\u0026rsquo;s set equal prior probabilities:\n$$P(H_0) = P(H_a) = \\frac{1}{2}$$\nAdditionally, all parameters of interest need a prior distribution. So, let\u0026rsquo;s put a prior distribution on $\\theta$. We could be fancy here, but let\u0026rsquo;s not. We\u0026rsquo;ll use a uniform distribution on $[0, 1]$.\nWe have everything we need to compute the posterior probability of $H_0$ given $\\theta$. For this, we\u0026rsquo;ll use Bayes theorem7:\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\nNow again let\u0026rsquo;s plug in all the values:\n$$P(H_0 \\mid \\theta) = \\frac{P(\\theta \\mid H_0) P(H_0)}{P(\\theta)}$$\nNote that by the axioms of probability and by the product rule of probability we can decompose $P(\\theta)$ into:\n$$P(\\theta) = P(\\theta \\mid H_0) P(H_0) + P(\\theta \\mid H_a) P(H_a)$$\nAgain, we\u0026rsquo;ll use the normal approximation:\n$$ \\begin{aligned} \u0026amp;P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \\\\ \u0026amp;= \\frac{ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\left( \\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 } { \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{ \\left( -\\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 + \\int_0^1 \\frac {1}{\\sqrt{2 \\pi \\sigma^2} } e^{- \\left( \\frac{\\mu - \\mu \\cdot \\theta)}{2 \\sigma} \\right)^2}d \\theta \\cdot 0.5 } \\\\ \u0026amp;= 0.9505 \\end{aligned} $$\nThe likelihood of the alternative hypothesis, $P(\\theta \\mid H_a)$, is just the CDF of all possible values of $\\theta \\ne 0.5$.\n$$P(H_0 \\mid \\text{data}) = P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \u0026gt; 0.95$$\nAnd we fail to reject the null hypothesis, in frequentist terms. However, we can also say in Bayesian terms, that we strongly favor $H_0$ over $H_a$.\nQuick! Grab the Bayesian celebratory cigar! The null is back on the game!\nComputational Solutional For the computational solution, we\u0026rsquo;ll use Julia and the following packages:\nHypothesisTest.jl Turing.jl Computational Solutions \u0026ndash; Frequentist Approach We can perform a BinomialTest with HypothesisTest.jl:\njulia\u0026gt; using HypothesisTests julia\u0026gt; BinomialTest(49_225, 98_451, 0.5036) Binomial test ------------- Population details: parameter of interest: Probability of success value under h_0: 0.5036 point estimate: 0.499995 95% confidence interval: (0.4969, 0.5031) Test summary: outcome with 95% confidence: reject h_0 two-sided p-value: 0.0239 Details: number of observations: 98451 number of successes: 49225 This is the two-sided test, and I had to round $49,225.5$ to $49,225$ since BinomialTest do not support real numbers. But the results match with the analytical solution, we still reject the null.\nComputational Solutions \u0026ndash; Bayesian Approach Now, for the Bayesian computational approach, I\u0026rsquo;m going to use a generative modeling approach, and one of my favorites probabilistic programming languages, Turing.jl:\njulia\u0026gt; using Turing julia\u0026gt; @model function birth_rate() Î¸ ~ Uniform(0, 1) total_births = 98_451 male_births ~ Binomial(total_births, Î¸) end; julia\u0026gt; model = birth_rate() | (; male_births = 49_225); julia\u0026gt; chain = sample(model, NUTS(1_000, 0.8), MCMCThreads(), 1_000, 4) Chains MCMC chain (1000Ã—13Ã—4 Array{Float64, 3}): Iterations = 1001:1:2000 Number of chains = 4 Samples per chain = 1000 Wall duration = 0.2 seconds Compute duration = 0.19 seconds parameters = Î¸ internals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size Summary Statistics parameters mean std mcse ess_bulk ess_tail rhat ess_per_sec Symbol Float64 Float64 Float64 Float64 Float64 Float64 Float64 Î¸ 0.4999 0.0016 0.0000 1422.2028 2198.1987 1.0057 7368.9267 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 Î¸ 0.4969 0.4988 0.4999 0.5011 0.5031 We can see from the output of the quantiles that the 95% quantile for $\\theta$ is the interval $(0.4969, 0.5031)$. Although it overlaps zero, that is not the equivalent of a hypothesis test. For that, we\u0026rsquo;ll use the highest posterior density interval (HPDI), which is defined as \u0026ldquo;choosing the narrowest interval\u0026rdquo; that captures a certain posterior density threshold value. In this case, we\u0026rsquo;ll use a threshold interval of 95%, i.e. an $\\alpha = 0.05$:\njulia\u0026gt; hpd(chain; alpha=0.05) HPD parameters lower upper Symbol Float64 Float64 Î¸ 0.4970 0.5031 We see that we fail to reject the null, $\\theta = 0.5$ at $\\alpha = 0.05$ which is in accordance with the analytical solution.\nWhy the Frequentist and Bayesian Approaches Disagree Why do the approaches disagree? What is going on under the hood?\nThe answer is disappointing8. The main problem is that the frequentist approach only allows fixed significance levels with respect to sample size. Whereas the Bayesian approach is consistent and robust to sample size variations.\nTaken to extreme, in some cases, due to huge sample sizes, the $p$-value is pretty much a proxy for sample size and have little to no utility on hypothesis testing. This is known as $p$-hacking9.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nReferences Lindley, Dennis V. \u0026ldquo;The future of statistics: A Bayesian 21st century\u0026rdquo;. Advances in Applied Probability 7 (1975): 106-115.\nas far as I know there\u0026rsquo;s only one coherent approach to uncertainty, and it is the Bayesian approach. Otherwise, as de Finetti and Ramsey proposed, you are susceptible to a Dutch book. This is a topic for another blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCromwell\u0026rsquo;s rule states that the use of prior probabilities of 1 (\u0026ldquo;the event will definitely occur\u0026rdquo;) or 0 (\u0026ldquo;the event will definitely not occur\u0026rdquo;) should be avoided, except when applied to statements that are logically true or false. Hence, anything that is not a math theorem should have priors in $(0,1)$. The reference comes from Oliver Cromwell, asking, very politely, for the Church of Scotland to consider that their prior probability might be wrong. This footnote also deserves a whole blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStigler\u0026rsquo;s law of eponymy states that no scientific discovery is named after its original discoverer. The paradox was already was discussed in Harold Jeffreys' 1939 textbook. Also, fun fact, Stigler\u0026rsquo;s is not the original creator of such law\u0026hellip; Now that\u0026rsquo;s a self-referential paradox, and a broad version of the Halting problem, which should earn its own footnote. Nevertheless, we are getting into self-referential danger zone here with footnotes\u0026rsquo; of footnotes\u0026rsquo; of footnotes\u0026rsquo;\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthis is called $p$-value and can be easily defined as \u0026ldquo;the probability of sampling data from a target population given that $H_0$ is true as the number of sampling procedures $\\to \\infty$\u0026rdquo;. Yes, it is not that intuitive, and it deserves not a blog post, but a full curriculum to hammer it home.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthat is not true for most of the real-world problems. For Bayesian approaches, we need to run computational asymptotic exact approximations using a class of methods called Markov chain Monte Carlo (MCMC). Furthermore, for some nasty problems we need to use different set of methods called variational inference (VI) or approximate Bayesian computation (ABC).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nif you are curious about how this approximation works, check the backup slides of my open access and open source graduate course on Bayesian statistics.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBayes\u0026rsquo; theorem is officially called Bayes-Price-Laplace theorem. Bayes was trying to disprove David Hume\u0026rsquo;s argument that miracles did not exist (How dare he?). He used the probabilistic approach of trying to quantify the probability of a parameter (god exists) given data (miracles happened). He died without publishing any of his ideas. His wife probably freaked out when she saw the huge pile of notes that he had and called his buddy Richard Price to figure out what to do with it. Price struck gold and immediately noticed the relevance of Bayes\u0026rsquo; findings. He read it aloud at the Royal Society. Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes, used the same probabilistic approach to perform statistical inference using France\u0026rsquo;s first census data in the early-Napoleonic era. Somehow we had the answer to statistical inference back then, and we had to rediscover everything again in the late-20th century\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ndisappointing because most of published scientific studies suffer from this flaw.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand, like all footnotes here, it deserves its own blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2023-11-23-lindley_paradox/","summary":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2.","title":"Lindley's Paradox, or The consistency of Bayesian Thinking"},{"content":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way. Alas, better late than never.\nIn this post, we\u0026rsquo;ll learn how to represent words using word embeddings, and how to use basic trigonometry to play around with them. Of course, we\u0026rsquo;ll use Julia.\nWord Embeddings Word embeddings is a way to represent words as a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\nOk, let\u0026rsquo;s unwrap the above definition. First, a real-valued vector is any vector which its elements belong to the real numbers. Generally we denote vectors with a bold lower-case letter, and we denote its elements (also called components) using square brackets. Hence, a vector $\\bold{v}$ that has 3 elements, $1$, $2$, and $3$, can be written as\n$$\\bold{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\nNext, what \u0026ldquo;close\u0026rdquo; means for vectors? We can use distance functions to get a measurable value. The most famous and commonly used distance function is the Euclidean distance, in honor of Euclid, the \u0026ldquo;father of geometry\u0026rdquo;, and the guy pictured in the image at the top of this post. The Euclidean distance is defined in trigonometry for 2-D and 3-D spaces. However, it can be generalized to any dimension $n \u0026gt; 1$ by using vectors.\nSince every word is represented by an $n$-dimensional vector, we can use distances to compute a metric that represent similarity between vectors. And, more interesting, we can add and subtract words (or any other linear combination of one or more words) to generate new words.\nBefore we jump to code and examples, a quick note about how word embeddings are constructed. They are trained like a regular machine learning algorithm, where the cost function measures the difference between some vector distance between the vectors and a \u0026ldquo;semantic distance\u0026rdquo;. The goal is to iteratively find good vector values that minimize the cost. So, if a vector is close to another vector measured by a distance function, but far apart measured by some semantic distance on the words that these vectors represent, then the cost function will be higher. The algorithm cannot change the semantic distance, it is treated as a fixed value. However, it can change the vector elements\u0026rsquo; values so that the vector distance function closely resembles the semantic distance function. Lastly, generally the dimensionality of the vectors used in word embeddings are high, $n \u0026gt; 50$, since it needs a proper amount of dimensions in order to represent all the semantic information of words with vectors.\nPre-Trained Word Embeddings Generally we don\u0026rsquo;t train our own word embeddings from scratch, we use pre-trained ones. Here is a list of some of the most popular ones:\nWord2Vec: One of the first public available word embeddings, made by Google in 2013. Only supports English. GloVe: made by Stanford in 2014. Only supports English. FastText: From Facebook, released in 2016. Supports hundreds of languages. Julia Code We will use the Embeddings.jl package to easily load word embeddings as vectors, and the Distances.jl package for the convenience of several distance functions. This is a nice example of the Julia package ecosystem composability, where one package can define types, another can define functions, and another can define custom behavior of these functions on types that are defined in other packages.\njulia\u0026gt; using Embeddings julia\u0026gt; using Distances Let\u0026rsquo;s load the GloVe word embeddings. First, let\u0026rsquo;s check what we have in store to choose from GloVe\u0026rsquo;s English language embeddings:\njulia\u0026gt; language_files(GloVe{:en}) 20-element Vector{String}: \u0026#34;glove.6B/glove.6B.50d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.100d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.300d.txt\u0026#34; \u0026#34;glove.42B.300d/glove.42B.300d.txt\u0026#34; \u0026#34;glove.840B.300d/glove.840B.300d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.25d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.50d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.100d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.50d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.100d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.300d.txt\u0026#34; \u0026#34;glove.42B.300d/glove.42B.300d.txt\u0026#34; \u0026#34;glove.840B.300d/glove.840B.300d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.25d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.50d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.100d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.200d.txt\u0026#34; I\u0026rsquo;ll use the \u0026quot;glove.6B/glove.6B.50d.txt\u0026quot;. This means that it was trained with 6 billion tokens, and it provides embeddings with 50-dimensional vectors. The load_embeddings function takes an optional second positional argument as an Int to choose from which index of the language_files to use. Finally, I just want the words \u0026ldquo;king\u0026rdquo;, \u0026ldquo;queen\u0026rdquo;, \u0026ldquo;man\u0026rdquo;, \u0026ldquo;woman\u0026rdquo;; so I am passing these words as a Set to the keep_words keyword argument:\njulia\u0026gt; const glove = load_embeddings(GloVe{:en}, 1; keep_words=Set([\u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;])); Embeddings.EmbeddingTable{Matrix{Float32}, Vector{String}}(Float32[-0.094386 0.50451 -0.18153 0.37854; 0.43007 0.68607 0.64827 1.8233; â€¦ ; 0.53135 -0.64426 0.48764 0.0092753; -0.11725 -0.51042 -0.10467 -0.60284], [\u0026#34;man\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;queen\u0026#34;]) Watch out with the order that we get back. If you see the output of load_embeddings, the order is \u0026quot;man\u0026quot;, \u0026quot;king\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;queen\u0026quot;] Let\u0026rsquo;s see how a word is represented:\njulia\u0026gt; queen = glove.embeddings[:, 4] 50-element Vector{Float32}: 0.37854 1.8233 -1.2648 â‹® -2.2839 0.0092753 -0.60284 They are 50-dimensional vectors of Float32.\nNow, here\u0026rsquo;s the fun part: let\u0026rsquo;s add words and check the similarity between the result and some other word. A classical example is to start with the word \u0026ldquo;king\u0026rdquo;, subtract the word \u0026ldquo;men\u0026rdquo;, add the word \u0026ldquo;woman\u0026rdquo;, and check the distance of the result to the word \u0026ldquo;queen\u0026rdquo;:\njulia\u0026gt; man = glove.embeddings[:, 1]; julia\u0026gt; king = glove.embeddings[:, 2]; julia\u0026gt; woman = glove.embeddings[:, 3]; julia\u0026gt; cosine_dist(king - man + woman, queen) 0.13904202f0 This is less than 1/4 of the distance of \u0026ldquo;woman\u0026rdquo; to \u0026ldquo;king\u0026rdquo;:\njulia\u0026gt; cosine_dist(woman, king) 0.58866215f0 Feel free to play around with others words. If you want suggestions, another classical example is:\ncosine_dist(Madrid - Spain + France, Paris) Conclusion I think that by allying interesting applications to abstract math topics like trigonometry is the vital missing piece in STEM education. I wish every new kid that is learning math could have the opportunity to contemplate how new and exciting technologies have some amazing simple math under the hood. If you liked this post, you would probably like linear algebra. I would highly recommend Gilbert Strang\u0026rsquo;s books and 3blue1brown series on linear algebra.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","permalink":"https://storopoli.io/2023-11-20-word_embeddings/","summary":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way.","title":"Word Embeddings"},{"content":" Let\u0026rsquo;s dive into the concept of \u0026ldquo;soydev\u0026rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.\nWhile we\u0026rsquo;ll start with a look at the soydev stereotype, our journey will lead us to a wider reflection on the importance of depth in technological understanding.\nDefinition First, let\u0026rsquo;s tackle the definition of soydev. Urban Dictionary provides two interesting definitions:\nUrban Dictionary definition 1:\nSoydev is a \u0026ldquo;programmer\u0026rdquo; that works at a bigh tech company and only knows JavaScript and HTML. They love IDEs like Visual Studio Code and inefficient frameworks that slow their code down. They represent the majority of \u0026ldquo;programmers\u0026rdquo; today and if their numbers continue growing, not one person on earth will know how a computer works by the year 2050 when all the gigachad 1980s C and Unix programmers are gone.\nUrban Dictionary definition 2:\nSoydev is a type of most abundant Software Developer. The Software he/she makes is always inefficient and uses more CPU and RAM than it should. This person always prefers hard work to smart work, Has little or no knowledge of existing solutions of a problem, Comes up with very complex solution for a simple problem and has fear of native and fast programming languages like C, C++ and Rust\nThese definitions give a glimpse of what a soydev is. However, they are loaded with pejorative language, and also are based on non-timeless technologies and tools. I, much prefer to rely on concepts and principles that are timeless. Hence, I will provide my own definition of soydev:\nSoydev is someone who only has a superficial conception of technology and computers that is restricted to repeating patterns learned from popular workflows on the internet; but who doesn\u0026rsquo;t dedicate time or effort to learning concepts in a deeper way.\nAlthough soydev is a term with specific connotations, it opens the door to a larger conversation about the depth of our engagement with technology. This superficiality is not unique to soydevs but is a symptom of a broader trend in our relationship with technology.\nMost of us start our journey in a skill by having the superficial conception of it. However, some are not satisfied with this superficial conception, and strive to understand what lies beyond the surface.\nUnderstanding concepts from first principles allows us to achieve a deep graceful kind of mastery that when seems almost effortless to others. Deep down lies a lot of effort and time spent in learning and practicing. Innumerable hours of deep thinking and reflecting on why things are the way they are, and how they could be different if you tried to implement them from scratch yourself.\nThere is also an inherently rare mixture of curiosity and creativity in the process of profoundly learning and understanding concepts in this way. You start not only to ask the \u0026ldquo;Why?\u0026rdquo; questions but also the \u0026ldquo;What if?\u0026rdquo; questions. I feel that this posture on understanding concepts paves the way for joyful mastery.\nRichard Feynman once said \u0026ldquo;What I cannot create, I do not understand\u0026rdquo;. You cannot create anything that you don\u0026rsquo;t know the underlying concepts. Therefore, by allying creativity and discovery with deep knowledge, Feynman\u0026rsquo;s argument was that in order for you truly master something, you\u0026rsquo;ll need to be able to recreate it from scratch.\nIf you are struggling with my abstractions, I can provide some concrete examples. A soydev might be someone who:\nInstead of using a simple text editor like vim/emacs/nano, prefers a totally bloated IDE like VSCode to write even basic things in plaintext. Or cannot SSH into a remote server without a GUI. Instead of choosing a native solution like HTML for a simple web page section, prefers to overengineer with a JavaScript framework that has a lot of dependencies, and takes 3 seconds to render in the browser. Prefers to use black box abstractions instead of learning about basic tech primitives and protocols that would get the job done. Has no idea what about most concepts that are at the core of computing like bit, byte, heap, stack, garbage collector, async, parallel, CPU cycles, cache miss, L1/L2/L3 cache etc. Don\u0026rsquo;t understand the math, or what is going on under the hood when using machine learning libraries. Why soydev is bad First, let\u0026rsquo;s understand that being a soydev is not necessarily bad, but is highly limited on his ability and curiosity. A soydev will never be able to achieve the same level of mastery as someone who is willing to go deep and learn concepts from first principles.\nNow, on the other hand, soydev is bad because it perpetuates a mindset of superficiality. The path of technology innovation is guided by curiosity and creativity. And paved with hard work and deep understanding. Imagine if all the great minds in technology took the easy path of mindless tooling and problem-solving? We would be in a stagnant and infertile scenario, where everyone would use the same technology and tools without questioning or thinking about the problems that they are trying to solve.\nHence, the culture of soydev is bad for the future of technology, where most new developers will be highly limited in their ability to innovate.\nWhy soydev is on the rise I think that soydev culture is highly correlated with the increase of technology and decrease of barriers to access such technology. We live in an age that not only technology is everywhere, but also to interact with it is quite effortless.\nMy computational statistician mind is always aware of cognitive and statistical bias. Whenever I see a correlation across time, I always take a step back and try to think about the assumptions and conceptual models behind it.\nDoes the increase in technology usage and importance in daily life results in more people using technology from a professional point-of-view? Yes. Does the increase in people professionally using technology results in an increase of tooling and conceptual abstractions that allows superficial interactions without need to deeply understand the concepts behind such technology? I do think that this is true as well.\nThese assumptions cover the constituents of the rise of soydev from a \u0026ldquo;demand\u0026rdquo; viewpoint. Nevertheless, there is also the analogous \u0026ldquo;supply\u0026rdquo; viewpoint. If these trends in demand are not met by trends in supply, we would not see the establishment of the soydev phenomenon. There is an emerging trend to standardize all the available tech into commodities.\nWhile commoditization of technological solutions has inherent advantages, such as scalability and lower opportunity costs, it has some disadvantages. The main disadvantage is the abrupt decrease of technological innovations. If we have strong standardization that are enforced by market and social forces, then why care to innovate? Why bring new solutions or new ways to solve problems if it will not be adopted and are doomed to oblivion? Why decide to try to do things different if there is such a high maintenance cost, especially when training and expanding human resources capable of dealing with such non-standard solutions?\nIn this context, technological innovation can only be undertaken by big corporations that, not only have big budgets, but also big influence to push its innovations as industry standards.\nDon\u0026rsquo;t get me wrong: I do think that industry standards are important. However, I much prefer a protocol standard than product standards. First, protocol standards are generally not tied to a single company or brand. Second, protocol standards have a higher propensity to expose its underlying concepts to developers. Think about TCP/IP versus your favorite front-end framework: Which one would result in deeper understanding of the underlying concepts?\nThe rise of soydevs mirrors a societal shift towards immediate gratification and away from the pursuit of deep knowledge.\nHow to stop being a soydev Despite these unstoppable trends I do think that it is possible to use tools and shallow abstractions without being a soydev. Or, to stop being a soydev and advance towards deep understanding of what constitutes your craft. Moving beyond the \u0026lsquo;soydev\u0026rsquo; mindset is about embracing the richness that comes from a deep understanding of technology. Here is a short, not by any means exhaustive list of things that you can start doing:\nStop thinking about what is latest and greatest. The tools are not important as the problems they are solving. Understand what is the need that the tool tries to address, and the major concepts on how it works. Focus on concepts, protocols and first principles. Forget about frameworks, languages, editors, apps, etc. Focus on what probably won\u0026rsquo;t change in the next decade in technology. It is easy (and less uncertain) to think concepts and problems, than in tools and solutions. Set aside some weekly time to explore new ideas. Time block it, make it a calendar event. Try to find a time slot that you can be free of distraction and worries. Free all of your working memory when you are starting a \u0026ldquo;new idea\u0026rdquo; session. Think about what problem is this idea trying to solve. Also don\u0026rsquo;t be stuck to just using the \u0026ldquo;problem-solving\u0026rdquo; lens. Most concepts that are worth knowing in computer science and programming don\u0026rsquo;t have a clear problem tied to them. Tinker. Play around. Break something down and see if you can build from scratch. Remember Feynman\u0026rsquo;s \u0026ldquo;What I cannot create, I do not understand\u0026rdquo;. Failure is almost certain. Yes, you will definitely fail. I say to a lot of junior devs, interns, and students that the only advantage that I have compared to them is that I\u0026rsquo;ve failed more times that they have tried. Stay curious and hungry for knowledge. I am always impressed on how children are so curious. I feel sad that almost no one retains their childhood curiosity as an adult (Maybe that\u0026rsquo;s why I love to interact with children). Also, I am amazed by how deprived of will to learn some are. That is an idea that for me, it is hard to grasp, since I feel the exactly opposite. Often I have to hold myself not diving into certain areas, ideas or concepts because I cannot afford the time to learn them. However, I am always tempted by them. License This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","permalink":"https://storopoli.io/2023-11-10-2023-11-13-soydev/","summary":"Let\u0026rsquo;s dive into the concept of \u0026ldquo;soydev\u0026rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.","title":"What is soydev? And why do I hate it"},{"content":"I am a software developer with a background in computational statististics, scienfitic computing, data science, machine learning/deep learning, backend development, and full-stack web development. In other words, I make computers go faster while crunching big numbers (which is almost everything that they do). I am also an ardent Bayesian. I like Rust, Julia, Nix, Vim, and htmx. I love math, computer science, coding, stats, and teaching. I practice Stoic Philosophy daily. I hate bloatware and the soydev phenomenon. Everything that I do is either open source or has a permissive Creative Commons license.\nHere\u0026rsquo;s a list of some resources that I\u0026rsquo;ve made or contributed:\nJulia Data Science book. Graduate course on Bayesian Statistics Bayesian Statistics with Julia and Turing TuringGLM.jl poweranalyses.org Online calculator for Sample Size, Significance Level, Power, and Effect Size Sudoku PWA WebAssembly Game in Rust (code at storopoli/sudoku) Data Science and Scientific Computing with Julia (Portuguese) Undergraduate course on Data Science, Machine Learning and Deep Learning (Portuguese) Why Julia? A gentle pitch Graduate course on Statistics (Portuguese) with R Rcpp tutorials (Portuguese) Topic Modeling workshop (Portuguese) I don\u0026rsquo;t have social media, since I think they are overrated and \u0026ldquo;they sell your data\u0026rdquo;. If you want to contact me, please send an email.\nFor career opportunities, check my CV and my website storopoli.io; where I keep a blog with technical aspects and backstages behind my projects and milestones.\n","permalink":"https://storopoli.io/about/","summary":"I am a software developer with a background in computational statististics, scienfitic computing, data science, machine learning/deep learning, backend development, and full-stack web development. In other words, I make computers go faster while crunching big numbers (which is almost everything that they do). I am also an ardent Bayesian. I like Rust, Julia, Nix, Vim, and htmx. I love math, computer science, coding, stats, and teaching. I practice Stoic Philosophy daily.","title":"About"}]