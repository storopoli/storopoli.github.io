[{"content":"In programming language circles there\u0026rsquo;s a recently trend of discussing a concept called zero-cost abstractions: the ability to use higher-levels abstractions without suffering any loss of performance.\nZero-cost abstractions allows you to write performant code without having to give up a single drop of convenience and expressiveness:\nYou want for-loops? You can have it. Generics? Yeah, why not? Data structures? Sure, keep\u0026rsquo;em coming. Async operations? You bet ya! Multi-threading? Hell yes!\nTo put more formally, I like this definition from StackOverflow:\nZero Cost Abstractions means adding higher-level programming concepts, like generics, collections and so on do not come with a run-time cost, only compile time cost (the code will be slower to compile). Any operation on zero-cost abstractions is as fast as you would write out matching functionality by hand using lower-level programming concepts like for loops, counters, ifs and using raw pointers.\nHere\u0026rsquo;s an analogy:\nImagine that you are going to buy a car. The sales person offers you a fancy car praising how easy it is to drive it, that you don\u0026rsquo;t need to think about RPM, clutch and stick shift, parking maneuver, fuel type, and other shenanigans. You just turn it on and drive. However, once you take a look at the car\u0026rsquo;s data sheet, you are horrified. The car is bad in every aspect except easy of use. It has dreadful fuel consumption, atrocious safety ratings, disastrous handling, and so on\u0026hellip;\nBelieve me, you wouldn\u0026rsquo;t want to own that car.\nMetaphors aside, that\u0026rsquo;s exactly what professional developers1 and whole teams choose to use every day: unacceptable inferior tools. Tools that, not only don\u0026rsquo;t have zero-cost abstractions, rather don\u0026rsquo;t allow you to even have non-zero-cost anything!\nLet\u0026rsquo;s do some Python bashing in the meantime. I know that\u0026rsquo;s easy to bash Python, but that\u0026rsquo;s not the point. If Python wasn\u0026rsquo;t used so widely in production, I would definitely leave it alone. Don\u0026rsquo;t get me wrong, Python is the second-best language for everything2.\nThe curious case of the Python boolean I wish this meme was a joke, but it isn\u0026rsquo;t. A boolean is one of the simplest data type taking only two possible values: true or false. Just grab your nearest Python REPL:\n\u0026gt;\u0026gt;\u0026gt; from sys import getsizeof \u0026gt;\u0026gt;\u0026gt; getsizeof(True) 28 The function sys.getsizeof returns the size of an object in bytes. How the hell Python needs 28 bytes to represent something that needs at most 1 byte3? Imagine incurring a 28x penalty in memory size requirements for every boolean that you use. Now multiply this by every operation that your code is going to run in production over time. Again: unacceptable.\nThat\u0026rsquo;s because all objects in Python, in the sense that everything that you can instantiate, i.e. everything that you can put on the left hand-side of the = assignment, is a PyObject:\nAll Python objects ultimately share a small number of fields at the beginning of the objectâ€™s representation in memory. These are represented by the PyObject and PyVarObject types.\nPython is dynamically-typed, which means that you don\u0026rsquo;t have primitives like 8-, 16-, 32-bit (un)signed integers and so on. Everything is a huge mess allocated in the heap that must carry not only its value, but also information about its type.\nMost important, everything that is fast in Python is not Python-based. Take a look at the image below, I grabbed some popular Python libraries from GitHub, namely NumPy (linear algebra package) and PyToch (deep learning package), and checked the language codebase percentage.\nSurprise, they are not Python libraries. They are C/C++ codebases. Even if Python is the main language used in these codebases4, I still think that this is not the case due to the nature of the Python code: all docstrings are written in Python. If you have a very fast C function in your codebase that takes 50 lines of code, follower by a Python wrapper function that calls it using 10 lines of code, but with a docstring that is 50 lines of code; you have a \u0026ldquo;Python\u0026rdquo;-majority codebase.\nIn a sense the most efficient Python programmer is a C/C++ programmer\u0026hellip;\nHere\u0026rsquo;s Julia, which is also dynamically-typed:\njulia\u0026gt; Base.summarysize(true) 1 And to your surprise, Julia is coded in \u0026hellip;. Julia! Check the image below for the language codebase percentage of Julia and Lux.jl5 (deep learning package).\nFinally, here\u0026rsquo;s Rust, which is not dynamically-, but static-typed:\n// main.rs use std::mem; fn main() { println!(\u0026#34;Size of bool: {} byte\u0026#34;, mem::size_of::\u0026lt;bool\u0026gt;()); } $ cargo run --release Compiling size_of_bool v0.1.0 Finished release [optimized] target(s) in 0.00s Running `target/release/size_of_bool` Size of bool: 1 byte More zero-costs abstractions Let\u0026rsquo;s cover two more zero-costs abstractions, both in Julia and in Rust: for-loops and enums.\nFor-loops A friend and a Julia-advocate once told me that Julia\u0026rsquo;s master plan is to secretly \u0026ldquo;make everyone aware about compilers\u0026rdquo;. The compiler is a program that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.\nPython uses CPython as the compiler. If you search around on why CPython/Python is so slow and inefficient, you\u0026rsquo;ll find that the culprits are:\nPython is dynamic-typed language. Python\u0026rsquo;s Global Interpreter Lock (GIL) restricts multi-threading capabilities. Python is interpreted, which means that Python code is executed sequentially: line-by-line. Python is garbage-collected: all memory its tracked, and allocated or deallocated which introduces overhead. I completely disagree with almost all the above reasons, except the GIL. Python is slow because of its design decisions, more specifically the way CPython works under the hood. It is not built for performance in mind. Actually, the main objective of Python was to be a \u0026ldquo;language that would be easy to read, write, and maintain\u0026rdquo;. I salute that: Python has remained true to its main objective.\nNow let\u0026rsquo;s switch to Julia:\nJulia is dynamic-typed language. Julia is interpreted, which means that Julia code is executed sequentially: line-by-line. Julia is garbage-collected: all memory its tracked, and allocated or deallocated which introduces overhead. I\u0026rsquo;ve copy-pasted all Python\u0026rsquo;s arguments for inefficiency, except the GIL. And, contrary to Python, Julia is fast! Sometimes even faster than C6. Actually, that was the goal all along since Julia\u0026rsquo;s inception. If you check the notorious Julia announcement blog post from 2012:\nWe want a language that\u0026rsquo;s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that\u0026rsquo;s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.\n(Did we mention it should be as fast as C?)\nIt mentions \u0026ldquo;speed\u0026rdquo; twice. Not only that, but also specifically says that it should match C\u0026rsquo;s speed.\nJulia is fast because of its design decisions. One of the major reasons why Julia is fast is because of the choice of compiler that it uses: LLVM.\nLLVM originally stood for low level virtual machine. Despite its name, LLVM has little to do with traditional virtual machines. LLVM can take intermediate representation (IR) code and compile it into machine-dependent instructions. It has support and sponsorship from a lot of big-tech corporations, such as Apple, Google, IBM, Meta, Arm, Intel, AMD, Nvidia, and so on. It is a pretty fast compiler that can do wonders in optimizing IR code to a plethora of computer architectures.\nIn a sense, Julia is a front-end for LLVM. It turns your easy-to-read and easy-to-write Julia code into LLVM IR code. Take this for-loop example inside a function:\nfunction sum_10() acc = 0 for i in 1:10 acc += i end return acc end Let\u0026rsquo;s check what Julia generates as LLVM IR code for this function. We can do that with the @code_llvm macro.\njulia\u0026gt; @code_llvm debuginfo=:none sum_10() define i64 @julia_sum_10_172() #0 { top: ret i64 55 } You can\u0026rsquo;t easily fool the compiler. Julia understands that the answer is 55, and the LLVM IR generated code is pretty much just \u0026ldquo;return 55 as a 64-bit integer\u0026rdquo;.\nLet\u0026rsquo;s also check the machine-dependent instructions with the @code_native macro. I am using an Apple Silicon machine, so these instructions might differ from yours:\njulia\u0026gt; @code_native debuginfo=:none sum_10() .section __TEXT,__text,regular,pure_instructions .build_version macos, 14, 0 .globl _julia_sum_10_214 ; -- Begin function julia_sum_10_214 .p2align 2 _julia_sum_10_214: ; @julia_sum_10_214 .cfi_startproc ; %bb.0: ; %top mov w0, #55 ret .cfi_endproc ; -- End function .subsections_via_symbols The only important instruction for our argument here is the mov w0, #55. This means \u0026ldquo;move the value 55 into the w0 register\u0026rdquo;, where w0 is one of registers available in ARM-based architectures (which Apple Silicon chips are).\nThis is a zero-cost abstraction! I don\u0026rsquo;t need to give up for-loops, because they might be slow and inefficient; like some Python users suggest newcomers. I can have the full convenience and expressiveness of for-loops without paying performance costs. Pretty much the definition of a zero-cost abstraction from above.\nUsing LLVM as a compiler backend is not something unique to Julia. Rust also uses LLVM under the hood. Take for example this simple Rust code:\n// main.rs pub fn sum_10() -\u0026gt; i32 { let mut acc = 0; for i in 1..=10 { acc += i } acc } fn main() { println!(\u0026#34;sum_10: {}\u0026#34;, sum_10()); } We can inspect both LLVM IR code machine instructions with the cargo-show-asm crate:\n$ cargo asm --llvm \u0026#34;sum_10::main\u0026#34; | grep 55 Finished release [optimized] target(s) in 0.00s store i32 55, ptr %_9, align 4 $ cargo asm \u0026#34;sum_10::main\u0026#34; | grep 55 Finished release [optimized] target(s) in 0.00s mov w8, #55 No coincidence that the LLVM IR code is very similar, with the difference that integers, by default, in Julia are 64 bits and in Rust 32 bits. However, the machine code is identical: \u0026ldquo;move the value 55 into a w something register\u0026rdquo;.\nEnums Another zero-cost abstraction, in Julia and Rust, is enums.\nIn Julia all enums, by default have a BaseType of Int32: a signed 32-bit integer. However, we can override this with type annotations:\njulia\u0026gt; @enum Thing::Bool One Two julia\u0026gt; Base.summarysize(Thing(false)) 1 Here we have an enum Thing with two variants: One and Two. Since we can safely represent all the possible variant space of Thing with a boolean type, we override the BaseType of Thing to be the Bool type. Unsurprised, any object of Thing occupies 1 byte in memory.\nWe can achieve the same with Rust:\n// main.rs use std::mem; #[allow(dead_code)] enum Thing { One, Two, } fn main() { println!(\u0026#34;Size of Thing: {} byte\u0026#34;, mem::size_of::\u0026lt;Thing\u0026gt;()); } $ cargo run --release Compiling enum_size v0.1.0 Finished release [optimized] target(s) in 0.09s Running `target/release/enum_size` Size of Thing: 1 byte However, contrary to Julia, Rust compiler automatically detects the enum\u0026rsquo;s variant space size and adjust accordingly. So, no need of overrides.\nConclusion Zero-cost abstractions are a joy to have in a programming language. It enables you, as a programmer, to just focus on what\u0026rsquo;s important: write expressive code that is easy to read, maintain, debug, and build upon.\nIt is no wonder that zero-cost abstractions is a pervasive feature of two of my top-favorite languages: Julia and Rust.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nthis post is somehow connected to my soydev rant.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand that\u0026rsquo;s not a compliment.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ntechnically, we can represent a boolean with just one bit. However, the short answer is still one byte, because that\u0026rsquo;s smallest addressable unit of memory.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand modifying .gitattributes is cheating. Yes, I am talking to you NumPy!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLux.jl doesn\u0026rsquo;t even have a .gitattributes file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nif you compare runtime execution.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2023-11-28-2023-11-28-zero_cost_abstractions/","summary":"In programming language circles there\u0026rsquo;s a recently trend of discussing a concept called zero-cost abstractions: the ability to use higher-levels abstractions without suffering any loss of performance.\nZero-cost abstractions allows you to write performant code without having to give up a single drop of convenience and expressiveness:\nYou want for-loops? You can have it. Generics? Yeah, why not? Data structures? Sure, keep\u0026rsquo;em coming. Async operations? You bet ya! Multi-threading? Hell yes!","title":"Zero-cost Abstractions"},{"content":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2. Lindley predicted in 1975 that \u0026ldquo;Bayesian methods will indeed become pervasive, enabled by the development of powerful computing facilities\u0026rdquo; (Lindley, 1975). You can find more about all of Lindley\u0026rsquo;s achievements in his obituary.\nLindley\u0026rsquo;s Paradox Lindley\u0026rsquo;s paradox3 is a counterintuitive situation in statistics in which the Bayesian and frequentist approaches to a hypothesis testing problem give different results for certain choices of the prior distribution.\nMore formally, the paradox is as follows. We have some parameter $\\theta$ that we are interested in. Then, we proceed with an experiment to test two competing hypotheses:\n$H_0$ (also known as null hypothesis): there is no \u0026ldquo;effect\u0026rdquo;, or, more specifically, $\\theta = 0$. $H_a$ (also known as alternative hypothesis): there is an \u0026ldquo;effect\u0026rdquo;, or, more specifically, $\\theta \\ne 0$. The paradox occurs when two conditions are met:\nThe result of the experiment is significant by a frequentist test of $H_0$, which indicates sufficient evidence to reject $H_0$, at a certain threshold of probability4. The posterior probability (Bayesian approach) of $H_0 \\mid \\theta$ (null hypothesis given $\\theta$) is high, which indicates strong evidence that $H_0$ should be favored over $H_a$, that is, to not reject $H_0$. These results can occur at the same time when $H_0$ is very specific, $H_a$ more diffuse, and the prior distribution does not strongly favor one or the other. These conditions are pervasive across science and common in traditional null-hypothesis significance testing approaches.\nThis is a duel of frequentist versus Bayesian approaches, and one of the many in which Bayesian emerges as the most coherent. Let\u0026rsquo;s give a example and go over the analytical result with a ton of math, but also a computational result with Julia.\nExample Here\u0026rsquo;s the setup for the example. In a certain city 49,581 boys and 48,870 girls have been born over a certain time period. The observed proportion of male births is thus $\\frac{49,581}{98,451} \\approx 0.5036$.\nWe assume that the birth of a child is independent with a certain probability $\\theta$. Since our data is a sequence of $n$ independent Bernoulli trials, i.e., $n$ independent random experiments with exactly two possible outcomes: \u0026ldquo;success\u0026rdquo; and \u0026ldquo;failure\u0026rdquo;, in which the probability of success is the same every time the experiment is conducted. We can safely assume that it follows a binomial distribution with parameters:\n$n$: the number of \u0026ldquo;trials\u0026rdquo; (or the total number of births). $\\theta$: the probability of male births. We then set up our two competing hypotheses:\n$H_0$: $\\theta = 0.5$. $H_a$: $\\theta \\ne 0.5$. Analytical Solution This is a toy-problem and, like most toy problems, we can solve it analytically5 for both the frequentist and the Bayesian approaches.\nAnalytical Solutions \u0026ndash; Frequentist Approach The frequentist approach to testing $H_0$ is to compute a $p$-value4, the probability of observing births of boys at least as large as 49,581 assuming $H_0$ is true. Because the number of births is very large, we can use a normal approximation6 for the binomial-distributed number of male births. Let\u0026rsquo;s define $X$ as the total number of male births, then $X$ follows a normal distribution:\n$$X \\sim \\text{Normal}(\\mu, \\sigma)$$\nwhere $\\mu$ is the mean parameter, $n \\theta$ in our case, and $\\sigma$ is the standard deviation parameter, $\\sqrt{n \\theta (1 - \\theta)}$. We need to calculate the conditional probability of $X \\geq \\frac{49,581}{98,451} \\approx 0.5036$ given $\\mu = n \\theta = 98,451 \\cdot \\frac{1}{2} = 49,225.5$ and $\\sigma = \\sqrt{n \\theta (1 - \\theta)} = \\sqrt{98,451 \\cdot \\frac{1}{2} \\cdot (1 - \\frac{1}{2})}$:\n$$P(X \\ge 0.5036 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75})$$\nThis is basically a cumulative distribution function (CDF) of $X$ on the interval $[49,225.5, 98,451]$:\n$$\\int_{49,225.5}^{98,451} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{\\left( \\frac{x - \\mu}{\\sigma} \\right)^2}{2}} dx$$\nAfter inserting the values and doing some arithmetic, our answer is approximately $0.0117$. Note that this is a one-sided test, since it is symmetrical, the two-sided test would be $0.0117 \\cdot 2 = 0.0235$. Since we don\u0026rsquo;t deviate from the Fisher\u0026rsquo;s canon, this is well below the 5% threshold. Hooray! We rejected the null hypothesis! Quick! Grab a frequentist celebratory cigar! But, wait. Let\u0026rsquo;s check the Bayesian approach.\nAnalytical Solutions \u0026ndash; Bayesian Approach For the Bayesian approach, we need to set prior probabilities on both hypotheses. Since we do not favor one from another, let\u0026rsquo;s set equal prior probabilities:\n$$P(H_0) = P(H_a) = \\frac{1}{2}$$\nAdditionally, all parameters of interest need a prior distribution. So, let\u0026rsquo;s put a prior distribution on $\\theta$. We could be fancy here, but let\u0026rsquo;s not. We\u0026rsquo;ll use a uniform distribution on $[0, 1]$.\nWe have everything we need to compute the posterior probability of $H_0$ given $\\theta$. For this, we\u0026rsquo;ll use Bayes theorem7:\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\nNow again let\u0026rsquo;s plug in all the values:\n$$P(H_0 \\mid \\theta) = \\frac{P(\\theta \\mid H_0) P(H_0)}{P(\\theta)}$$\nNote that by the axioms of probability and by the product rule of probability we can decompose $P(\\theta)$ into:\n$$P(\\theta) = P(\\theta \\mid H_0) P(H_0) + P(\\theta \\mid H_a) P(H_a)$$\nAgain, we\u0026rsquo;ll use the normal approximation:\n$$\\begin{aligned} \u0026amp;P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \\\\ \u0026amp;= \\frac{ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\left( \\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 } { \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{ \\left( -\\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 + \\int_0^1 \\frac {1}{\\sqrt{2 \\pi \\sigma^2} } e^{- \\left( \\frac{\\mu - \\mu \\cdot \\theta)}{2 \\sigma} \\right)^2}d \\theta \\cdot 0.5 } \\\\ \u0026amp;= 0.9505 \\end{aligned}$$\nThe likelihood of the alternative hypothesis, $P(\\theta \\mid H_a)$, is just the CDF of all possible values of $\\theta \\ne 0.5$.\n$$P(H_0 \\mid \\text{data}) = P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \u0026gt; 0.95$$\nAnd we fail to reject the null hypothesis, in frequentist terms. However, we can also say in Bayesian terms, that we strongly favor $H_0$ over $H_a$.\nQuick! Grab the Bayesian celebratory cigar! The null is back on the game!\nComputational Solutional For the computational solution, we\u0026rsquo;ll use Julia and the following packages:\nHypothesisTest.jl Turing.jl Computational Solutions \u0026ndash; Frequentist Approach We can perform a BinomialTest with HypothesisTest.jl:\njulia\u0026gt; using HypothesisTests julia\u0026gt; BinomialTest(49_225, 98_451, 0.5036) Binomial test ------------- Population details: parameter of interest: Probability of success value under h_0: 0.5036 point estimate: 0.499995 95% confidence interval: (0.4969, 0.5031) Test summary: outcome with 95% confidence: reject h_0 two-sided p-value: 0.0239 Details: number of observations: 98451 number of successes: 49225 This is the two-sided test, and I had to round $49,225.5$ to $49,225$ since BinomialTest do not support real numbers. But the results match with the analytical solution, we still reject the null.\nComputational Solutions \u0026ndash; Bayesian Approach Now, for the Bayesian computational approach, I\u0026rsquo;m going to use a generative modeling approach, and one of my favorites probabilistic programming languages, Turing.jl:\njulia\u0026gt; using Turing julia\u0026gt; @model function birth_rate() Î¸ ~ Uniform(0, 1) total_births = 98_451 male_births ~ Binomial(total_births, Î¸) end; julia\u0026gt; model = birth_rate() | (; male_births = 49_225); julia\u0026gt; chain = sample(model, NUTS(1_000, 0.8), MCMCThreads(), 1_000, 4) Chains MCMC chain (1000Ã—13Ã—4 Array{Float64, 3}): Iterations = 1001:1:2000 Number of chains = 4 Samples per chain = 1000 Wall duration = 0.2 seconds Compute duration = 0.19 seconds parameters = Î¸ internals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size Summary Statistics parameters mean std mcse ess_bulk ess_tail rhat ess_per_sec Symbol Float64 Float64 Float64 Float64 Float64 Float64 Float64 Î¸ 0.4999 0.0016 0.0000 1422.2028 2198.1987 1.0057 7368.9267 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 Î¸ 0.4969 0.4988 0.4999 0.5011 0.5031 We can see from the output of the quantiles that the 95% quantile for $\\theta$ is the interval $(0.4969, 0.5031)$. Although it overlaps zero, that is not the equivalent of a hypothesis test. For that, we\u0026rsquo;ll use the highest posterior density interval (HPDI), which is defined as \u0026ldquo;choosing the narrowest interval\u0026rdquo; that captures a certain posterior density threshold value. In this case, we\u0026rsquo;ll use a threshold interval of 95%, i.e. an $\\alpha = 0.05$:\njulia\u0026gt; hpd(chain; alpha=0.05) HPD parameters lower upper Symbol Float64 Float64 Î¸ 0.4970 0.5031 We see that we fail to reject the null, $\\theta = 0.5$ at $\\alpha = 0.05$ which is in accordance with the analytical solution.\nWhy the Frequentist and Bayesian Approaches Disagree Why do the approaches disagree? What is going on under the hood?\nThe answer is disappointing8. The main problem is that the frequentist approach only allows fixed significance levels with respect to sample size. Whereas the Bayesian approach is consistent and robust to sample size variations.\nTaken to extreme, in some cases, due to huge sample sizes, the $p$-value is pretty much a proxy for sample size and have little to no utility on hypothesis testing. This is known as $p$-hacking9.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nReferences Lindley, Dennis V. \u0026ldquo;The future of statistics: A Bayesian 21st century\u0026rdquo;. Advances in Applied Probability 7 (1975): 106-115.\nas far as I know there\u0026rsquo;s only one coherent approach to uncertainty, and it is the Bayesian approach. Otherwise, as de Finetti and Ramsey proposed, you are susceptible to a Dutch book. This is a topic for another blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCromwell\u0026rsquo;s rule states that the use of prior probabilities of 1 (\u0026ldquo;the event will definitely occur\u0026rdquo;) or 0 (\u0026ldquo;the event will definitely not occur\u0026rdquo;) should be avoided, except when applied to statements that are logically true or false. Hence, anything that is not a math theorem should have priors in $(0,1)$. The reference comes from Oliver Cromwell, asking, very politely, for the Church of Scotland to consider that their prior probability might be wrong. This footnote also deserves a whole blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStigler\u0026rsquo;s law of eponymy states that no scientific discovery is named after its original discoverer. The paradox was already was discussed in Harold Jeffreys' 1939 textbook. Also, fun fact, Stigler\u0026rsquo;s is not the original creator of such law\u0026hellip; Now that\u0026rsquo;s a self-referential paradox, and a broad version of the Halting problem, which should earn its own footnote. Nevertheless, we are getting into self-referential danger zone here with footnotes\u0026rsquo; of footnotes\u0026rsquo; of footnotes\u0026rsquo;\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthis is called $p$-value and can be easily defined as \u0026ldquo;the probability of sampling data from a target population given that $H_0$ is true as the number of sampling procedures $\\to \\infty$\u0026rdquo;. Yes, it is not that intuitive, and it deserves not a blog post, but a full curriculum to hammer it home.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthat is not true for most of the real-world problems. For Bayesian approaches, we need to run computational asymptotic exact approximations using a class of methods called Markov chain Monte Carlo (MCMC). Furthermore, for some nasty problems we need to use different set of methods called variational inference (VI) or approximate Bayesian computation (ABC).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nif you are curious about how this approximation works, check the backup slides of my open access and open source graduate course on Bayesian statistics.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBayes\u0026rsquo; theorem is officially called Bayes-Price-Laplace theorem. Bayes was trying to disprove David Hume\u0026rsquo;s argument that miracles did not exist (How dare he?). He used the probabilistic approach of trying to quantify the probability of a parameter (god exists) given data (miracles happened). He died without publishing any of his ideas. His wife probably freaked out when she saw the huge pile of notes that he had and called his buddy Richard Price to figure out what to do with it. Price struck gold and immediately noticed the relevance of Bayes\u0026rsquo; findings. He read it aloud at the Royal Society. Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes, used the same probabilistic approach to perform statistical inference using France\u0026rsquo;s first census data in the early-Napoleonic era. Somehow we had the answer to statistical inference back then, and we had to rediscover everything again in the late-20th century\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ndisappointing because most of published scientific studies suffer from this flaw.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand, like all footnotes here, it deserves its own blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2023-11-23-lindley_paradox/","summary":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2.","title":"Lindley's Paradox, or The consistency of Bayesian Thinking"},{"content":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way. Alas, better late than never.\nIn this post, we\u0026rsquo;ll learn how to represent words using word embeddings, and how to use basic trigonometry to play around with them. Of course, we\u0026rsquo;ll use Julia.\nWord Embeddings Word embeddings is a way to represent words as a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\nOk, let\u0026rsquo;s unwrap the above definition. First, a real-valued vector is any vector which its elements belong to the real numbers. Generally we denote vectors with a bold lower-case letter, and we denote its elements (also called components) using square brackets. Hence, a vector $\\bold{v}$ that has 3 elements, $1$, $2$, and $3$, can be written as\n$$\\bold{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\nNext, what \u0026ldquo;close\u0026rdquo; means for vectors? We can use distance functions to get a measurable value. The most famous and commonly used distance function is the Euclidean distance, in honor of Euclid, the \u0026ldquo;father of geometry\u0026rdquo;, and the guy pictured in the image at the top of this post. The Euclidean distance is defined in trigonometry for 2-D and 3-D spaces. However, it can be generalized to any dimension $n \u0026gt; 1$ by using vectors.\nSince every word is represented by an $n$-dimensional vector, we can use distances to compute a metric that represent similarity between vectors. And, more interesting, we can add and subtract words (or any other linear combination of one or more words) to generate new words.\nBefore we jump to code and examples, a quick note about how word embeddings are constructed. They are trained like a regular machine learning algorithm, where the cost function measures the difference between some vector distance between the vectors and a \u0026ldquo;semantic distance\u0026rdquo;. The goal is to iteratively find good vector values that minimize the cost. So, if a vector is close to another vector measured by a distance function, but far apart measured by some semantic distance on the words that these vectors represent, then the cost function will be higher. The algorithm cannot change the semantic distance, it is treated as a fixed value. However, it can change the vector elements\u0026rsquo; values so that the vector distance function closely resembles the semantic distance function. Lastly, generally the dimensionality of the vectors used in word embeddings are high, $n \u0026gt; 50$, since it needs a proper amount of dimensions in order to represent all the semantic information of words with vectors.\nPre-Trained Word Embeddings Generally we don\u0026rsquo;t train our own word embeddings from scratch, we use pre-trained ones. Here is a list of some of the most popular ones:\nWord2Vec: One of the first public available word embeddings, made by Google in 2013. Only supports English. GloVe: made by Stanford in 2014. Only supports English. FastText: From Facebook, released in 2016. Supports hundreds of languages. Julia Code We will use the Embeddings.jl package to easily load word embeddings as vectors, and the Distances.jl package for the convenience of several distance functions. This is a nice example of the Julia package ecosystem composability, where one package can define types, another can define functions, and another can define custom behavior of these functions on types that are defined in other packages.\njulia\u0026gt; using Embeddings julia\u0026gt; using Distances Let\u0026rsquo;s load the GloVe word embeddings. First, let\u0026rsquo;s check what we have in store to choose from GloVe\u0026rsquo;s English language embeddings:\njulia\u0026gt; language_files(GloVe{:en}) 20-element Vector{String}: \u0026#34;glove.6B/glove.6B.50d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.100d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.300d.txt\u0026#34; \u0026#34;glove.42B.300d/glove.42B.300d.txt\u0026#34; \u0026#34;glove.840B.300d/glove.840B.300d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.25d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.50d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.100d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.50d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.100d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.300d.txt\u0026#34; \u0026#34;glove.42B.300d/glove.42B.300d.txt\u0026#34; \u0026#34;glove.840B.300d/glove.840B.300d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.25d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.50d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.100d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.200d.txt\u0026#34; I\u0026rsquo;ll use the \u0026quot;glove.6B/glove.6B.50d.txt\u0026quot;. This means that it was trained with 6 billion tokens, and it provides embeddings with 50-dimensional vectors. The load_embeddings function takes an optional second positional argument as an Int to choose from which index of the language_files to use. Finally, I just want the words \u0026ldquo;king\u0026rdquo;, \u0026ldquo;queen\u0026rdquo;, \u0026ldquo;man\u0026rdquo;, \u0026ldquo;woman\u0026rdquo;; so I am passing these words as a Set to the keep_words keyword argument:\njulia\u0026gt; const glove = load_embeddings(GloVe{:en}, 1; keep_words=Set([\u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;])); Embeddings.EmbeddingTable{Matrix{Float32}, Vector{String}}(Float32[-0.094386 0.50451 -0.18153 0.37854; 0.43007 0.68607 0.64827 1.8233; â€¦ ; 0.53135 -0.64426 0.48764 0.0092753; -0.11725 -0.51042 -0.10467 -0.60284], [\u0026#34;man\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;queen\u0026#34;]) Watch out with the order that we get back. If you see the output of load_embeddings, the order is \u0026quot;man\u0026quot;, \u0026quot;king\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;queen\u0026quot;] Let\u0026rsquo;s see how a word is represented:\njulia\u0026gt; queen = glove.embeddings[:, 4] 50-element Vector{Float32}: 0.37854 1.8233 -1.2648 â‹® -2.2839 0.0092753 -0.60284 They are 50-dimensional vectors of Float32.\nNow, here\u0026rsquo;s the fun part: let\u0026rsquo;s add words and check the similarity between the result and some other word. A classical example is to start with the word \u0026ldquo;king\u0026rdquo;, subtract the word \u0026ldquo;men\u0026rdquo;, add the word \u0026ldquo;woman\u0026rdquo;, and check the distance of the result to the word \u0026ldquo;queen\u0026rdquo;:\njulia\u0026gt; man = glove.embeddings[:, 1]; julia\u0026gt; king = glove.embeddings[:, 2]; julia\u0026gt; woman = glove.embeddings[:, 3]; julia\u0026gt; cosine_dist(king - man + woman, queen) 0.13904202f0 This is less than 1/4 of the distance of \u0026ldquo;woman\u0026rdquo; to \u0026ldquo;king\u0026rdquo;:\njulia\u0026gt; cosine_dist(woman, king) 0.58866215f0 Feel free to play around with others words. If you want suggestions, another classical example is:\ncosine_dist(Madrid - Spain + France, Paris) Conclusion I think that by allying interesting applications to abstract math topics like trigonometry is the vital missing piece in STEM education. I wish every new kid that is learning math could have the opportunity to contemplate how new and exciting technologies have some amazing simple math under the hood. If you liked this post, you would probably like linear algebra. I would highly recommend Gilbert Strang\u0026rsquo;s books and 3blue1brown series on linear algebra.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","permalink":"https://storopoli.io/2023-11-20-word_embeddings/","summary":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way.","title":"Word Embeddings"},{"content":" Let\u0026rsquo;s dive into the concept of \u0026ldquo;soydev\u0026rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.\nWhile we\u0026rsquo;ll start with a look at the soydev stereotype, our journey will lead us to a wider reflection on the importance of depth in technological understanding.\nDefinition First, let\u0026rsquo;s tackle the definition of soydev. Urban Dictionary provides two interesting definitions:\nUrban Dictionary definition 1:\nSoydev is a \u0026ldquo;programmer\u0026rdquo; that works at a bigh tech company and only knows JavaScript and HTML. They love IDEs like Visual Studio Code and inefficient frameworks that slow their code down. They represent the majority of \u0026ldquo;programmers\u0026rdquo; today and if their numbers continue growing, not one person on earth will know how a computer works by the year 2050 when all the gigachad 1980s C and Unix programmers are gone.\nUrban Dictionary definition 2:\nSoydev is a type of most abundant Software Developer. The Software he/she makes is always inefficient and uses more CPU and RAM than it should. This person always prefers hard work to smart work, Has little or no knowledge of existing solutions of a problem, Comes up with very complex solution for a simple problem and has fear of native and fast programming languages like C, C++ and Rust\nThese definitions give a glimpse of what a soydev is. However, they are loaded with pejorative language, and also are based on non-timeless technologies and tools. I, much prefer to rely on concepts and principles that are timeless. Hence, I will provide my own definition of soydev:\nSoydev is someone who only has a superficial conception of technology and computers that is restricted to repeating patterns learned from popular workflows on the internet; but who doesn\u0026rsquo;t dedicate time or effort to learning concepts in a deeper way.\nAlthough soydev is a term with specific connotations, it opens the door to a larger conversation about the depth of our engagement with technology. This superficiality is not unique to soydevs but is a symptom of a broader trend in our relationship with technology.\nMost of us start our journey in a skill by having the superficial conception of it. However, some are not satisfied with this superficial conception, and strive to understand what lies beyond the surface.\nUnderstanding concepts from first principles allows us to achieve a deep graceful kind of mastery that when seems almost effortless to others. Deep down lies a lot of effort and time spent in learning and practicing. Innumerable hours of deep thinking and reflecting on why things are the way they are, and how they could be different if you tried to implement them from scratch yourself.\nThere is also an inherently rare mixture of curiosity and creativity in the process of profoundly learning and understanding concepts in this way. You start not only to ask the \u0026ldquo;Why?\u0026rdquo; questions but also the \u0026ldquo;What if?\u0026rdquo; questions. I feel that this posture on understanding concepts paves the way for joyful mastery.\nRichard Feynman once said \u0026ldquo;What I cannot create, I do not understand\u0026rdquo;. You cannot create anything that you don\u0026rsquo;t know the underlying concepts. Therefore, by allying creativity and discovery with deep knowledge, Feynman\u0026rsquo;s argument was that in order for you truly master something, you\u0026rsquo;ll need to be able to recreate it from scratch.\nIf you are struggling with my abstractions, I can provide some concrete examples. A soydev might be someone who:\nInstead of using a simple text editor like vim/emacs/nano, prefers a totally bloated IDE like VSCode to write even basic things in plaintext. Or cannot SSH into a remote server without a GUI. Instead of choosing a native solution like HTML for a simple web page section, prefers to overengineer with a JavaScript framework that has a lot of dependencies, and takes 3 seconds to render in the browser. Prefers to use black box abstractions instead of learning about basic tech primitives and protocols that would get the job done. Has no idea what about most concepts that are at the core of computing like bit, byte, heap, stack, garbage collector, async, parallel, CPU cycles, cache miss, L1/L2/L3 cache etc. Don\u0026rsquo;t understand the math, or what is going on under the hood when using machine learning libraries. Why soydev is bad First, let\u0026rsquo;s understand that being a soydev is not necessarily bad, but is highly limited on his ability and curiosity. A soydev will never be able to achieve the same level of mastery as someone who is willing to go deep and learn concepts from first principles.\nNow, on the other hand, soydev is bad because it perpetuates a mindset of superficiality. The path of technology innovation is guided by curiosity and creativity. And paved with hard work and deep understanding. Imagine if all the great minds in technology took the easy path of mindless tooling and problem-solving? We would be in a stagnant and infertile scenario, where everyone would use the same technology and tools without questioning or thinking about the problems that they are trying to solve.\nHence, the culture of soydev is bad for the future of technology, where most new developers will be highly limited in their ability to innovate.\nWhy soydev is on the rise I think that soydev culture is highly correlated with the increase of technology and decrease of barriers to access such technology. We live in an age that not only technology is everywhere, but also to interact with it is quite effortless.\nMy computational statistician mind is always aware of cognitive and statistical bias. Whenever I see a correlation across time, I always take a step back and try to think about the assumptions and conceptual models behind it.\nDoes the increase in technology usage and importance in daily life results in more people using technology from a professional point-of-view? Yes. Does the increase in people professionally using technology results in an increase of tooling and conceptual abstractions that allows superficial interactions without need to deeply understand the concepts behind such technology? I do think that this is true as well.\nThese assumptions cover the constituents of the rise of soydev from a \u0026ldquo;demand\u0026rdquo; viewpoint. Nevertheless, there is also the analogous \u0026ldquo;supply\u0026rdquo; viewpoint. If these trends in demand are not met by trends in supply, we would not see the establishment of the soydev phenomenon. There is an emerging trend to standardize all the available tech into commodities.\nWhile commoditization of technological solutions has inherent advantages, such as scalability and lower opportunity costs, it has some disadvantages. The main disadvantage is the abrupt decrease of technological innovations. If we have strong standardization that are enforced by market and social forces, then why care to innovate? Why bring new solutions or new ways to solve problems if it will not be adopted and are doomed to oblivion? Why decide to try to do things different if there is such a high maintenance cost, especially when training and expanding human resources capable of dealing with such non-standard solutions?\nIn this context, technological innovation can only be undertaken by big corporations that, not only have big budgets, but also big influence to push its innovations as industry standards.\nDon\u0026rsquo;t get me wrong: I do think that industry standards are important. However, I much prefer a protocol standard than product standards. First, protocol standards are generally not tied to a single company or brand. Second, protocol standards have a higher propensity to expose its underlying concepts to developers. Think about TCP/IP versus your favorite front-end framework: Which one would result in deeper understanding of the underlying concepts?\nThe rise of soydevs mirrors a societal shift towards immediate gratification and away from the pursuit of deep knowledge.\nHow to stop being a soydev Despite these unstoppable trends I do think that it is possible to use tools and shallow abstractions without being a soydev. Or, to stop being a soydev and advance towards deep understanding of what constitutes your craft. Moving beyond the \u0026lsquo;soydev\u0026rsquo; mindset is about embracing the richness that comes from a deep understanding of technology. Here is a short, not by any means exhaustive list of things that you can start doing:\nStop thinking about what is latest and greatest. The tools are not important as the problems they are solving. Understand what is the need that the tool tries to address, and the major concepts on how it works. Focus on concepts, protocols and first principles. Forget about frameworks, languages, editors, apps, etc. Focus on what probably won\u0026rsquo;t change in the next decade in technology. It is easy (and less uncertain) to think concepts and problems, than in tools and solutions. Set aside some weekly time to explore new ideas. Time block it, make it a calendar event. Try to find a time slot that you can be free of distraction and worries. Free all of your working memory when you are starting a \u0026ldquo;new idea\u0026rdquo; session. Think about what problem is this idea trying to solve. Also don\u0026rsquo;t be stuck to just using the \u0026ldquo;problem-solving\u0026rdquo; lens. Most concepts that are worth knowing in computer science and programming don\u0026rsquo;t have a clear problem tied to them. Tinker. Play around. Break something down and see if you can build from scratch. Remember Feynman\u0026rsquo;s \u0026ldquo;What I cannot create, I do not understand\u0026rdquo;. Failure is almost certain. Yes, you will definitely fail. I say to a lot of junior devs, interns, and students that the only advantage that I have compared to them is that I\u0026rsquo;ve failed more times that they have tried. Stay curious and hungry for knowledge. I am always impressed on how children are so curious. I feel sad that almost no one retains their childhood curiosity as an adult (Maybe that\u0026rsquo;s why I love to interact with children). Also, I am amazed by how deprived of will to learn some are. That is an idea that for me, it is hard to grasp, since I feel the exactly opposite. Often I have to hold myself not diving into certain areas, ideas or concepts because I cannot afford the time to learn them. However, I am always tempted by them. License This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","permalink":"https://storopoli.io/2023-11-10-2023-11-13-soydev/","summary":"Let\u0026rsquo;s dive into the concept of \u0026ldquo;soydev\u0026rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.","title":"What is soydev? And why do I hate it"},{"content":"I am a computational statistician, data scientist, machine learning engineer; and an ardent Bayesian. I like Julia, Rust, Neovim, and Nix. I love math, computer science, coding, stats, and teaching. I hate bloatware and soydevs. My natural habitat is the terminal. Everything that I do is either open source or has a permissive Creative Commons license.\nHere\u0026rsquo;s a list of some resources that I\u0026rsquo;ve made or contributed:\nJulia Data Science book. Graduate course on Bayesian Statistics with Stan and Turing.jl code examples. Bayesian Statistics with Julia and Turing.jl TuringGLM.jl Introduction to Julia (JuliaCon 2022 Workshop) Data Science and Scientific Computing with Julia (Portuguese) Undergraduate course on Data Science, Machine Learning and Deep Learning (Portuguese) Why Julia? A gentle pitch Graduate course on Statistics (Portuguese) with R Rcpp tutorials (Portuguese) Topic Modeling workshop (Portuguese) I don\u0026rsquo;t have social media, since I think they are overrated and \u0026ldquo;they sell your data\u0026rdquo;. If you want to contact me, please send an email.\n","permalink":"https://storopoli.io/about/","summary":"I am a computational statistician, data scientist, machine learning engineer; and an ardent Bayesian. I like Julia, Rust, Neovim, and Nix. I love math, computer science, coding, stats, and teaching. I hate bloatware and soydevs. My natural habitat is the terminal. Everything that I do is either open source or has a permissive Creative Commons license.\nHere\u0026rsquo;s a list of some resources that I\u0026rsquo;ve made or contributed:\nJulia Data Science book.","title":"About"}]