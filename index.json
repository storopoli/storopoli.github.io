[{"content":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2. Lindley predicted in 1975 that \u0026ldquo;Bayesian methods will indeed become pervasive, enabled by the development of powerful computing facilities\u0026rdquo; (Lindley, 1975). You can find more about all of Lindley\u0026rsquo;s achievements in his obituary.\nLindley\u0026rsquo;s Paradox Lindley\u0026rsquo;s paradox3 is a counterintuitive situation in statistics in which the Bayesian and frequentist approaches to a hypothesis testing problem give different results for certain choices of the prior distribution.\nMore formally, the paradox is as follows. We have some parameter $\\theta$ that we are interested in. Then, we proceed with an experiment to test two competing hypotheses:\n$H_0$ (also known as null hypothesis): there is no \u0026ldquo;effect\u0026rdquo;, or, more specifically, $\\theta = 0$. $H_a$ (also known as alternative hypothesis): there is an \u0026ldquo;effect\u0026rdquo;, or, more specifically, $\\theta \\ne 0$. The paradox occurs when two conditions are met:\nThe result of the experiment is significant by a frequentist test of $H_0$, which indicates sufficient evidence to reject $H_0$, at a certain threshold of probability4. The posterior probability (Bayesian approach) of $H_0 \\mid \\theta$ (null hypothesis given $\\theta$) is high, which indicates strong evidence that $H_0$ should be favored over $H_a$, that is, to not reject $H_0$. These results can occur at the same time when $H_0$ is very specific, $H_a$ more diffuse, and the prior distribution does not strongly favor one or the other. These conditions are pervasive across science and common in traditional null-hypothesis significance testing approaches.\nThis is a duel of frequentist versus Bayesian approaches, and one of the many in which Bayesian emerges as the most coherent. Let\u0026rsquo;s give a example and go over the analytical result with a ton of math, but also a computational result with Julia.\nExample Here\u0026rsquo;s the setup for the example. In a certain city 49,581 boys and 48,870 girls have been born over a certain time period. The observed proportion of male births is thus $\\frac{49,581}{98,451} \\approx 0.5036$.\nWe assume that the birth of a child is independent with a certain probability $\\theta$. Since our data is a sequence of $n$ independent Bernoulli trials, i.e., $n$ independent random experiments with exactly two possible outcomes: \u0026ldquo;success\u0026rdquo; and \u0026ldquo;failure\u0026rdquo;, in which the probability of success is the same every time the experiment is conducted. We can safely assume that it follows a binomial distribution with parameters:\n$n$: the number of \u0026ldquo;trials\u0026rdquo; (or the total number of births). $\\theta$: the probability of male births. We then set up our two competing hypotheses:\n$H_0$: $\\theta = 0.5$. $H_a$: $\\theta \\ne 0.5$. Analytical Solution This is a toy-problem and, like most toy problems, we can solve it analytically5 for both the frequentist and the Bayesian approaches.\nAnalytical Solutions \u0026ndash; Frequentist Approach The frequentist approach to testing $H_0$ is to compute a $p$-value4, the probability of observing births of boys at least as large as 49,581 assuming $H_0$ is true. Because the number of births is very large, we can use a normal approximation6 for the binomial-distributed number of male births. Let\u0026rsquo;s define $X$ as the total number of male births, then $X$ follows a normal distribution:\n$$X \\sim \\text{Normal}(\\mu, \\sigma)$$\nwhere $\\mu$ is the mean parameter, $n \\theta$ in our case, and $\\sigma$ is the standard deviation parameter, $\\sqrt{n \\theta (1 - \\theta)}$. We need to calculate the conditional probability of $X \\geq \\frac{49,581}{98,451} \\approx 0.5036$ given $\\mu = n \\theta = 98,451 \\cdot \\frac{1}{2} = 49,225.5$ and $\\sigma = \\sqrt{n \\theta (1 - \\theta)} = \\sqrt{98,451 \\cdot \\frac{1}{2} \\cdot (1 - \\frac{1}{2})}$:\n$$P(X \\ge 0.5036 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75})$$\nThis is basically a cumulative distribution function (CDF) of $X$ on the interval $[49,225.5, 98,451]$:\n$$\\int_{49,225.5}^{98,451} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{\\left( \\frac{x - \\mu}{\\sigma} \\right)^2}{2}} dx$$\nAfter inserting the values and doing some arithmetic, our answer is approximately $0.0117$. Note that this is a one-sided test, since it is symmetrical, the two-sided test would be $0.0117 \\cdot 2 = 0.0235$. Since we don\u0026rsquo;t deviate from the Fisher\u0026rsquo;s canon, this is well below the 5% threshold. Hooray! We rejected the null hypothesis! Quick! Grab a frequentist celebratory cigar! But, wait. Let\u0026rsquo;s check the Bayesian approach.\nAnalytical Solutions \u0026ndash; Bayesian Approach For the Bayesian approach, we need to set prior probabilities on both hypotheses. Since we do not favor one from another, let\u0026rsquo;s set equal prior probabilities:\n$$P(H_0) = P(H_a) = \\frac{1}{2}$$\nAdditionally, all parameters of interest need a prior distribution. So, let\u0026rsquo;s put a prior distribution on $\\theta$. We could be fancy here, but let\u0026rsquo;s not. We\u0026rsquo;ll use a uniform distribution on $[0, 1]$.\nWe have everything we need to compute the posterior probability of $H_0$ given $\\theta$. For this, we\u0026rsquo;ll use Bayes theorem7:\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\nNow again let\u0026rsquo;s plug in all the values:\n$$P(H_0 \\mid \\theta) = \\frac{P(\\theta \\mid H_0) P(H_0)}{P(\\theta)}$$\nNote that by the axioms of probability and by the product rule of probability we can decompose $P(\\theta)$ into:\n$$P(\\theta) = P(\\theta \\mid H_0) P(H_0) + P(\\theta \\mid H_a) P(H_a)$$\nAgain, we\u0026rsquo;ll use the normal approximation:\n$$\\begin{aligned} \u0026amp;P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \\\\ \u0026amp;= \\frac{ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\left( \\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 } { \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{ \\left( -\\frac{(\\mu - \\mu \\cdot 0.5)}{2 \\sigma} \\right)^2} \\cdot 0.5 + \\int_0^1 \\frac {1}{\\sqrt{2 \\pi \\sigma^2} } e^{- \\left( \\frac{\\mu - \\mu \\cdot \\theta)}{2 \\sigma} \\right)^2}d \\theta \\cdot 0.5 } \\\\ \u0026amp;= 0.9505 \\end{aligned}$$\nThe likelihood of the alternative hypothesis, $P(\\theta \\mid H_a)$, is just the CDF of all possible values of $\\theta \\ne 0.5$.\n$$P(H_0 \\mid \\text{data}) = P \\left( \\theta = 0.5 \\mid \\mu = 49,225.5, \\sigma = \\sqrt{24.612.75} \\right) \u0026gt; 0.95$$\nAnd we fail to reject the null hypothesis, in frequentist terms. However, we can also say in Bayesian terms, that we strongly favor $H_0$ over $H_a$.\nQuick! Grab the Bayesian celebratory cigar! The null is back on the game!\nComputational Solutional For the computational solution, we\u0026rsquo;ll use Julia and the following packages:\nHypothesisTest.jl Turing.jl Computational Solutions \u0026ndash; Frequentist Approach We can perform a BinomialTest with HypothesisTest.jl:\njulia\u0026gt; using HypothesisTests julia\u0026gt; BinomialTest(49_225, 98_451, 0.5036) Binomial test ------------- Population details: parameter of interest: Probability of success value under h_0: 0.5036 point estimate: 0.499995 95% confidence interval: (0.4969, 0.5031) Test summary: outcome with 95% confidence: reject h_0 two-sided p-value: 0.0239 Details: number of observations: 98451 number of successes: 49225 This is the two-sided test, and I had to round $49,225.5$ to $49,225$ since BinomialTest do not support real numbers. But the results match with the analytical solution, we still reject the null.\nComputational Solutions \u0026ndash; Bayesian Approach Now, for the Bayesian computational approach, I\u0026rsquo;m going to use a generative modeling approach, and one of my favorites probabilistic programming languages, Turing.jl:\njulia\u0026gt; using Turing julia\u0026gt; @model function birth_rate() θ ~ Uniform(0, 1) total_births = 98_451 male_births ~ Binomial(total_births, θ) end; julia\u0026gt; model = birth_rate() | (; male_births = 49_225); julia\u0026gt; chain = sample(model, NUTS(1_000, 0.8), MCMCThreads(), 1_000, 4) Chains MCMC chain (1000×13×4 Array{Float64, 3}): Iterations = 1001:1:2000 Number of chains = 4 Samples per chain = 1000 Wall duration = 0.2 seconds Compute duration = 0.19 seconds parameters = θ internals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size Summary Statistics parameters mean std mcse ess_bulk ess_tail rhat ess_per_sec Symbol Float64 Float64 Float64 Float64 Float64 Float64 Float64 θ 0.4999 0.0016 0.0000 1422.2028 2198.1987 1.0057 7368.9267 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% Symbol Float64 Float64 Float64 Float64 Float64 θ 0.4969 0.4988 0.4999 0.5011 0.5031 We can see from the output of the quantiles that the 95% quantile for $\\theta$ is the interval $(0.4969, 0.5031)$. Although it overlaps zero, that is not the equivalent of a hypothesis test. For that, we\u0026rsquo;ll use the highest posterior density interval (HPDI), which is defined as \u0026ldquo;choosing the narrowest interval\u0026rdquo; that captures a certain posterior density threshold value. In this case, we\u0026rsquo;ll use a threshold interval of 95%, i.e. an $\\alpha = 0.05$:\njulia\u0026gt; hpd(chain; alpha=0.05) HPD parameters lower upper Symbol Float64 Float64 θ 0.4970 0.5031 We see that we fail to reject the null, $\\theta = 0.5$ at $\\alpha = 0.05$ which is in accordance with the analytical solution.\nWhy the Frequentist and Bayesian Approaches Disagree Why do the approaches disagree? What is going on under the hood?\nThe answer is disappointing8. The main problem is that the frequentist approach only allows fixed significance levels with respect to sample size. Whereas the Bayesian approach is consistent and robust to sample size variations.\nTaken to extreme, in some cases, due to huge sample sizes, the $p$-value is pretty much a proxy for sample size and have little to no utility on hypothesis testing. This is known as $p$-hacking9.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\nReferences Lindley, Dennis V. \u0026ldquo;The future of statistics: A Bayesian 21st century\u0026rdquo;. Advances in Applied Probability 7 (1975): 106-115.\nas far as I know there\u0026rsquo;s only one coherent approach to uncertainty, and it is the Bayesian approach. Otherwise, as de Finetti and Ramsey proposed, you are susceptible to a Dutch book. This is a topic for another blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCromwell\u0026rsquo;s rule states that the use of prior probabilities of 1 (\u0026ldquo;the event will definitely occur\u0026rdquo;) or 0 (\u0026ldquo;the event will definitely not occur\u0026rdquo;) should be avoided, except when applied to statements that are logically true or false. Hence, anything that is not a math theorem should have priors in $(0,1)$. The reference comes from Oliver Cromwell, asking, very politely, for the Church of Scotland to consider that their prior probability might be wrong. This footnote also deserves a whole blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStigler\u0026rsquo;s law of eponymy states that no scientific discovery is named after its original discoverer. The paradox was already was discussed in Harold Jeffreys' 1939 textbook. Also, fun fact, Stigler\u0026rsquo;s is not the original creator of such law\u0026hellip; Now that\u0026rsquo;s a self-referential paradox, and a broad version of the Halting problem, which should earn its own footnote. Nevertheless, we are getting into self-referential danger zone here with footnotes\u0026rsquo; of footnotes\u0026rsquo; of footnotes\u0026rsquo;\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthis is called $p$-value and can be easily defined as \u0026ldquo;the probability of sampling data from a target population given that $H_0$ is true as the number of sampling procedures $\\to \\infty$\u0026rdquo;. Yes, it is not that intuitive, and it deserves not a blog post, but a full curriculum to hammer it home.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nthat is not true for most of the real-world problems. For Bayesian approaches, we need to run computational asymptotic exact approximations using a class of methods called Markov chain Monte Carlo (MCMC). Furthermore, for some nasty problems we need to use different set of methods called variational inference (VI) or approximate Bayesian computation (ABC).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nif you are curious about how this approximation works, check the backup slides of my open access and open source graduate course on Bayesian statistics.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBayes\u0026rsquo; theorem is officially called Bayes-Price-Laplace theorem. Bayes was trying to disprove David Hume\u0026rsquo;s argument that miracles did not exist (How dare he?). He used the probabilistic approach of trying to quantify the probability of a parameter (god exists) given data (miracles happened). He died without publishing any of his ideas. His wife probably freaked out when she saw the huge pile of notes that he had and called his buddy Richard Price to figure out what to do with it. Price struck gold and immediately noticed the relevance of Bayes\u0026rsquo; findings. He read it aloud at the Royal Society. Later, Pierre-Simon Laplace, unbeknownst to the work of Bayes, used the same probabilistic approach to perform statistical inference using France\u0026rsquo;s first census data in the early-Napoleonic era. Somehow we had the answer to statistical inference back then, and we had to rediscover everything again in the late-20th century\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ndisappointing because most of published scientific studies suffer from this flaw.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nand, like all footnotes here, it deserves its own blog post\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://storopoli.io/2023-11-23-lindley_paradox/","summary":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nDennis Lindley, one of my many heroes, was an English statistician, decision theorist and leading advocate of Bayesian statistics. He published a pivotal book, Understanding Uncertainty, that changed my view on what is and how to handle uncertainty in a coherent1 way. He is responsible for one of my favorites quotes: \u0026ldquo;Inside every non-Bayesian there is a Bayesian struggling to get out\u0026rdquo;; and one of my favorite heuristics around prior probabilities: Cromwell\u0026rsquo;s Rule2.","title":"Lindley's Paradox, or The consistency of Bayesian Thinking"},{"content":" Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way. Alas, better late than never.\nIn this post, we\u0026rsquo;ll learn how to represent words using word embeddings, and how to use basic trigonometry to play around with them. Of course, we\u0026rsquo;ll use Julia.\nWord Embeddings Word embeddings is a way to represent words as a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\nOk, let\u0026rsquo;s unwrap the above definition. First, a real-valued vector is any vector which its elements belong to the real numbers. Generally we denote vectors with a bold lower-case letter, and we denote its elements (also called components) using square brackets. Hence, a vector $\\bold{v}$ that has 3 elements, $1$, $2$, and $3$, can be written as\n$$\\bold{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\nNext, what \u0026ldquo;close\u0026rdquo; means for vectors? We can use distance functions to get a measurable value. The most famous and commonly used distance function is the Euclidean distance, in honor of Euclid, the \u0026ldquo;father of geometry\u0026rdquo;, and the guy pictured in the image at the top of this post. The Euclidean distance is defined in trigonometry for 2-D and 3-D spaces. However, it can be generalized to any dimension $n \u0026gt; 1$ by using vectors.\nSince every word is represented by an $n$-dimensional vector, we can use distances to compute a metric that represent similarity between vectors. And, more interesting, we can add and subtract words (or any other linear combination of one or more words) to generate new words.\nBefore we jump to code and examples, a quick note about how word embeddings are constructed. They are trained like a regular machine learning algorithm, where the cost function measures the difference between some vector distance between the vectors and a \u0026ldquo;semantic distance\u0026rdquo;. The goal is to iteratively find good vector values that minimize the cost. So, if a vector is close to another vector measured by a distance function, but far apart measured by some semantic distance on the words that these vectors represent, then the cost function will be higher. The algorithm cannot change the semantic distance, it is treated as a fixed value. However, it can change the vector elements\u0026rsquo; values so that the vector distance function closely resembles the semantic distance function. Lastly, generally the dimensionality of the vectors used in word embeddings are high, $n \u0026gt; 50$, since it needs a proper amount of dimensions in order to represent all the semantic information of words with vectors.\nPre-Trained Word Embeddings Generally we don\u0026rsquo;t train our own word embeddings from scratch, we use pre-trained ones. Here is a list of some of the most popular ones:\nWord2Vec: One of the first public available word embeddings, made by Google in 2013. Only supports English. GloVe: made by Stanford in 2014. Only supports English. FastText: From Facebook, released in 2016. Supports hundreds of languages. Julia Code We will use the Embeddings.jl package to easily load word embeddings as vectors, and the Distances.jl package for the convenience of several distance functions. This is a nice example of the Julia package ecosystem composability, where one package can define types, another can define functions, and another can define custom behavior of these functions on types that are defined in other packages.\njulia\u0026gt; using Embeddings julia\u0026gt; using Distances Let\u0026rsquo;s load the GloVe word embeddings. First, let\u0026rsquo;s check what we have in store to choose from GloVe\u0026rsquo;s English language embeddings:\njulia\u0026gt; language_files(GloVe{:en}) 20-element Vector{String}: \u0026#34;glove.6B/glove.6B.50d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.100d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.300d.txt\u0026#34; \u0026#34;glove.42B.300d/glove.42B.300d.txt\u0026#34; \u0026#34;glove.840B.300d/glove.840B.300d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.25d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.50d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.100d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.50d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.100d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.200d.txt\u0026#34; \u0026#34;glove.6B/glove.6B.300d.txt\u0026#34; \u0026#34;glove.42B.300d/glove.42B.300d.txt\u0026#34; \u0026#34;glove.840B.300d/glove.840B.300d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.25d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.50d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.100d.txt\u0026#34; \u0026#34;glove.twitter.27B/glove.twitter.27B.200d.txt\u0026#34; I\u0026rsquo;ll use the \u0026quot;glove.6B/glove.6B.50d.txt\u0026quot;. This means that it was trained with 6 billion tokens, and it provides embeddings with 50-dimensional vectors. The load_embeddings function takes an optional second positional argument as an Int to choose from which index of the language_files to use. Finally, I just want the words \u0026ldquo;king\u0026rdquo;, \u0026ldquo;queen\u0026rdquo;, \u0026ldquo;man\u0026rdquo;, \u0026ldquo;woman\u0026rdquo;; so I am passing these words as a Set to the keep_words keyword argument:\njulia\u0026gt; const glove = load_embeddings(GloVe{:en}, 1; keep_words=Set([\u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;])); Embeddings.EmbeddingTable{Matrix{Float32}, Vector{String}}(Float32[-0.094386 0.50451 -0.18153 0.37854; 0.43007 0.68607 0.64827 1.8233; … ; 0.53135 -0.64426 0.48764 0.0092753; -0.11725 -0.51042 -0.10467 -0.60284], [\u0026#34;man\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;queen\u0026#34;]) Watch out with the order that we get back. If you see the output of load_embeddings, the order is \u0026quot;man\u0026quot;, \u0026quot;king\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;queen\u0026quot;] Let\u0026rsquo;s see how a word is represented:\njulia\u0026gt; queen = glove.embeddings[:, 4] 50-element Vector{Float32}: 0.37854 1.8233 -1.2648 ⋮ -2.2839 0.0092753 -0.60284 They are 50-dimensional vectors of Float32.\nNow, here\u0026rsquo;s the fun part: let\u0026rsquo;s add words and check the similarity between the result and some other word. A classical example is to start with the word \u0026ldquo;king\u0026rdquo;, subtract the word \u0026ldquo;men\u0026rdquo;, add the word \u0026ldquo;woman\u0026rdquo;, and check the distance of the result to the word \u0026ldquo;queen\u0026rdquo;:\njulia\u0026gt; man = glove.embeddings[:, 1]; julia\u0026gt; king = glove.embeddings[:, 2]; julia\u0026gt; woman = glove.embeddings[:, 3]; julia\u0026gt; cosine_dist(king - man + woman, queen) 0.13904202f0 This is less than 1/4 of the distance of \u0026ldquo;woman\u0026rdquo; to \u0026ldquo;king\u0026rdquo;:\njulia\u0026gt; cosine_dist(woman, king) 0.58866215f0 Feel free to play around with others words. If you want suggestions, another classical example is:\ncosine_dist(Madrid - Spain + France, Paris) Conclusion I think that by allying interesting applications to abstract math topics like trigonometry is the vital missing piece in STEM education. I wish every new kid that is learning math could have the opportunity to contemplate how new and exciting technologies have some amazing simple math under the hood. If you liked this post, you would probably like linear algebra. I would highly recommend Gilbert Strang\u0026rsquo;s books and 3blue1brown series on linear algebra.\nLicense This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","permalink":"https://storopoli.io/2023-11-20-word_embeddings/","summary":"Warning: This post has KaTeX enabled, so if you want to view the rendered math formulas, you\u0026rsquo;ll have to unfortunately enable JavaScript.\nI wish I could go back in time and tell my younger self that you can make a machine understand human language with trigonometry. That would definitely have made me more aware and interested in the subject during my school years. I would have looked at triangles, circles, sines, cosines, and tangents in a whole different way.","title":"Word Embeddings"},{"content":" Let\u0026rsquo;s dive into the concept of \u0026ldquo;soydev\u0026rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.\nWhile we\u0026rsquo;ll start with a look at the soydev stereotype, our journey will lead us to a wider reflection on the importance of depth in technological understanding.\nDefinition First, let\u0026rsquo;s tackle the definition of soydev. Urban Dictionary provides two interesting definitions:\nUrban Dictionary definition 1:\nSoydev is a \u0026ldquo;programmer\u0026rdquo; that works at a bigh tech company and only knows JavaScript and HTML. They love IDEs like Visual Studio Code and inefficient frameworks that slow their code down. They represent the majority of \u0026ldquo;programmers\u0026rdquo; today and if their numbers continue growing, not one person on earth will know how a computer works by the year 2050 when all the gigachad 1980s C and Unix programmers are gone.\nUrban Dictionary definition 2:\nSoydev is a type of most abundant Software Developer. The Software he/she makes is always inefficient and uses more CPU and RAM than it should. This person always prefers hard work to smart work, Has little or no knowledge of existing solutions of a problem, Comes up with very complex solution for a simple problem and has fear of native and fast programming languages like C, C++ and Rust\nThese definitions give a glimpse of what a soydev is. However, they are loaded with pejorative language, and also are based on non-timeless technologies and tools. I, much prefer to rely on concepts and principles that are timeless. Hence, I will provide my own definition of soydev:\nSoydev is someone who only has a superficial conception of technology and computers that is restricted to repeating patterns learned from popular workflows on the internet; but who doesn\u0026rsquo;t dedicate time or effort to learning concepts in a deeper way.\nAlthough soydev is a term with specific connotations, it opens the door to a larger conversation about the depth of our engagement with technology. This superficiality is not unique to soydevs but is a symptom of a broader trend in our relationship with technology.\nMost of us start our journey in a skill by having the superficial conception of it. However, some are not satisfied with this superficial conception, and strive to understand what lies beyond the surface.\nUnderstanding concepts from first principles allows us to achieve a deep graceful kind of mastery that when seems almost effortless to others. Deep down lies a lot of effort and time spent in learning and practicing. Innumerable hours of deep thinking and reflecting on why things are the way they are, and how they could be different if you tried to implement them from scratch yourself.\nThere is also an inherently rare mixture of curiosity and creativity in the process of profoundly learning and understanding concepts in this way. You start not only to ask the \u0026ldquo;Why?\u0026rdquo; questions but also the \u0026ldquo;What if?\u0026rdquo; questions. I feel that this posture on understanding concepts paves the way for joyful mastery.\nRichard Feynman once said \u0026ldquo;What I cannot create, I do not understand\u0026rdquo;. You cannot create anything that you don\u0026rsquo;t know the underlying concepts. Therefore, by allying creativity and discovery with deep knowledge, Feynman\u0026rsquo;s argument was that in order for you truly master something, you\u0026rsquo;ll need to be able to recreate it from scratch.\nIf you are struggling with my abstractions, I can provide some concrete examples. A soydev might be someone who:\nInstead of using a simple text editor like vim/emacs/nano, prefers a totally bloated IDE like VSCode to write even basic things in plaintext. Or cannot SSH into a remote server without a GUI. Instead of choosing a native solution like HTML for a simple web page section, prefers to overengineer with a JavaScript framework that has a lot of dependencies, and takes 3 seconds to render in the browser. Prefers to use black box abstractions instead of learning about basic tech primitives and protocols that would get the job done. Has no idea what about most concepts that are at the core of computing like bit, byte, heap, stack, garbage collector, async, parallel, CPU cycles, cache miss, L1/L2/L3 cache etc. Don\u0026rsquo;t understand the math, or what is going on under the hood when using machine learning libraries. Why soydev is bad First, let\u0026rsquo;s understand that being a soydev is not necessarily bad, but is highly limited on his ability and curiosity. A soydev will never be able to achieve the same level of mastery as someone who is willing to go deep and learn concepts from first principles.\nNow, on the other hand, soydev is bad because it perpetuates a mindset of superficiality. The path of technology innovation is guided by curiosity and creativity. And paved with hard work and deep understanding. Imagine if all the great minds in technology took the easy path of mindless tooling and problem-solving? We would be in a stagnant and infertile scenario, where everyone would use the same technology and tools without questioning or thinking about the problems that they are trying to solve.\nHence, the culture of soydev is bad for the future of technology, where most new developers will be highly limited in their ability to innovate.\nWhy soydev is on the rise I think that soydev culture is highly correlated with the increase of technology and decrease of barriers to access such technology. We live in an age that not only technology is everywhere, but also to interact with it is quite effortless.\nMy computational statistician mind is always aware of cognitive and statistical bias. Whenever I see a correlation across time, I always take a step back and try to think about the assumptions and conceptual models behind it.\nDoes the increase in technology usage and importance in daily life results in more people using technology from a professional point-of-view? Yes. Does the increase in people professionally using technology results in an increase of tooling and conceptual abstractions that allows superficial interactions without need to deeply understand the concepts behind such technology? I do think that this is true as well.\nThese assumptions cover the constituents of the rise of soydev from a \u0026ldquo;demand\u0026rdquo; viewpoint. Nevertheless, there is also the analogous \u0026ldquo;supply\u0026rdquo; viewpoint. If these trends in demand are not met by trends in supply, we would not see the establishment of the soydev phenomenon. There is an emerging trend to standardize all the available tech into commodities.\nWhile commoditization of technological solutions has inherent advantages, such as scalability and lower opportunity costs, it has some disadvantages. The main disadvantage is the abrupt decrease of technological innovations. If we have strong standardization that are enforced by market and social forces, then why care to innovate? Why bring new solutions or new ways to solve problems if it will not be adopted and are doomed to oblivion? Why decide to try to do things different if there is such a high maintenance cost, especially when training and expanding human resources capable of dealing with such non-standard solutions?\nIn this context, technological innovation can only be undertaken by big corporations that, not only have big budgets, but also big influence to push its innovations as industry standards.\nDon\u0026rsquo;t get me wrong: I do think that industry standards are important. However, I much prefer a protocol standard than product standards. First, protocol standards are generally not tied to a single company or brand. Second, protocol standards have a higher propensity to expose its underlying concepts to developers. Think about TCP/IP versus your favorite front-end framework: Which one would result in deeper understanding of the underlying concepts?\nThe rise of soydevs mirrors a societal shift towards immediate gratification and away from the pursuit of deep knowledge.\nHow to stop being a soydev Despite these unstoppable trends I do think that it is possible to use tools and shallow abstractions without being a soydev. Or, to stop being a soydev and advance towards deep understanding of what constitutes your craft. Moving beyond the \u0026lsquo;soydev\u0026rsquo; mindset is about embracing the richness that comes from a deep understanding of technology. Here is a short, not by any means exhaustive list of things that you can start doing:\nStop thinking about what is latest and greatest. The tools are not important as the problems they are solving. Understand what is the need that the tool tries to address, and the major concepts on how it works. Focus on concepts, protocols and first principles. Forget about frameworks, languages, editors, apps, etc. Focus on what probably won\u0026rsquo;t change in the next decade in technology. It is easy (and less uncertain) to think concepts and problems, than in tools and solutions. Set aside some weekly time to explore new ideas. Time block it, make it a calendar event. Try to find a time slot that you can be free of distraction and worries. Free all of your working memory when you are starting a \u0026ldquo;new idea\u0026rdquo; session. Think about what problem is this idea trying to solve. Also don\u0026rsquo;t be stuck to just using the \u0026ldquo;problem-solving\u0026rdquo; lens. Most concepts that are worth knowing in computer science and programming don\u0026rsquo;t have a clear problem tied to them. Tinker. Play around. Break something down and see if you can build from scratch. Remember Feynman\u0026rsquo;s \u0026ldquo;What I cannot create, I do not understand\u0026rdquo;. Failure is almost certain. Yes, you will definitely fail. I say to a lot of junior devs, interns, and students that the only advantage that I have compared to them is that I\u0026rsquo;ve failed more times that they have tried. Stay curious and hungry for knowledge. I am always impressed on how children are so curious. I feel sad that almost no one retains their childhood curiosity as an adult (Maybe that\u0026rsquo;s why I love to interact with children). Also, I am amazed by how deprived of will to learn some are. That is an idea that for me, it is hard to grasp, since I feel the exactly opposite. Often I have to hold myself not diving into certain areas, ideas or concepts because I cannot afford the time to learn them. However, I am always tempted by them. License This post is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n","permalink":"https://storopoli.io/2023-11-10-2023-11-13-soydev/","summary":"Let\u0026rsquo;s dive into the concept of \u0026ldquo;soydev\u0026rdquo;, a term often used pejoratively to describe developers with a superficial understanding of technology. I provide my definition of what soydev is, why is bad, and how it came to be. To counteract soydev inclinations, I propose an abstract approach centered on timeless concepts, protocols, and first principles, fostering a mindset of exploration, resilience in the face of failure, and an insatiable hunger for knowledge.","title":"What is soydev? And why do I hate it"},{"content":"I am a computational statistician and an ardent Bayesian. I like Julia, Rust, Neovim, and Nix. I love math, computer science, coding, stats, and teaching. I hate bloatware and soydevs. My natural habitat is the terminal. Everything that I do is either open source or has a permissive Creative Commons license.\nHere\u0026rsquo;s a list of some resources that I\u0026rsquo;ve made or contributed:\nJulia Data Science book. Graduate course on Bayesian Statistics with Stan and Turing.jl code examples. Bayesian Statistics with Julia and Turing.jl TuringGLM.jl Data Science and Scientific Computing with Julia (Portuguese) Undergraduate course on Data Science, Machine Learning and Deep Learning (Portuguese) Why Julia? A gentle pitch Graduate course on Statistics (Portuguese) with R Rcpp tutorials (Portuguese) Topic Modeling workshop (Portuguese) I don\u0026rsquo;t have social media, since I think they are overrated and \u0026ldquo;they sell your data\u0026rdquo;. If you want to contact me, please send an email.\n","permalink":"https://storopoli.io/about/","summary":"I am a computational statistician and an ardent Bayesian. I like Julia, Rust, Neovim, and Nix. I love math, computer science, coding, stats, and teaching. I hate bloatware and soydevs. My natural habitat is the terminal. Everything that I do is either open source or has a permissive Creative Commons license.\nHere\u0026rsquo;s a list of some resources that I\u0026rsquo;ve made or contributed:\nJulia Data Science book. Graduate course on Bayesian Statistics with Stan and Turing.","title":"About"}]